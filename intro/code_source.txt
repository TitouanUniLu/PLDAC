The following text is a Git repository with code. The structure of the text are sections that begin with ----!@#$----, followed by a single line containing the file path and file name, followed by a variable amount of lines containing the file contents. The text representing the Git repository ends when the symbols --END-- are encounted. Any further text beyond --END-- are meant to be interpreted as instructions using the aforementioned Git repository as context.
----!@#$----
output.txt

----!@#$----
stats.py
from abc import ABC, abstractmethod
import numpy as np
from scipy.stats import ttest_ind, mannwhitneyu, rankdata
import bootstrapped.bootstrap as bs
import bootstrapped.compare_functions as bs_compare
import bootstrapped.stats_functions as bs_stats
import matplotlib.pyplot as plt

# Code adapted from https://github.com/flowersteam/rl_stats
#


def run_permutation_test(all_data, n1, n2):
    np.random.shuffle(all_data)
    data_a = all_data[:n1]
    data_b = all_data[-n2:]
    return data_a.mean() - data_b.mean()


def compute_central_tendency_and_error(id_central, id_error, sample):
    try:
        id_error = int(id_error)
    except Exception:
        pass

    if id_central == "mean":
        central = np.nanmean(sample, axis=1)
    elif id_central == "median":
        central = np.nanmedian(sample, axis=1)
    else:
        raise NotImplementedError

    if isinstance(id_error, int):
        low = np.nanpercentile(sample, q=int((100 - id_error) / 2), axis=1)
        high = np.nanpercentile(sample, q=int(100 - (100 - id_error) / 2), axis=1)
    elif id_error == "std":
        low = central - np.nanstd(sample, axis=1)
        high = central + np.nanstd(sample, axis=1)
    elif id_error == "sem":
        low = central - np.nanstd(sample, axis=1) / np.sqrt(sample.shape[0])
        high = central + np.nanstd(sample, axis=1) / np.sqrt(sample.shape[0])
    else:
        raise NotImplementedError

    return central, low, high


class Test(ABC):
    def plot(
        self,
        data1,  # array of performance of dimension (n_steps, n_seeds) for alg 1
        data2,  # array of performance of dimension (n_steps, n_seeds) for alg 2
        point_every=1,  # evaluation frequency, one datapoint every X steps/episodes
        confidence_level=0.01,  # confidence level alpha of the test
        id_central="median",  # id of the central tendency ('mean' or 'median')
        id_error=80,  # id of the error areas ('std', 'sem', or percentiles in ]0, 100]
        legends="alg 1/alg 2",  # labels of the two input vectors
        xlabel="training steps",  # label of the x axis
        save=True,  # save in ./plot.png if True
        downsampling_fact=5,  # factor of downsampling on the x-axis for visualization purpose (increase for smoother plots)
    ):
        assert (
            data1.ndim == 2
        ), "data should be an array of dimension 2 (n_steps, n_seeds)"
        assert (
            data2.ndim == 2
        ), "data should be an array of dimension 2 (n_steps, n_seeds)"

        nb_steps = max(data1.shape[0], data2.shape[0])
        steps = [0]
        while len(steps) < nb_steps:
            steps.append(steps[-1] + point_every)
        steps = np.array(steps)
        if steps is not None:
            assert (
                steps.size == nb_steps
            ), "x should be of the size of the longest data array"

        sample_size1 = data1.shape[1]
        sample_size2 = data2.shape[1]

        # downsample for visualization purpose
        sub_steps = np.arange(0, nb_steps, downsampling_fact)
        steps = steps[sub_steps]
        nb_steps = steps.size

        # handle arrays of different lengths by padding with nans
        sample1 = np.zeros([nb_steps, sample_size1])
        sample1.fill(np.nan)
        sample2 = np.zeros([nb_steps, sample_size2])
        sample2.fill(np.nan)
        sub_steps1 = sub_steps[: data1.shape[0] // downsampling_fact]
        sub_steps2 = sub_steps[: data2.shape[0] // downsampling_fact]
        sample1[: data1[sub_steps1, :].shape[0], :] = data1[sub_steps1, :]
        sample2[: data2[sub_steps2, :].shape[0], :] = data2[sub_steps2, :]

        # test
        sign_diff = np.zeros([len(steps)])
        for i in range(len(steps)):
            sign_diff[i] = self.run_test(
                sample1[i, :].squeeze(), sample2[i, :].squeeze(), alpha=confidence_level
            )

        central1, low1, high1 = compute_central_tendency_and_error(
            id_central, id_error, sample1
        )
        central2, low2, high2 = compute_central_tendency_and_error(
            id_central, id_error, sample2
        )

        # plot
        fig, ax = plt.subplots(1, 1, figsize=(20, 10))
        lab1 = plt.xlabel(xlabel)
        lab2 = plt.ylabel("performance")

        plt.plot(steps, central1, linewidth=10)
        plt.plot(steps, central2, linewidth=10)
        plt.fill_between(steps, low1, high1, alpha=0.3)
        plt.fill_between(steps, low2, high2, alpha=0.3)
        splitted = legends.split("/")
        leg = ax.legend((splitted[0], splitted[1]), frameon=False)

        # plot significative difference as dots
        idx = np.argwhere(sign_diff == 1)
        y = max(np.nanmax(high1), np.nanmax(high2))
        plt.scatter(
            steps[idx], y * 1.05 * np.ones([idx.size]), s=100, c="k", marker="o"
        )

        # style
        for line in leg.get_lines():
            line.set_linewidth(10.0)
        ax.spines["top"].set_linewidth(5)
        ax.spines["right"].set_linewidth(5)
        ax.spines["bottom"].set_linewidth(5)
        ax.spines["left"].set_linewidth(5)

        if save:
            plt.savefig(
                "./plot.png",
                bbox_extra_artists=(leg, lab1, lab2),
                bbox_inches="tight",
                dpi=100,
            )

        plt.show()

    @abstractmethod
    def run_test(self, data1, data2, alpha=0.05):
        """
        Compute tests comparing data1 and data2 with confidence level alpha
        :param data1: (np.ndarray) sample 1
        :param data2: (np.ndarray) sample 2
        :param alpha: (float) confidence level of the test
        :return: (bool) if True, the null hypothesis is rejected
        """


class BootstrapTest(Test):
    def run_test(self, data1, data2, alpha=0.05):
        assert alpha < 1 and alpha > 0, "alpha should be between 0 and 1"
        res = bs.bootstrap_ab(
            data1,
            data2,
            bs_stats.mean,
            bs_compare.difference,
            alpha=alpha,
            num_iterations=1000,
        )
        rejection = np.sign(res.upper_bound) == np.sign(res.lower_bound)
        return rejection


class TTest(Test):
    def run_test(self, data1, data2, alpha=0.05):
        _, p = ttest_ind(data1, data2, equal_var=True)
        return p < alpha


class WelchTTest(Test):
    """Welch t-test (recommended)"""

    def run_test(self, data1, data2, alpha=0.05):
        _, p = ttest_ind(data1, data2, equal_var=False)
        return p < alpha


class MannWhitneyTest(Test):
    def run_test(self, data1, data2, alpha=0.05):
        _, p = mannwhitneyu(data1, data2, alternative="two-sided")
        return p < alpha


class RankedTTest(Test):
    def run_test(self, data1, data2, alpha=0.05):
        n1, n2 = data1.size, data2.size
        all_data = np.concatenate([data1.copy(), data2.copy()], axis=0)
        ranks = rankdata(all_data)
        ranks1 = ranks[:n1]
        ranks2 = ranks[n1 : n1 + n2]
        assert ranks2.size == n2
        _, p = ttest_ind(ranks1, ranks2, equal_var=True)
        return p < alpha


class PermutationTest(Test):
    def run_test(self, data1, data2, alpha=0.05):
        n1, n2 = data1.size, data2.size
        all_data = np.concatenate([data1.copy(), data2.copy()], axis=0)
        delta = np.abs(data1.mean() - data2.mean())
        num_samples = 1000
        estimates = []
        for _ in range(num_samples):
            estimates.append(run_permutation_test(all_data.copy(), n1, n2))
        estimates = np.abs(np.array(estimates))
        diff_count = len(np.where(estimates <= delta)[0])
        return (1.0 - (float(diff_count) / float(num_samples))) < alpha


# if __name__ == '__main__':
#     import argparse
#     import sys
#     data1 = np.loadtxt('./data/sac_hc_all_perfs.txt')
#     data2 = np.loadtxt('./data/td3_hc_all_perfs.txt')
#     sample_size = 20
#     data1 = data1[:, np.random.randint(0, data1.shape[1], sample_size)]
#     data2 = data2[:, np.random.randint(0, data1.shape[1], sample_size)]
#     parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
#     add = parser.add_argument
#     add('--data1', type=str, default=data1, help='path to text file containing array of performance of dimension (n_steps, n_seeds) for alg 1. Can also receive the array '
#                                                  'directly.')
#     add('--data2', type=str, default=data2, help='path to text file containing array of performance of dimension (n_steps, n_seeds) for alg 2. Can also receive the array '
#                                              'directly.')
#     add('--point_every', type=int, default=1, help='evaluation frequency, one datapoint every X steps/episodes')
#     add('--test_id', type=str, default="Welch t-test", help="choose in [t-test, Welch t-test, Mann-Whitney, Ranked t-test, bootstrap, permutation], welch recommended (see paper)")
#     add('--confidence_level', type=float, default=0.01, help='confidence level alpha of the test')
#     add('--id_central', type=str, default='median', help="id of the central tendency ('mean' or 'median')")
#     add('--id_error', default=80, help="id of the error areas ('std', 'sem', or percentiles in ]0, 100]")
#     add('--legends', type=str, default='SAC/TD3', help='labels of the two input vectors "legend1/legend2"')
#     add('--xlabel', type=str, default='training episodes', help='label of the x axis, usually episodes or steps')
#     add('--save', type=bool, default=True, help='save in ./plot.png if True')
#     add('--downsampling_fact', type=int, default=5, help='factor of downsampling on the x-axis for visualization purpose (increase for smoother plots)')
#     kwargs = vars(parser.parse_args())
#     run_test_and_plot(**kwargs)

----!@#$----
workspace.py
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
#


from __future__ import annotations

import numpy as np
import torch
from typing import Optional

""" This module provides different ways to store tensors that are more flexible than the torch.Tensor class.
It also defines the `Workspace` as a dictionary of tensors and a version of the workspace
where tensors are in shared memory for multiprocessing
"""


class SlicedTemporalTensor:
    """A SlicedTemporalTensor represents a tensor of size TxBx... by using a list of tensors of size Bx...
    The interest is that this tensor automatically adapts its timestep dimension and does not need to have a predefined size.
    """

    def __init__(self):
        """Initialize an empty tensor"""
        self.tensors: list[torch.Tensor] = []
        self.size: torch.Size = None
        self.device: torch.device = None
        self.dtype: torch.dtype = None

    def set(self, t: int, value: torch.Tensor, batch_dims: Optional[tuple[int, int]]):
        """Set a value (dim Bx...) at time t"""
        assert (
            batch_dims is None
        ), "Unable to use batch dimensions with SlicedTemporalTensor"
        if self.size is None:
            self.size = value.size()
            self.device = value.device
            self.dtype = value.dtype
        assert self.size == value.size(), (
            "Incompatible size:" + str(self.size) + " vs " + str(value.size())
        )
        assert self.device == value.device, "Incompatible device:" + str(self.device)
        assert self.dtype == value.dtype, f"Incompatible type ({self.dtype} vs {value.dtype})"
        while len(self.tensors) <= t:
            self.tensors.append(
                torch.zeros(*self.size, device=self.device, dtype=self.dtype)
            )
        self.tensors[t] = value

    def to(self, device: torch.device):
        """Move the tensor to a specific device"""
        s = SlicedTemporalTensor()
        for k in range(len(self.tensors)):
            s.set(k, self.tensors[k].to(device))
        return s

    def get(self, t: int, batch_dims: Optional[tuple[int, int]]):
        """Get the value of the tensor at time t"""

        assert (
            batch_dims is None
        ), "Unable to use batch dimensions with SlicedTemporalTensor"
        assert t < len(self.tensors), f"Temporal index out of bounds: {t} vs {len(self.tensors)}"
        return self.tensors[t]

    def get_full(self, batch_dims):
        """Returns the complete tensor of size TxBx..."""

        assert (
            batch_dims is None
        ), "Unable to use batch dimensions with SlicedTemporalTensor"
        return torch.cat([a.unsqueeze(0) for a in self.tensors], dim=0)

    def get_time_truncated(
        self, from_time: int, to_time: int, batch_dims: Optional[tuple[int, int]]
    ):
        """Returns tensor[from_time:to_time]"""
        assert from_time >= 0 and to_time >= 0 and to_time > from_time
        assert batch_dims is None
        return torch.cat(
            [
                self.tensors[k].unsqueeze(0)
                for k in range(from_time, min(len(self.tensors), to_time))
            ],
            dim=0,
        )

    def set_full(self, value: torch.Tensor, batch_dims: Optional[tuple[int, int]]):
        """Set the tensor given a BxTx... tensor.
        The input tensor is cut into slices that are stored in a list of tensors
        """
        assert (
            batch_dims is None
        ), "Unable to use batch dimensions with SlicedTemporalTensor"
        for t in range(value.size()[0]):
            self.set(t, value[t], batch_dims=batch_dims)

    def time_size(self):
        """
        Return the size of the time dimension
        """
        return len(self.tensors)

    def batch_size(self):
        """Return the size of the batch dimension"""
        return self.tensors[0].size()[0]

    def select_batch(self, batch_indexes: torch.LongTensor):
        """Return the tensor where the batch dimension has been selected by the index"""
        var = SlicedTemporalTensor()
        for t, v in enumerate(self.tensors):
            batch_indexes = batch_indexes.to(v.device)
            var.set(t, v[batch_indexes], None)
        return var

    def clear(self):
        """Clear the tensor"""
        self.tensors = []
        self.size = None
        self.device = None
        self.dtype = None

    def copy_time(self, from_time: int, to_time: int, n_steps: int):
        """Copy temporal slices of the tensor from from_time:from_time+n_steps to to_time:to_time+n_steps"""
        for t in range(n_steps):
            v = self.get(from_time + t, batch_dims=None)
            self.set(to_time + t, v, batch_dims=None)

    def subtime(self, from_t: int, to_t: int):
        """
        Return tensor[from_t:to_t]

        """
        return CompactTemporalTensor(
            torch.cat([a.unsqueeze(0) for a in self.tensors[from_t:to_t]], dim=0)
        )

    def zero_grad(self):
        """Clear any gradient information in the tensor"""
        self.tensors = [v.detach() for v in self.tensors]


class CompactTemporalTensor:
    """A CompactTemporalTensor is a tensor of size TxBx...
    It behaves like the `SlicedTemporalTensor` but has a fixed size that cannot change.
    It is faster than the SlicedTemporalTensor.
        See `SlicedTemporalTensor`
    """

    def __init__(self, value: torch.Tensor = None):
        self.size = None
        self.device = None
        self.dtype = None
        self.tensor = None
        if value is not None:
            self.tensor = value
            self.device = value.device
            self.size = value.size()
            self.dtype = value.dtype

    def set(self, t, value, batch_dims):
        assert False, "Should not be used"
        assert self.tensor is not None, "Tensor must be initialized"
        assert self.size[1:] == value.size(), "Incompatible size"
        assert self.device == value.device, "Incompatible device"
        assert self.dtype == value.dtype, "Incompatible type"
        assert t < self.tensor.size()[0], "Temporal index out of bounds: {t} vs {self.tensor.size()[0]}"
        if batch_dims is None:
            self.tensor[t] = value
        else:
            self.tensor[t, batch_dims[0] : batch_dims[1]] = value

    def select_batch(self, batch_indexes):
        v = CompactTemporalTensor(self.tensor[:, batch_indexes])
        return v

    def to_sliced(self) -> SlicedTemporalTensor:
        """Transform the tensor to a `SlicedTemporalTensor`"""
        v = SlicedTemporalTensor()
        for t in range(self.tensor.size()[0]):
            v.set(t, self.tensor[t], None)
        return v

    def to(self, device):
        if device == self.tensor.device:
            return self
        t = self.tensor.to(device)
        return CompactTemporalTensor(t)

    def get(self, t, batch_dims):
        assert t < self.tensor.size()[0], f"Temporal index out of bounds: {t} vs {self.tensor.size()[0]}"
        if batch_dims is None:
            return self.tensor[t]
        else:
            return self.tensor[t, batch_dims[0] : batch_dims[1]]

    def get_full(self, batch_dims):
        if batch_dims is None:
            return self.tensor
        else:
            return self.tensor[:, batch_dims[0] : batch_dims[1]]

    def time_size(self):
        return self.tensor.size()[0]

    def batch_size(self):
        return self.tensor.size()[1]

    def set_full(self, value, batch_dims):
        if self.tensor is None:
            assert batch_dims is None
            self.size = value.size()
            self.dtype = value.dtype
            self.device = value.device
        if batch_dims is None:
            self.tensor = value
        else:
            self.tensor[:, batch_dims[0] : batch_dims[1]] = value

    def subtime(self, from_t, to_t):
        return CompactTemporalTensor(self.tensor[from_t:to_t])

    def clear(self):
        self.size = None
        self.device = None
        self.dtype = None
        self.tensor = None

    def copy_time(self, from_time, to_time, n_steps):
        self.tensor[to_time : to_time + n_steps] = self.tensor[
            from_time : from_time + n_steps
        ]

    def zero_grad(self):
        self.tensor = self.tensor.detach()


class CompactSharedTensor:
    """It corresponds to a tensor in shared memory.
    It is used when building a workspace shared by multiple processes.
        All the methods behaves like the methods of `SlicedTemporalTensor`
    """

    def __init__(self, _tensor: torch.Tensor):
        self.tensor = _tensor
        self.tensor.share_memory_()

    def set(self, t, value, batch_dims):
        if batch_dims is None:
            self.tensor[t] = value.detach()
        else:
            self.tensor[t, batch_dims[0] : batch_dims[1]] = value.detach()

    def get(self, t, batch_dims):
        assert t < self.tensor.size()[0], f"Temporal index out of bounds: {t}, {self.tensor.size()[0]}"
        if batch_dims is None:
            return self.tensor[t]
        else:
            return self.tensor[t, batch_dims[0] : batch_dims[1]]

    def to(self, device):
        if device == self.tensor.device:
            return self
        t = self.tensor.to(device)
        t.share_memory_()
        return CompactSharedTensor(t)

    def select_batch(self, batch_indexes):
        v = CompactSharedTensor(self.tensor[:, batch_indexes])
        return v

    def get_full(self, batch_dims):
        if batch_dims is None:
            return self.tensor
        else:
            return self.tensor[:, batch_dims[0] : batch_dims[1]]

    def time_size(self):
        return self.tensor.size()[0]

    def batch_size(self):
        return self.tensor.size()[1]

    def set_full(self, value, batch_dims):
        if batch_dims is None:
            self.tensor = value.detach()
        else:
            self.tensor[:, batch_dims[0] : batch_dims[1]] = value.detach()

    def clear(self):
        assert False, "Cannot clear a shared tensor"

    def subtime(self, from_t, to_t):
        t = self.tensor[from_t:to_t]
        return CompactSharedTensor(t)

    def copy_time(self, from_time, to_time, n_steps):
        self.tensor[to_time : to_time + n_steps] = self.tensor[
            from_time : from_time + n_steps
        ]

    def zero_grad(self):
        pass


def take_per_row_strided(a, index, num_elem=2):
    # TODO: Optimize this function
    all_index = index
    arange = torch.arange(a.size()[1], device=a.device)
    return torch.cat(
        [a[all_index + t, arange].unsqueeze(0) for t in range(num_elem)], dim=0
    )


class Workspace:
    """Workspace is the most important class in `bbrl`.
    It correponds to a collection of tensors
    ('SlicedTemporalTensor`, `CompactTemporalTensor` or ` CompactSharedTensor`).
    In the majority of cases, we consider that all the tensors have the same time and batch sizes
    (but it is not mandatory for most of the functions)
    """

    def __init__(self, workspace: Optional[Workspace] = None):
        """Create an empty workspace

        Args:
            workspace (Workspace, optional): If specified, it creates a copy of the workspace
        (where tensors are cloned as CompactTemporalTensors)
        """
        self.variables = {}
        self.is_shared = False
        if workspace is not None:
            for k in workspace.keys():
                self.set_full(k, workspace[k].clone())

    def set(
        self,
        var_name: str,
        t: int,
        v: torch.Tensor,
        batch_dims: Optional[tuple[int, int]] = None,
    ):
        """Set the variable var_name at time t"""
        if var_name not in self.variables:
            assert not self.is_shared, "Cannot add new variable into a shared workspace"
            self.variables[var_name] = SlicedTemporalTensor()
        elif isinstance(self.variables[var_name], CompactTemporalTensor):
            self.variables[var_name] = self.variables[var_name].to_sliced()

        self.variables[var_name].set(t, v, batch_dims=batch_dims)

    def get(
        self, var_name: str, t: int, batch_dims: Optional[tuple[int, int]] = None
    ) -> torch.Tensor:
        """Get the variable var_name at time t"""
        assert var_name in self.variables, f"Unknown variable: {var_name}"
        return self.variables[var_name].get(t, batch_dims=batch_dims)

    def clear(self, name=None):
        """Remove all the variables from the workspace"""
        if name is None:
            for k, v in self.variables.items():
                v.clear()
            self.variables = {}
        else:
            self.variables[name].clear()
            del self.variables[name]

    def contiguous(self) -> Workspace:
        """Generates a workspace where all tensors are stored in the Compact format."""
        workspace = Workspace()
        for k in self.keys():
            workspace.set_full(k, self.get_full(k))
        return workspace

    def set_full(
        self,
        var_name: str,
        value: torch.Tensor,
        batch_dims: Optional[tuple[int, int]] = None,
    ):
        """Set variable var_name with a complete tensor (TxBx...) where T is the time dimension
        and B is the batch size
        """
        if var_name not in self.variables:
            assert not self.is_shared, "Cannot add new variable into a shared workspace"
            self.variables[var_name] = CompactTemporalTensor()
        self.variables[var_name].set_full(value, batch_dims=batch_dims)

    def get_full(
        self, var_name: str, batch_dims: Optional[tuple[int, int]] = None
    ) -> torch.Tensor:
        """Return the complete tensor for var_name"""
        assert var_name in self.variables, (
            f"[Workspace.get_full] unknown variable: {var_name}"
        )
        return self.variables[var_name].get_full(batch_dims=batch_dims)

    def keys(self):
        """Return an iterator over the variables names"""
        return self.variables.keys()

    def __getitem__(self, key):
        """If key is a string, then it returns a torch.Tensor
        If key is a list of string, it returns a tuple of torch.Tensor
        """
        if isinstance(key, str):
            return self.get_full(key, None)
        else:
            return (self.get_full(k, None) for k in key)

    def _all_variables_same_time_size(self) -> bool:
        """Check that all variables have the same time size"""
        _ts = None
        for k, v in self.variables.items():
            if _ts is None:
                _ts = v.time_size()
            if _ts != v.time_size():
                return False
        return True

    def time_size(self) -> int:
        """Return the time size of the variables in the workspace"""
        _ts = None
        for k, v in self.variables.items():
            if _ts is None:
                _ts = v.time_size()
            assert _ts == v.time_size(), f"Variables must have the same time size: {_ts} vs {v.time_size()}"
        return _ts

    def batch_size(self) -> int:
        """Return the batch size of the variables in the workspace"""
        _bs = None
        for k, v in self.variables.items():
            if _bs is None:
                _bs = v.batch_size()
            assert _bs == v.batch_size(), f"Variables must have the same batch size: {_bs} vs {v.batch_size()}"
        return _bs

    def select_batch(self, batch_indexes: torch.LongTensor) -> Workspace:
        """Given a tensor of indexes, returns a new workspace
        with the selected elements (over the batch dimension)
        """
        _bs = None
        for k, v in self.variables.items():
            if _bs is None:
                _bs = v.batch_size()
            assert _bs == v.batch_size(), f"Variables must have the same batch size: {_bs} vs {v.batch_size()}"

        workspace = Workspace()
        for k, v in self.variables.items():
            v = v.select_batch(batch_indexes)
            workspace.variables[k] = v
        return workspace

    def select_batch_n(self, n):
        """Return a new Workspace of batch_size n by randomly sampling over the batch dimensions"""
        who = torch.randint(low=0, high=self.batch_size(), size=(n,))
        return self.select_batch(who)

    def copy_time(
        self,
        from_time: int,
        to_time: int,
        n_steps: int,
        var_names: Optional[list[str]] = None,
    ):
        """Copy all the variables values from time `from_time` to `from_time+n_steps`
        to `to_time` to `to_time+n_steps`
        It can be restricted to specific variables using `var_names`.
        """
        for k, v in self.variables.items():
            if var_names is None or k in var_names:
                v.copy_time(from_time, to_time, n_steps)

    def get_time_truncated(
        self,
        var_name: str,
        from_time: int,
        to_time: int,
        batch_dims: Optional[tuple[int, int]] = None,
    ) -> torch.Tensor:
        """Return workspace[var_name][from_time:to_time]"""
        assert 0 <= from_time < to_time and to_time >= 0

        v = self.variables[var_name]
        if isinstance(v, SlicedTemporalTensor):
            return v.get_time_truncated(from_time, to_time, batch_dims)
        else:
            return v.get_full(batch_dims)[from_time:to_time]

    def get_time_truncated_workspace(self, from_time: int, to_time: int) -> Workspace:
        """Return a workspace where all variables are truncated between from_time and to_time"""
        workspace = Workspace()
        for k in self.keys():
            workspace.set_full(k, self.get_time_truncated(k, from_time, to_time, None))
        return workspace

    # Static function
    def cat_batch(self, workspaces: list[Workspace]) -> Workspace:
        """Concatenate multiple workspaces over the batch dimension.
        The workspaces must have the same time dimension.
        """

        ts = None
        for w in workspaces:
            if ts is None:
                ts = w.time_size()
            assert ts == w.time_size(), f"Workspaces must have the same time size: {ts} vs {w.time_size()}"

        workspace = Workspace()
        for k in workspaces[0].keys():
            vals = [w[k] for w in workspaces]
            v = torch.cat(vals, dim=1)
            workspace.set_full(k, v)
        return workspace

    def copy_n_last_steps(self, n: int, var_names: Optional[list[str]] = None) -> None:
        """Copy the n last timesteps of each variable to the n first timesteps."""
        _ts = None
        for k, v in self.variables.items():
            if var_names is None or k in var_names:
                if _ts is None:
                    _ts = v.time_size()
                assert _ts == v.time_size(), (
                    f"Variables must have the same time size: {_ts} vs {v.time_size()}"
                )

        for k, v in self.variables.items():
            if var_names is None or k in var_names:
                self.copy_time(_ts - n, 0, n)

    def zero_grad(self) -> None:
        """Remove any gradient information"""
        for k, v in self.variables.items():
            v.zero_grad()

    def to(self, device: torch.device) -> Workspace:
        """Return a workspace where all tensors are on a particular device"""
        workspace = Workspace()
        for k, v in self.variables.items():
            workspace.variables[k] = v.to(device)
        return workspace

    def _convert_to_shared_workspace(self, n_repeat=1, time_size=None):
        """INTERNAL METHOD.
        It converts a workspace to a shared workspace, by repeating this workspace n times on the batch dimension
        It also automatically adapts the time_size if specified (used in NRemoteAgent.create)
        """

        with torch.no_grad():
            workspace = Workspace()
            for k, v in self.variables.items():
                value = v.get_full(None).detach()
                if time_size is not None:
                    s = value.size()
                    value = torch.zeros(
                        time_size, *s[1:], dtype=value.dtype, device=value.device
                    )
                ts = [value for _ in range(n_repeat)]
                value = torch.cat(ts, dim=1)
                workspace.variables[k] = CompactSharedTensor(value)
                workspace.is_shared = True
        return workspace

    def subtime(self, from_t: int, to_t: int) -> Workspace:
        """
        Return a workspace restricted to a subset of the time dimension
        """
        assert (
            self._all_variables_same_time_size()
        ), "All variables must have the same time size"
        workspace = Workspace()
        for k, v in self.variables.items():
            workspace.variables[k] = v.subtime(from_t, to_t)
        return workspace

    def remove_variable(self, var_name: str):
        """Remove a variable from the Workspace"""
        del self.variables[var_name]

    def __str__(self):
        r = ["Workspace:"]
        for k, v in self.variables.items():
            r.append(
                "\t"
                + k
                + ": time_size = "
                + str(v.time_size())
                + ", batch_size = "
                + str(v.batch_size())
            )
        return "\n".join(r)

    def select_subtime(self, t: torch.LongTensor, window_size: int) -> Workspace:
        """
        `t` is a tensor of size `batch_size` that provides one time index for each element of the workspace.
        Then the function returns a new workspace by aggregating `window_size` timesteps starting from index `t`
        This method allows to sample multiple windows in the Workspace.
        Note that the function may be quite slow.
        """
        _vars = {k: v.get_full(batch_dims=None) for k, v in self.variables.items()}
        workspace = Workspace()
        for k, v in _vars.items():
            workspace.set_full(
                k, take_per_row_strided(v, t, num_elem=window_size), batch_dims=None
            )
        return workspace

    # Static
    def sample_subworkspace(self, n_times, n_batch_elements, n_timesteps):
        """Sample a workspace from the  workspace. The process is the following:
                * Let us consider that workspace batch_size is B and time_size is T
                * For n_times iterations:
                    * We sample a time window of size n_timesteps
                    * We then sample n_batch_elements elements on the batch size
                    * =>> we obtain a workspace of size n_batch_elements x n_timesteps
                * We concatenate all the workspaces collected (over the batch dimension)

        Args:
            n_times ([type]): The number of sub workspaces to sample (and concatenate)
            n_batch_elements ([type]): <=workspace.batch_size(): nb of batch elements to sample for each sub workspace
            n_timesteps ([type]): <=workspace.time_size(): the number of timesteps to keep

        Returns:
            [Workspace]: The resulting workspace
        """
        b = self.batch_size()
        t = self.time_size()
        to_aggregate = []
        for _ in range(n_times):
            assert not n_timesteps > t
            mini_workspace = self
            if n_timesteps < t:
                t = np.random.randint(t - n_timesteps)
                mini_workspace = self.subtime(t, t + n_timesteps)

            # Batch sampling
            if n_batch_elements < b:
                idx_envs = torch.randperm(b)[:n_batch_elements]
                mini_workspace = mini_workspace.select_batch(idx_envs)
            to_aggregate.append(mini_workspace)

        if len(to_aggregate) > 1:
            mini_workspace = Workspace.cat_batch(to_aggregate)
        else:
            mini_workspace = to_aggregate[0]
        return mini_workspace

    def get_transitions(
        self, no_final_state=False, filter_key: str = "env/done"
    ) -> Workspace:
        """Return a new workspace containing the transitions of the current workspace.
            Each key of the current workspace have dimensions [n_step, n_env, key_dim]
            {
                Key1 :
                    [
                        [step1, step1, step1], # for env 1,2,3 ...
                        [step2, step2, step2], # for env 1,2,3 ...
                        ...
                    ]
                ...

            }

            Return a workspace of transitions with the following structure :
            Each key of the returned workspace have dimensions [2, n_transitions, key_dim]
            key[0][0], key[1][0] = (step_1, step_2) # for env 1
            key[0][1], key[1][1] = (step_1, step_2) # for env 2
            key[0][2], key[1][2] = (step_2, step_3) # for env 1
            key[0][3], key[1][3] = (step_2, step_3) # for env 2
            ...

            Filters every transitions [step_final, step_initial] unless no_final_state is True

        Parameters:
            no_final_state: To be set to True if the workspace does not contain final states
        Returns:
            [Workspace]: The resulting workspace of transitions
        """

        # Prepares the transitions
        transitions = {}
        if no_final_state:
            # No need to filter transitions here
            for key in self.keys():
                array = self[key]
                x = array[:-1].view(-1, *array.shape[2:])
                x_next = array[1:].view(-1, *array.shape[2:])
                transitions[key] = torch.stack([x, x_next])
        else:
            done = self[filter_key][:-1]
            for key in self.keys():
                array = self[key]

                # remove transitions (s_terminal -> s_initial)
                x = array[:-1][~done]
                x_next = array[1:][~done]
                transitions[key] = torch.stack([x, x_next])

        # Fill up the workspace
        workspace = Workspace()
        for k, v in transitions.items():
            workspace.set_full(k, v)
        return workspace

    def debug_transitions(self, truncated):
        """ """
        critic, done, action_probs, reward, action = self[
            "critic", "env/done", "action_probs", "env/reward", "action"
        ]
        timestep = self["env/timestep"]
        assert not done[
            0
        ].max()  # dones is must be always false in the first timestep of the transition.
        # if not it means we have a transition (step final) => (step initial)

        # timesteps must always follow each other.
        assert (timestep[0] == timestep[1] - 1).all()

        assert (
            truncated[not done].sum().item() == 0
        )  # when done is false, truncated is always false

        if done[truncated].numel() > 0:
            assert torch.amin(
                done[truncated]
            )  # when truncated is true, done is always true
        assert reward[1].sum() == len(
            reward[1]
        ), "in cartpole, rewards are always 1"  # only 1 rewards


class _SplitSharedWorkspace:
    """This is a view over a Workspace, restricted to particular batch dimensions.
    It is used when multiple agents are reading/writing in the same workspace
    but for specific batch dimensions (see NRemoteAgent)
    """

    def __init__(self, workspace, batch_dims):
        self.workspace = workspace
        self.batch_dims = batch_dims
        self.is_shared = self.workspace.is_shared

    def set(self, var_name, t, v):
        self.workspace.set(var_name, t, v, batch_dims=self.batch_dims)

    def get(self, var_name, t):
        return self.workspace.get(var_name, t, batch_dims=self.batch_dims)

    def keys(self):
        return self.workspace.keys()

    def get_time_truncated(self, var_name, from_time, to_time):
        assert 0 <= from_time < to_time
        return self.workspace.get_time_truncated(
            var_name, from_time, to_time, batch_dims=self.batch_dims
        )

    def set_full(self, var_name, value):
        self.workspace.set_full(var_name, value, batch_dims=self.batch_dims)

    def get_full(self, var_name):
        return self.workspace.get_full(var_name, batch_dims=self.batch_dims)

----!@#$----
__init__.py
#
# Copyright (c) Sorbonne Universite
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
#

from ._version import __version__, __version_tuple__
from .agents.agent import Agent, TimeAgent, SerializableAgent
from .agents.seeding import SeedableAgent

trace_workspace = False
trace = []
trace_maximum_size = 10000


def instantiate_class(arguments):
    from importlib import import_module

    d = dict(arguments)
    classname = d["classname"]
    del d["classname"]
    module_path, class_name = classname.rsplit(".", 1)
    module = import_module(module_path)
    c = getattr(module, class_name)
    return c(**d)


def get_class(arguments):
    from importlib import import_module

    if isinstance(arguments, dict):
        classname = arguments["classname"]
        module_path, class_name = classname.rsplit(".", 1)
        module = import_module(module_path)
        c = getattr(module, class_name)
        return c
    else:
        classname = arguments.classname
        module_path, class_name = classname.rsplit(".", 1)
        # print(module_path)
        module = import_module(module_path)
        c = getattr(module, class_name)
        return c


def get_arguments(arguments):
    from importlib import import_module

    d = dict(arguments)
    if "classname" in d:
        del d["classname"]
    return d

----!@#$----
agents\agent.py
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
#

from typing import Any
import copy
import time
from abc import ABC
import pickle

import torch
import torch.nn as nn


class Agent(nn.Module):
    """An `Agent` is a `torch.nn.Module` that reads and writes into a
    `bbrl.Workspace`"""

    def __init__(self, name: str = None, verbose=False):
        """To create a new Agent

        Args:
            name ([type], optional): An agent can have a name that will allow to
            perform operations on agents that are composed into more complex
            agents.
        """
        super().__init__()
        self._name = name
        self.__trace_file = None
        self.verbose = verbose
        self.workspace = None
        self.prefix = ""

    def seed(self, seed: int):
        """Provide a seed to this agent. Useful is the agent is stochastic.

        Args:
            seed (str): [description]
        """
        pass

    def set_name(self, n):
        """Set the name of this agent

        Args:
            n (str): The name
        """
        self._name = n

    def get_name(self):
        """Returns the name of the agent

        Returns:
            str: the name
        """
        return self._name

    def with_prefix(self, prefix: str):
        """Returns the prefix in environments"""
        self.prefix = prefix
        return self

    def set_trace_file(self, filename):
        print("[TRACE]: Tracing agent in file " + filename)  # noqa: T201
        self.__trace_file = open(filename, "wt")

    def __call__(self, workspace, **kwargs):
        """Execute an agent of a `bbrl.Workspace`

        Args:
            workspace (bbrl.Workspace): the workspace on which the agent operates
        """
        assert workspace is not None, "[Agent.__call__] workspace must not be None"
        self.workspace = workspace
        self.forward(**kwargs)
        self.workspace = None

    def _asynchronous_call(self, workspace, **kwargs):
        """Execute the `__call__` in non-blocking mode (if the agent is in another process)

        Args:
             workspace (bbrl.Workspace): the workspace on which the agent operates
        """
        self.__call__(workspace, **kwargs)

    def is_running(self):
        """Returns True if the agent is currently executing (for remote agents)"""
        return False

    def forward(self, **kwargs):
        """The generic function to override when defining a new agent"""
        raise NotImplementedError

    def clone(self):
        """Create a clone of the agent

        Returns:
            bbrl.Agent: A clone
        """
        self.workspace = None
        self.zero_grad()
        return copy.deepcopy(self)

    def get(self, index):
        """Returns the value of a particular variable in the agent workspace

        Args:
            index (str or tuple(str,int)): if str, returns the variable workspace[str].
            If tuple(var_name,t), returns workspace[var_name] at time t
        """
        if self.__trace_file is not None:
            t = time.time()
            self.__trace_file.write(
                str(self) + " type = " + type(self) + " time = ",
                t,
                " get ",
                index,
                "\n",
            )
        if isinstance(index, str):
            return self.workspace.get_full(index)
        else:
            return self.workspace.get(index[0], index[1])

    def get_time_truncated(self, var_name, from_time, to_time):
        """Return a variable truncated between from_time and to_time"""
        return self.workspace.get_time_truncated(var_name, from_time, to_time)

    def set(self, index, value):
        """Write a variable in the workspace

        Args:
            index (str or tuple(str,int)):
            value (torch.Tensor): the value to write
        """
        if self.__trace_file is not None:
            t = time.time()
            self.__trace_file.write(
                str(self) + " type = " + type(self) + " time = ",
                t,
                " set ",
                index,
                " = ",
                value.size(),
                "/",
                value.dtype,
                "\n",
            )
        if isinstance(index, str):
            self.workspace.set_full(index, value)
        else:
            self.workspace.set(index[0], index[1], value)

    def get_by_name(self, n):
        """Returns the list of agents included in this agent that have a particular name."""
        if n == self._name:
            return [self]
        return []

    def save_model(self, filename) -> None:
        """
        Save a neural network model into a file
        :param filename: the filename, including the path
        :return: nothing
        """
        torch.save(self, filename)

    def load_model(self, filename) -> nn.Module:
        """
        Load a neural network model from a file
        :param filename: the filename, including the path
        :return: the resulting pytorch network
        """
        return torch.load(filename)


class TimeAgent(Agent, ABC):
    """
    `TimeAgent` is used as a convention to represent agents that
    use a time index in their `__call__` function (not mandatory)
    """

    def __init__(self, *args, **kwargs) -> None:
        super().__init__(*args, **kwargs)

    def forward(self, t: int, *args, **kwargs) -> Any:
        raise NotImplementedError(
            "Your TemporalAgent must override forward with a time index"
        )


class SerializableAgent(Agent, ABC):
    """
    `SerializableAgent` is used as a convention to represent agents that are serializable (not mandatory)
    You can override the serialize method to return the agent without the attributes that are not serializable.
    """

    def __init__(self, *args, **kwargs) -> None:
        super().__init__(*args, **kwargs)

    def serialize(self) -> "SerializableAgent":
        """
        Return the `SerializableAgent` without the unsersializable attributes
        """
        try:
            return self
        except Exception as e:
            raise NotImplementedError(
                "Could not serialize your {c} SerializableAgent because of {e}\n"
                "You have to override the serialize method".format(
                    c=self.__class__.__name__, e=e
                )
            )

    def save(self, filename: str) -> None:
        """Save the agent to a file

        Args:
            filename (str): The filename to use
        """
        try:
            with open(filename, "wb") as f:
                pickle.dump(self.serialize(), f, pickle.DEFAULT_PROTOCOL)
        except Exception as e:
            raise Exception(
                "Could not save agent to file {filename} because of {e} \n"
                "Make sure to have properly overriden the serialize method.".format(
                    filename=filename, e=e
                )
            )


def load(filename: str) -> Agent:
    """Load the agent from a file

    Args:
        filename (str): The filename to use

    Returns:
        bbrl2.Agent: The agent or a subclass of it
    """
    try:
        with open(filename, "rb") as f:
            return pickle.load(f)
    except Exception as e:
        raise Exception(
            "Could not load agent from file {filename} because of {e}".format(
                filename=filename, e=e
            )
        )

----!@#$----
agents\asynchronous.py
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
#

import torch
import torch.multiprocessing as mp

from bbrl.agents.agent import Agent
from bbrl.workspace import Workspace


def f(agent, in_queue, out_queue):
    while True:
        args = in_queue.get()
        if args == "exit":
            out_queue.put("ok")
            return
        workspace = Workspace()
        with torch.no_grad():
            agent(workspace, **args)
        out_queue.put("ok")
        for k in workspace.keys():
            out_queue.put((k, workspace.get_full(k)))
        out_queue.put("ok")


class AsynchronousAgent(Agent):
    """Implements an agent that is executed aynchronously in another process, and that returns its own workspace

    Usage is:
    * agent(workspace)
    * while agent.is_running():
    *     .....
    * workspace=agent.get_workspace()
    """

    def __init__(self, agent, verbose=False):
        super().__init__(verbose=verbose)
        """ Create the AsynchronousAgent

        Args:
            agent ([bbrl.Agent]): The agent to execute in another process
        """
        self._is_running = False
        self.process = None
        self._workspace = None
        self.agent = agent

    def __call__(self, **kwargs):
        """Executes the agent in non-blocking mode. A new workspace is created by the agent."""
        assert not self._is_running
        if self.process is None:
            self.o_queue = mp.Queue()
            self.o_queue.cancel_join_thread()
            self.i_queue = mp.Queue()
            self.i_queue.cancel_join_thread()
            self.process = mp.Process(
                target=f, args=(self.agent, self.i_queue, self.o_queue)
            )
            self.process.daemon = False
            self.process.start()
        self._is_running = True
        self.i_queue.put(kwargs)

    def is_running(self):
        """Is the agent still running ?

        Returns:
            [bool]: True is the agent is running
        """
        if self._is_running:
            try:
                r = self.o_queue.get(False)
                assert r == "ok"
                self._is_running = False
                r = self.o_queue.get()
                workspace = Workspace()
                while r != "ok":
                    key, val = r
                    workspace.set_full(key, val)
                    r = self.o_queue.get()
                self._workspace = workspace.to("cpu")
            except:
                pass  # TODO: shouldn't we assert False?
        return self._is_running

    def get_workspace(self):
        """Returns the built workspace is the agent has stopped its execution

        Returns:
            [bbrl.Workspace]: The built workspace
        """
        if self.is_running():
            return None
        return self._workspace

    def close(self):
        """Close the agent and kills the corresponding process"""
        if self.process is None:
            return

        if self.verbose:
            print("[AsynchronousAgent] closing process")
        self.i_queue.put("exit")
        self.o_queue.get()
        self.process.terminate()
        self.process.join()
        self.i_queue.close()
        self.o_queue.close()
        del self.i_queue
        del self.o_queue
        self.process = None

    def __del__(self):
        self.close()

----!@#$----
agents\dataloader.py
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
#

import torch
from bbrl.agents.agent import Agent
from typing import Any, Optional, Tuple
import numpy as np


RNG = RandomNumberGenerator = np.random.Generator


class ShuffledDatasetAgent(Agent):
    """An agent that read a dataset in a shuffle order, in an infinite way."""

    def __init__(self, dataset, batch_size, output_names=("x", "y")):
        """Create the agent

        Args:
            dataset ([torch.utils.data.Dataset]): the Dataset
            batch_size ([int]): The number of datapoints to write at each call
            output_names (tuple, optional): The name of the variables. Defaults to ("x", "y").
        """
        super().__init__()
        self.output_names = output_names
        self.dataset = dataset
        self.batch_size = batch_size
        self.ghost_params = torch.nn.Parameter(torch.randn(()))

    def seed(self, seed=None):
        assert not (
            seed is not None and not (isinstance(seed, int) and 0 <= seed)
        ), f"Seed must be a non-negative integer or omitted, not {seed}"

        seed_seq = np.random.SeedSequence(seed)
        np_seed = seed_seq.entropy
        self.np_random = RandomNumberGenerator(np.random.PCG64(seed_seq))
        return [np_seed]

    def forward(self, **kwargs):
        """Write a batch of data at timestep==0 in the workspace"""
        vs = []
        for k in range(self.batch_size):
            idx = self.np_random.randint(len(self.dataset))
            x = self.dataset[idx]
            xs = []
            for xx in x:
                if isinstance(xx, torch.Tensor):
                    xs.append(xx.unsqueeze(0))
                else:
                    xs.append(torch.tensor(xx).unsqueeze(0))
            vs.append(xs)

        vals = []
        for k in range(len(vs[0])):
            val = [v[k] for v in vs]
            val = torch.cat(val, dim=0)
            vals.append(val)

        for name, value in zip(self.output_names, vals):
            self.set((name, 0), value.to(self.ghost_params.device))


class DataLoaderAgent(Agent):
    """An agent based on a DataLoader that read a single dataset
    Usage is: agent.forward(), then one has to check if agent.finished() is True or Not. If True, then no data have been written in the workspace since the reading of the daaset is terminated
    """

    def __init__(self, dataloader, output_names=("x", "y")):
        """Create the agent based on a dataloader

        Args:
            dataloader ([DataLader]): The underlying pytoch daaloader object
            output_names (tuple, optional): Names of the variable to write in the workspace. Defaults to ("x", "y").
        """
        super().__init__()
        self.dataloader = dataloader
        self.iter = iter(self.dataloader)
        self.output_names = output_names
        self._finished = False
        self.ghost_params = torch.nn.Parameter(torch.randn(()))

    def reset(self):
        self.iter = iter(self.dataloader)
        self._finished = False

    def finished(self):
        return self._finished

    def forward(self, **kwargs):
        try:
            output_values = next(self.iter)
        except StopIteration:
            self.iter = None
            self._finished = True
        else:
            for name, value in zip(self.output_names, output_values):
                self.set((name, 0), value.to(self.ghost_params.device))

----!@#$----
agents\gyma.py
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
#

import numpy as np
import copy
import torch
from typing import Tuple, Union
import gym
from bbrl.agents.agent import Agent


def _convert_action(action):
    if len(action.size()) == 0:
        action = action.item()
        assert isinstance(action, int)
    else:
        action = np.array(action.tolist())
    return action


def _format_frame(frame):
    if isinstance(frame, dict):
        r = {}
        for k in frame:
            r[k] = _format_frame(frame[k])
        return r
    elif isinstance(frame, list):
        t = torch.tensor(frame).unsqueeze(0)
        if t.dtype == torch.float64 or t.dtype == torch.float32:
            t = t.float()
        else:
            t = t.long()
        return t
    elif isinstance(frame, np.ndarray):
        t = torch.from_numpy(frame).unsqueeze(0)
        if t.dtype == torch.float64 or t.dtype == torch.float32:
            t = t.float()
        else:
            t = t.long()
        return t
    elif isinstance(frame, torch.Tensor):
        return frame.unsqueeze(0)  # .float()
    elif isinstance(frame, bool):
        return torch.tensor([frame]).bool()
    elif isinstance(frame, int):
        return torch.tensor([frame]).long()
    elif isinstance(frame, float):
        return torch.tensor([frame]).float()

    else:
        try:
            # Check if it is a LazyFrame from OpenAI Baselines
            o = torch.from_numpy(frame.__array__()).unsqueeze(0).float()
            return o
        except TypeError:
            assert False


def _torch_type(d):
    nd = {}
    for k in d:
        if d[k].dtype == torch.float64:
            nd[k] = d[k].float()
        else:
            nd[k] = d[k]
    return nd


def _torch_cat_dict(d):
    r = {}
    for k in d[0]:
        r[k] = torch.cat([dd[k] for dd in d], dim=0)
    return r


class GymAgent(Agent):
    """Create an Agent from a gym environment"""

    def __init__(
        self,
        make_env_fn=None,
        make_env_args={},
        n_envs=None,
        seed=None,
        action_string="action",
        output="env/",
    ):
        """Create an agent from a Gym environment

        Args:
            make_env_fn ([function that returns a gym.Env]): The function to create a single gym environments
            make_env_args (dict): The arguments of the function that creates a gym.Env
            n_envs ([int]): The number of environments to create.
            action_string (str, optional): [the name of the action variable in the workspace]. Defaults to "action".
            output (str, optional): [the output prefix of the environment]. Defaults to "env/".
            seed (int): the seed used to initialize the environment
            and each environment will have its own seed]. Defaults to True.
        """
        super().__init__()
        assert n_envs > 0
        self.envs = None
        self.env_args = make_env_args
        self._seed = seed
        assert self._seed is not None, "[GymAgent] seeds must be specified"

        self.n_envs = n_envs
        self.output = output
        self.input = action_string
        self.make_env_fn = make_env_fn
        self.ghost_params = torch.nn.Parameter(torch.randn(()))
        self.timestep = torch.tensor([0 for _ in range(n_envs)])
        self.finished = torch.tensor([True for _ in range(n_envs)])
        self.truncated = torch.tensor([False for _ in range(n_envs)])

        self.envs = [self.make_env_fn(**self.env_args) for _ in range(self.n_envs)]
        for k in range(self.n_envs):
            self.envs[k].seed(self._seed + k)

        self.observation_space = self.envs[0].observation_space
        self.action_space = self.envs[0].action_space
        self.finished = torch.tensor([True for _ in self.envs])
        self.truncated = torch.tensor([True for _ in self.envs])
        self.timestep = torch.tensor([0 for _ in self.envs])
        self.cumulated_reward = {}
        self.last_frame = {}

    def _common_reset(self, k, save_render, render):
        env = self.envs[k]
        self.cumulated_reward[k] = 0.0
        o = env.reset()
        observation = _format_frame(o)

        if isinstance(observation, torch.Tensor):
            observation = {"env_obs": observation}
        else:
            assert isinstance(observation, dict)
        if save_render:
            image = env.render(mode="image").unsqueeze(0)
            observation["rendering"] = image
        elif render:
            env.render(mode="human")

        self.finished[k] = False
        self.truncated[k] = False
        self.timestep[k] = 0

        ret = {
            **observation,
            "done": torch.tensor([False]),
            "truncated": torch.tensor([False]),
            "reward": torch.tensor([0.0]).float(),
            "timestep": torch.tensor([self.timestep[k]]),
            "cumulated_reward": torch.tensor([self.cumulated_reward[k]]).float(),
        }
        return _torch_type(ret), observation

    def _reset(self, k, save_render, render):
        ret, observation = self._common_reset(k, save_render, render)
        self.last_frame[k] = observation
        return ret

    def _make_step(self, env, action, k, save_render, render):
        action = _convert_action(action)

        obs, reward, done, info = env.step(action)
        if "TimeLimit.truncated" in info.keys():
            truncated = info["TimeLimit.truncated"]
        else:
            truncated = False
        self.cumulated_reward[k] += reward
        observation = _format_frame(obs)
        if isinstance(observation, torch.Tensor):
            observation = {"env_obs": observation}
        else:
            assert isinstance(observation, dict)
        if save_render:
            image = env.render(mode="image").unsqueeze(0)
            observation["rendering"] = image
        elif render:
            env.render(mode="human")
        ret = {
            **observation,
            "done": torch.tensor([done]),
            "truncated": torch.tensor([truncated]),
            "reward": torch.tensor([reward]).float(),
            "cumulated_reward": torch.tensor([self.cumulated_reward[k]]),
            "timestep": torch.tensor([self.timestep[k]]),
        }
        return _torch_type(ret), done, truncated, observation

    def _step(self, k, action, save_render, render):
        if self.finished[k]:
            assert k in self.last_frame
            return {
                **self.last_frame[k],
                "done": torch.tensor([True]),
                "truncated": torch.tensor([self.truncated[k]]),
                "reward": torch.tensor([0.0]).float(),
                "cumulated_reward": torch.tensor([self.cumulated_reward[k]]).float(),
                "timestep": torch.tensor([self.timestep[k]]),
            }
        self.timestep[k] += 1
        ret, done, truncated, observation = self._make_step(
            self.envs[k], action, k, save_render, render
        )

        self.last_frame[k] = observation
        if done:
            self.finished[k] = True
            self.truncated[k] = truncated
        return ret

    def set_obs(self, observations, t):
        observations = _torch_cat_dict(observations)
        for k in observations:
            self.set((self.output + k, t), observations[k].to(self.ghost_params.device))

    def forward(self, t=0, save_render=False, render=False, **kwargs):
        """Do one step by reading the `action` at t-1
        If t==0, environments are reset
        If save_render is True, then the output of env.render(mode="image") is written as env/rendering
        """

        if t == 0:
            self.timestep = torch.tensor([0 for _ in self.envs])
            observations = []
            for k, e in enumerate(self.envs):
                obs = self._reset(k, save_render, render)
                observations.append(obs)
            observations = _torch_cat_dict(observations)
            for k in observations:
                self.set(
                    (self.output + k, t), observations[k].to(self.ghost_params.device)
                )
        else:
            assert t > 0
            action = self.get((self.input, t - 1))
            assert action.size()[0] == self.n_envs, "Incompatible number of envs"
            observations = []
            for k, e in enumerate(self.envs):
                obs = self._step(k, action[k], save_render, render)
                observations.append(obs)
            self.set_obs(observations, t)

    def is_continuous_action(self):
        return isinstance(self.action_space, gym.spaces.Box)

    def is_discrete_action(self):
        return isinstance(self.action_space, gym.spaces.Discrete)

    def is_continuous_state(self):
        return isinstance(self.observation_space, gym.spaces.Box)

    def is_discrete_state(self):
        return isinstance(self.observation_space, gym.spaces.Discrete)

    def get_obs_and_actions_sizes(self):
        action_dim = 0
        state_dim = 0
        if self.is_continuous_action():
            action_dim = self.action_space.shape[0]
        elif self.is_discrete_action():
            action_dim = self.action_space.n
        if self.is_continuous_state():
            state_dim = self.observation_space.shape[0]
        elif self.is_discrete_state():
            state_dim = 1  # self.observation_space.n
        return state_dim, action_dim


class AutoResetGymAgent(GymAgent):
    """The same as GymAgent, but with an automatic reset when done is True"""

    def __init__(
        self,
        make_env_fn=None,
        make_env_args={},
        n_envs=None,
        seed=None,
        action_string="action",
        output="env/",
    ):
        """Create an agent from a Gym environment with Autoreset

        Args:
            make_env_fn ([function that returns a gym.Env]): The function to create a single gym environments
            make_env_args (dict): [The arguments of the function that creates a gym.Env]
            n_envs ([int]): [The number of environments to create].
            seed (int, optional): [the seed used to initialize the environment].
            action_string (str, optional): [the name of the action variable in the workspace]. Defaults to "action".
            output (str, optional): [the output prefix of the environment]. Defaults to "env/".
        """
        super().__init__(
            make_env_fn=make_env_fn,
            make_env_args=make_env_args,
            n_envs=n_envs,
            seed=seed,
            action_string=action_string,
            output=output,
        )
        self.is_running = [False for _ in range(self.n_envs)]

    def _reset(self, k, save_render, render):
        self.is_running[k] = True
        ret, _ = self._common_reset(k, save_render, render)
        return ret

    def _step(self, k, action, save_render, render):
        self.timestep[k] += 1
        ret, done, truncated, _ = self._make_step(
            self.envs[k], action, k, save_render, render
        )
        if done:
            self.is_running[k] = False
            self.truncated[k] = truncated
        return ret

    def forward(self, t=0, save_render=False, render=False, **kwargs):
        """
        Perform one step by reading the `action`
        """

        observations = []
        for k, env in enumerate(self.envs):
            if not self.is_running[k] or t == 0:
                observations.append(self._reset(k, save_render, render))
            else:
                assert t > 0
                action = self.get((self.input, t - 1))
                assert action.size()[0] == self.n_envs, "Incompatible number of envs"
                observations.append(self._step(k, action[k], save_render, render))

        self.set_obs(observations, t)


class NoAutoResetGymAgent(GymAgent):
    """The same as GymAgent, named to make sure it is not AutoReset"""

    def __init__(
        self,
        make_env_fn=None,
        make_env_args={},
        n_envs=None,
        seed=None,
        action_string="action",
        output="env/",
    ):
        super().__init__(
            make_env_fn=make_env_fn,
            make_env_args=make_env_args,
            n_envs=n_envs,
            seed=seed,
            action_string=action_string,
            output=output,
        )


# --------------------------- part added from the code of Folco Bertini and Nikola Matevski --------------------


class RunningMeanStd:
    def __init__(self, epsilon: float = 1e-4, shape: Tuple[int, ...] = ()):
        """
        Calulates the running mean and std of a data stream
        https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance
        :param epsilon: helps with arithmetic issues
        :param shape: the shape of the data stream's output
        """
        self.mean = torch.zeros(shape, dtype=torch.float32)
        self.var = torch.ones(shape, dtype=torch.float32)
        self.count = epsilon
        self.steps = 0
        self.update_threshold = 4
        self.buffer = None

    def copy(self) -> "RunningMeanStd":
        """
        :return: Return a copy of the current object.
        """
        new_object = RunningMeanStd(shape=self.mean.shape)
        new_object.mean = copy.deepcopy(self.mean)
        new_object.var = copy.deepcopy(self.var)
        new_object.count = float(self.count)
        return new_object

    def combine(self, other: "RunningMeanStd") -> None:
        """
        Combine stats from another ``RunningMeanStd`` object.
        :param other: The other object to combine with.
        """
        self.update_from_moments(other.mean, other.var, other.count)

    def update(self, arr: torch.Tensor) -> None:
        self.steps += 1
        if self.buffer is None:
            self.buffer = copy.deepcopy(arr)
        else:
            self.buffer = torch.concat((self.buffer, copy.deepcopy(arr)), dim=0)
        if self.steps > self.update_threshold:
            batch_mean = torch.mean(self.buffer, dim=0)
            batch_var = torch.var(self.buffer, dim=0)
            batch_count = self.buffer.size(0)
            self.steps = 0
            self.buffer = None
            self.update_from_moments(batch_mean, batch_var, batch_count)

    def update_from_moments(
        self,
        batch_mean: torch.Tensor,
        batch_var: torch.Tensor,
        batch_count: Union[int, float],
    ) -> None:
        delta = batch_mean - self.mean
        tot_count = self.count + batch_count

        new_mean = self.mean + delta * batch_count / tot_count
        m_a = self.var * self.count
        m_b = batch_var * batch_count
        m_2 = (
            m_a
            + m_b
            + torch.square(delta)
            * self.count
            * batch_count
            / (self.count + batch_count)
        )
        new_var = m_2 / (self.count + batch_count)

        new_count = batch_count + self.count

        self.mean = new_mean
        self.var = new_var
        self.count = new_count


# not checked
class NormalizedNoAutoResetGymAgent(NoAutoResetGymAgent):
    """The same as AutoResetGymAgent, but normalizes observations"""

    def __init__(
        self,
        make_env_fn=None,
        make_env_args={},
        n_envs=None,
        seed=None,
        action_string="action",
        output="env/",
        obs_rms=None,
    ):
        super().__init__(
            make_env_fn=make_env_fn,
            make_env_args=make_env_args,
            n_envs=n_envs,
            seed=seed,
            action_string=action_string,
            output=output,
        )
        self.clip_obs = 10.0
        self.epsilon = 1e-08
        self.obs_rms = obs_rms

    def normalize_obs(self, obs: np.ndarray) -> np.ndarray:
        """
        Helper to normalize observation.
        :param obs:
        :return: normalized observation
        """
        obs_ = copy.deepcopy(obs)
        obs_rms = self.obs_rms
        return np.clip(
            (obs_ - obs_rms.mean) / np.sqrt(obs_rms.var + self.epsilon),
            -self.clip_obs,
            self.clip_obs,
        )

    def _reset(self, k, save_render, render):
        full_obs, observation = self._common_reset(k, save_render, render)
        self.last_frame[k] = observation

        full_obs["env_obs"] = self.normalize_obs(full_obs["env_obs"])

        return full_obs

    def _step(self, k, action, save_render, render):
        if self.finished[k]:
            assert k in self.last_frame
            rew = _torch_type({"reward": torch.tensor([0.0]).float()})
            return (
                {
                    **self.last_frame[k],
                    "done": torch.tensor([True]),
                    "truncated": torch.tensor([self.truncated[k]]),
                    "cumulated_reward": torch.tensor(
                        [self.cumulated_reward[k]]
                    ).float(),
                    "timestep": torch.tensor([self.timestep[k]]),
                },
                rew,
            )
        self.timestep[k] += 1
        ret, done, truncated, observation = self._make_step(
            self.envs[k], action, k, save_render, render
        )

        self.last_frame[k] = observation
        if done:
            self.finished[k] = True
            self.truncated[k] = truncated

        ret["env_obs"] = self.normalize_obs(ret["env_obs"])
        return ret


# not checked
class NormalizedAutoResetGymAgent(AutoResetGymAgent):
    """The same as AutoResetGymAgent, but normalizes observations"""

    def __init__(
        self,
        make_env_fn=None,
        make_env_args={},
        n_envs=None,
        seed=None,
        action_string="action",
        output="env/",
    ):
        super().__init__(
            make_env_fn=make_env_fn,
            make_env_args=make_env_args,
            n_envs=n_envs,
            seed=seed,
            action_string=action_string,
            output=output,
        )
        self.clip_obs = 10.0
        self.epsilon = 1e-08
        self.obs_rms = RunningMeanStd(shape=self.observation_space.shape)

    def normalize_obs(self, obs: np.ndarray) -> np.ndarray:
        """
        Helper to normalize observation.
        :param obs:
        :return: normalized observation
        """
        obs_ = copy.deepcopy(obs)
        obs_rms = self.obs_rms

        return np.clip(
            (obs_ - obs_rms.mean) / np.sqrt(obs_rms.var + self.epsilon),
            -self.clip_obs,
            self.clip_obs,
        )

    def _reset(self, k, save_render, render):
        self.is_running[k] = True
        full_obs, _ = self._common_reset(k, save_render, render)

        self.obs_rms.update(full_obs["env_obs"])
        full_obs["env_obs"] = self.normalize_obs(full_obs["env_obs"])
        return full_obs

    def _step(self, k, action, save_render, render):
        self.timestep[k] += 1
        ret, done, truncated, _ = self._make_step(
            self.envs[k], action, k, save_render, render
        )
        if done:
            self.is_running[k] = False
            self.truncated[k] = truncated

        self.obs_rms.update(ret["env_obs"])
        ret["env_obs"] = self.normalize_obs(ret["env_obs"])

        return ret

----!@#$----
agents\gymb.py
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
#

import numpy as np
import torch
import copy
import gym
from bbrl.agents.agent import Agent
from typing import Tuple, Union


def _convert_action(action):
    if len(action.size()) == 0:
        action = action.item()
        assert isinstance(action, int)
    else:
        action = np.array(action.tolist())
    return action


def _format_frame(frame):
    if isinstance(frame, dict):
        r = {}
        for k in frame:
            r[k] = _format_frame(frame[k])
        return r
    elif isinstance(frame, list):
        t = torch.tensor(frame).unsqueeze(0)
        if t.dtype == torch.float64 or t.dtype == torch.float32:
            t = t.float()
        else:
            t = t.long()
        return t
    elif isinstance(frame, np.ndarray):
        t = torch.from_numpy(frame).unsqueeze(0)
        if t.dtype == torch.float64 or t.dtype == torch.float32:
            t = t.float()
        else:
            t = t.long()
        return t
    elif isinstance(frame, torch.Tensor):
        return frame.unsqueeze(0)  # .float()
    elif isinstance(frame, bool):
        return torch.tensor([frame]).bool()
    elif isinstance(frame, int):
        return torch.tensor([frame]).long()
    elif isinstance(frame, float):
        return torch.tensor([frame]).float()

    else:
        try:
            # Check if it is a LazyFrame from OpenAI Baselines
            o = torch.from_numpy(frame.__array__()).unsqueeze(0).float()
            return o
        except TypeError:
            assert False


def _torch_type(d):
    nd = {}
    for k in d:
        if d[k].dtype == torch.float64:
            nd[k] = d[k].float()
        else:
            nd[k] = d[k]
    return nd


def _torch_cat_dict(d):
    r = {}
    for k in d[0]:
        a = [dd[k] for dd in d]
        r[k] = torch.cat(a, dim=0)
    return r


class GymAgent(Agent):
    """Create an Agent from a gym environment"""

    def __init__(
        self,
        make_env_fn=None,
        make_env_args={},
        n_envs=None,
        seed=None,
        action_string="action",
        output="env/",
    ):
        """Create an agent from a Gym environment

        Args:
            make_env_fn ([function that returns a gym.Env]): The function to create a single gym environments
            make_env_args (dict): The arguments of the function that creates a gym.Env
            n_envs ([int]): The number of environments to create.
            action_string (str, optional): [the name of the action variable in the workspace]. Defaults to "action".
            output (str, optional): [the output prefix of the environment]. Defaults to "env/".
            seed (int): the seed used to initialize the environment
        """
        super().__init__()
        assert n_envs > 0
        self.envs = None
        self.env_args = make_env_args
        self._seed = seed
        assert self._seed is not None, "[GymAgent] seeds must be specified"

        self.n_envs = n_envs
        self.output = output
        self.input = action_string
        self.make_env_fn = make_env_fn
        self.ghost_params = torch.nn.Parameter(torch.randn(()))
        self.timestep = torch.tensor([0 for _ in range(n_envs)])
        self.finished = torch.tensor([True for _ in range(n_envs)])
        self.truncated = torch.tensor([False for _ in range(n_envs)])

        self.envs = [self.make_env_fn(**self.env_args) for _ in range(self.n_envs)]
        for k in range(self.n_envs):
            self.envs[k].seed(self._seed + k)

        self.observation_space = self.envs[0].observation_space
        self.action_space = self.envs[0].action_space
        self.finished = torch.tensor([True for _ in self.envs])
        self.truncated = torch.tensor([True for _ in self.envs])
        self.timestep = torch.tensor([0 for _ in self.envs])
        self.cumulated_reward = {}
        self.last_frame = {}

    def _common_reset(self, k, save_render, render):
        env = self.envs[k]
        self.cumulated_reward[k] = 0.0
        o = env.reset()
        observation = _format_frame(o)

        if isinstance(observation, torch.Tensor):
            observation = {"env_obs": observation}

        else:
            assert isinstance(observation, dict)
        if save_render:
            image = env.render(mode="rgb_array")
            # print("image in reset", image)
            # image = image.unsqueeze(0)
            observation["rendering"] = image
        elif render:
            env.render(mode="human")

        self.finished[k] = False
        self.truncated[k] = False
        self.timestep[k] = 0

        ret = {
            **observation,
            "done": torch.tensor([False]),
            "truncated": torch.tensor([False]),
            "timestep": torch.tensor([self.timestep[k]]),
            "cumulated_reward": torch.tensor([0.0]).float(),
        }
        return _torch_type(ret), observation

    def _reset(self, k, save_render, render):
        full_obs, observation = self._common_reset(k, save_render, render)
        self.last_frame[k] = observation
        return full_obs

    def _make_step(self, env, action, k, save_render, render):
        action = _convert_action(action)

        obs, reward, done, info = env.step(action)
        if "TimeLimit.truncated" in info.keys():
            truncated = info["TimeLimit.truncated"]
        else:
            truncated = False
        self.cumulated_reward[k] += reward
        observation = _format_frame(obs)
        if isinstance(observation, torch.Tensor):
            observation = {"env_obs": observation}
        else:
            assert isinstance(observation, dict)
        if save_render:
            image = env.render(mode="rgb_array")
            # print("image in reset", image)
            # image = image.unsqueeze(0)
            observation["rendering"] = image
        elif render:
            env.render(mode="human")
        ret = {
            **observation,
            "done": torch.tensor([done]),
            "truncated": torch.tensor([truncated]),
            "cumulated_reward": torch.tensor([self.cumulated_reward[k]]),
            "timestep": torch.tensor([self.timestep[k]]),
        }
        rew = _torch_type({"reward": torch.tensor([reward]).float()})
        return _torch_type(ret), rew, done, truncated, observation

    def _step(self, k, action, save_render, render):
        if self.finished[k]:
            assert k in self.last_frame
            rew = _torch_type({"reward": torch.tensor([0.0]).float()})
            return (
                {
                    **self.last_frame[k],
                    "done": torch.tensor([True]),
                    "truncated": torch.tensor([self.truncated[k]]),
                    "cumulated_reward": torch.tensor(
                        [self.cumulated_reward[k]]
                    ).float(),
                    "timestep": torch.tensor([self.timestep[k]]),
                },
                rew,
            )
        self.timestep[k] += 1
        full_obs, reward, done, truncated, observation = self._make_step(
            self.envs[k], action, k, save_render, render
        )

        self.last_frame[k] = observation
        if done:
            self.finished[k] = True
            self.truncated[k] = truncated
        return full_obs, reward

    def set_obs(self, observations, t):
        observations = _torch_cat_dict(observations)
        for k in observations:
            self.set((self.output + k, t), observations[k].to(self.ghost_params.device))

    def set_next_obs(self, observations, t):
        observations = _torch_cat_dict(observations)
        for k in observations:
            self.set(
                ("env/env_next_obs" + k, t),
                observations[k].to(self.ghost_params.device),
            )

    def set_reward(self, rewards, t):
        rewards = _torch_cat_dict(rewards)
        for k in rewards:
            self.set((self.output + k, t), rewards[k].to(self.ghost_params.device))

    def forward(self, t=0, save_render=False, render=False, **kwargs):
        """Do one step by reading the `action` at t-1
        If t==0, environments are reset
        If save_render is True, then the output of env.render(mode="image") is written as env/rendering
        """

        if t == 0:
            self.timestep = torch.tensor([0 for _ in self.envs])
            observations = []
            for k, e in enumerate(self.envs):
                obs = self._reset(k, save_render, render)
                observations.append(obs)
            self.set_obs(observations, t)
        else:
            assert t > 0
            action = self.get((self.input, t - 1))
            assert action.size()[0] == self.n_envs, "Incompatible number of envs"
            observations = []
            rewards = []
            for k, e in enumerate(self.envs):
                obs, reward = self._step(k, action[k], save_render, render)
                observations.append(obs)
                rewards.append(reward)
            self.set_reward(rewards, t - 1)
            self.set_reward(rewards, t)
            self.set_obs(observations, t)

    def is_continuous_action(self):
        return isinstance(self.action_space, gym.spaces.Box)

    def is_discrete_action(self):
        return isinstance(self.action_space, gym.spaces.Discrete)

    def is_continuous_state(self):
        return isinstance(self.observation_space, gym.spaces.Box)

    def is_discrete_state(self):
        return isinstance(self.observation_space, gym.spaces.Discrete)

    def get_obs_and_actions_sizes(self):
        action_dim = 0
        state_dim = 0
        if self.is_continuous_action():
            action_dim = self.action_space.shape[0]
        elif self.is_discrete_action():
            action_dim = self.action_space.n
        if self.is_continuous_state():
            state_dim = self.observation_space.shape[0]
        elif self.is_discrete_state():
            state_dim = 1  # self.observation_space.n
        return state_dim, action_dim


class AutoResetGymAgent(GymAgent):
    """The same as GymAgent, but with an automatic reset when done is True"""

    def __init__(
        self,
        make_env_fn=None,
        make_env_args={},
        n_envs=None,
        seed=None,
        action_string="action",
        output="env/",
    ):
        """Create an agent from a Gym environment  with Autoreset

        Args:
            make_env_fn ([function that returns a gym.Env]): The function to create a single gym environments
            make_env_args (dict): The arguments of the function that creates a gym.Env
            n_envs ([int]): The number of environments to create.
            seed (int, optional): [the seed used to initialize the environment].
            action_string (str, optional): [the name of the action variable in the workspace]. Defaults to "action".
            output (str, optional): [the output prefix of the environment]. Defaults to "env/".
        """
        super().__init__(
            make_env_fn=make_env_fn,
            make_env_args=make_env_args,
            n_envs=n_envs,
            seed=seed,
            action_string=action_string,
            output=output,
        )
        self.is_running = [False for _ in range(self.n_envs)]
        self.previous_reward = [0 for _ in range(self.n_envs)]

    def _reset(self, k, save_render, render):
        self.is_running[k] = True
        full_obs, _ = self._common_reset(k, save_render, render)
        return full_obs

    def _step(self, k, action, save_render, render):
        self.timestep[k] += 1
        full_obs, reward, done, truncated, _ = self._make_step(
            self.envs[k], action, k, save_render, render
        )
        if done:
            self.is_running[k] = False
            self.truncated[k] = truncated
        return full_obs, reward

    def forward(self, t=0, save_render=False, render=False, **kwargs):
        """
        Perform one step by reading the `action`
        """

        observations = []
        rewards = []
        for k, env in enumerate(self.envs):
            if not self.is_running[k] or t == 0:
                observations.append(self._reset(k, save_render, render))

                if t > 0:
                    rew = self.previous_reward[k]
                    rewards.append(rew)
            else:
                assert t > 0
                action = self.get((self.input, t - 1))
                assert action.size()[0] == self.n_envs, "Incompatible number of envs"
                full_obs, reward = self._step(k, action[k], save_render, render)
                self.previous_reward[k] = reward
                observations.append(full_obs)
                rewards.append(reward)

        if t > 0:
            self.set_reward(rewards, t - 1)
            self.set_reward(rewards, t)
        self.set_obs(observations, t)


class NoAutoResetGymAgent(GymAgent):
    """The same as GymAgent, named to make sure it is not AutoReset"""

    def __init__(
        self,
        make_env_fn=None,
        make_env_args={},
        n_envs=None,
        seed=None,
        action_string="action",
        output="env/",
    ):
        super().__init__(
            make_env_fn=make_env_fn,
            make_env_args=make_env_args,
            n_envs=n_envs,
            seed=seed,
            action_string=action_string,
            output=output,
        )


# --------------------------- part added from the code of Folco Bertini and Nikola Matevski --------------------


class RunningMeanStd:
    def __init__(self, epsilon: float = 1e-4, shape: Tuple[int, ...] = ()):
        """
        Calulates the running mean and std of a data stream
        https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance
        :param epsilon: helps with arithmetic issues
        :param shape: the shape of the data stream's output
        """
        self.mean = torch.zeros(shape, dtype=torch.float32)
        self.var = torch.ones(shape, dtype=torch.float32)
        self.count = epsilon
        self.steps = 0
        self.update_threshold = 4
        self.buffer = None

    def copy(self) -> "RunningMeanStd":
        """
        :return: Return a copy of the current object.
        """
        new_object = RunningMeanStd(shape=self.mean.shape)
        new_object.mean = copy.deepcopy(self.mean)
        new_object.var = copy.deepcopy(self.var)
        new_object.count = float(self.count)
        return new_object

    def combine(self, other: "RunningMeanStd") -> None:
        """
        Combine stats from another ``RunningMeanStd`` object.
        :param other: The other object to combine with.
        """
        self.update_from_moments(other.mean, other.var, other.count)

    def update(self, arr: torch.Tensor) -> None:
        self.steps += 1
        if self.buffer is None:
            self.buffer = copy.deepcopy(arr)
        else:
            self.buffer = torch.concat((self.buffer, copy.deepcopy(arr)), dim=0)
        if self.steps > self.update_threshold:
            batch_mean = torch.mean(self.buffer, dim=0)
            batch_var = torch.var(self.buffer, dim=0)
            batch_count = self.buffer.size(0)
            self.steps = 0
            self.buffer = None
            self.update_from_moments(batch_mean, batch_var, batch_count)

    def update_from_moments(
        self,
        batch_mean: torch.Tensor,
        batch_var: torch.Tensor,
        batch_count: Union[int, float],
    ) -> None:
        delta = batch_mean - self.mean
        tot_count = self.count + batch_count

        new_mean = self.mean + delta * batch_count / tot_count
        m_a = self.var * self.count
        m_b = batch_var * batch_count
        m_2 = (
            m_a
            + m_b
            + torch.square(delta)
            * self.count
            * batch_count
            / (self.count + batch_count)
        )
        new_var = m_2 / (self.count + batch_count)

        new_count = batch_count + self.count

        self.mean = new_mean
        self.var = new_var
        self.count = new_count


class NormalizedNoAutoResetGymAgent(NoAutoResetGymAgent):
    """The same as AutoResetGymAgent, but normalizes observations"""

    def __init__(
        self,
        make_env_fn=None,
        make_env_args={},
        n_envs=None,
        seed=None,
        action_string="action",
        output="env/",
        obs_rms=None,
    ):
        super().__init__(
            make_env_fn=make_env_fn,
            make_env_args=make_env_args,
            n_envs=n_envs,
            seed=seed,
            action_string=action_string,
            output=output,
        )
        self.clip_obs = 10.0
        self.epsilon = 1e-08
        self.obs_rms = obs_rms

    def normalize_obs(self, obs: np.ndarray) -> np.ndarray:
        """
        Helper to normalize observation.
        :param obs:
        :return: normalized observation
        """
        obs_ = copy.deepcopy(obs)
        obs_rms = self.obs_rms
        return np.clip(
            (obs_ - obs_rms.mean) / np.sqrt(obs_rms.var + self.epsilon),
            -self.clip_obs,
            self.clip_obs,
        )

    def _reset(self, k, save_render, render):
        full_obs, observation = self._common_reset(k, save_render, render)
        self.last_frame[k] = observation

        full_obs["env_obs"] = self.normalize_obs(full_obs["env_obs"])

        return full_obs

    def _step(self, k, action, save_render, render):
        if self.finished[k]:
            assert k in self.last_frame
            rew = _torch_type({"reward": torch.tensor([0.0]).float()})
            return (
                {
                    **self.last_frame[k],
                    "done": torch.tensor([True]),
                    "truncated": torch.tensor([self.truncated[k]]),
                    "cumulated_reward": torch.tensor(
                        [self.cumulated_reward[k]]
                    ).float(),
                    "timestep": torch.tensor([self.timestep[k]]),
                },
                rew,
            )
        self.timestep[k] += 1
        full_obs, reward, done, truncated, observation = self._make_step(
            self.envs[k], action, k, save_render, render
        )

        self.last_frame[k] = observation
        if done:
            self.finished[k] = True
            self.truncated[k] = truncated

        full_obs["env_obs"] = self.normalize_obs(full_obs["env_obs"])

        return full_obs, reward


class NormalizedAutoResetGymAgent(AutoResetGymAgent):
    """The same as AutoResetGymAgent, but normalizes observations"""

    def __init__(
        self,
        make_env_fn=None,
        make_env_args={},
        n_envs=None,
        seed=None,
        action_string="action",
        output="env/",
    ):
        super().__init__(
            make_env_fn=make_env_fn,
            make_env_args=make_env_args,
            n_envs=n_envs,
            seed=seed,
            action_string=action_string,
            output=output,
        )
        self.clip_obs = 10.0
        self.epsilon = 1e-08
        self.obs_rms = RunningMeanStd(shape=self.observation_space.shape)

    def normalize_obs(self, obs: np.ndarray) -> np.ndarray:
        """
        Helper to normalize observation.
        :param obs:
        :return: normalized observation
        """
        obs_ = copy.deepcopy(obs)
        obs_rms = self.obs_rms

        return np.clip(
            (obs_ - obs_rms.mean) / np.sqrt(obs_rms.var + self.epsilon),
            -self.clip_obs,
            self.clip_obs,
        )

    def _reset(self, k, save_render, render):
        self.is_running[k] = True
        full_obs, _ = self._common_reset(k, save_render, render)

        self.obs_rms.update(full_obs["env_obs"])
        full_obs["env_obs"] = self.normalize_obs(full_obs["env_obs"])
        return full_obs

    def _step(self, k, action, save_render, render):
        self.timestep[k] += 1
        full_obs, reward, done, truncated, _ = self._make_step(
            self.envs[k], action, k, save_render, render
        )
        if done:
            self.is_running[k] = False
            self.truncated[k] = truncated

        self.obs_rms.update(full_obs["env_obs"])
        full_obs["env_obs"] = self.normalize_obs(full_obs["env_obs"])

        return full_obs, reward

----!@#$----
agents\gymnasium.py
# coding=utf-8
#
# Copyright © Sorbonne University
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
#
import warnings
from abc import ABC
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Union, Tuple
import logging
import numpy as np
import torch
from torch import nn, Tensor
import gymnasium as gym
from gymnasium import Env, Space, Wrapper, make
from gymnasium.core import ActType, ObsType
from gymnasium.vector import VectorEnv
from gymnasium.wrappers import AutoResetWrapper
from gymnasium.wrappers.monitoring.video_recorder import VideoRecorder

from bbrl import SeedableAgent, SerializableAgent, TimeAgent, Agent
from bbrl.agents.utils import TemporalAgent
from bbrl.workspace import Workspace


def make_env(env_name, autoreset=False, wrappers: List = [], **kwargs):
    """Utility function to create an environment

    Other parameters are forwarded to the gymnasium `make`

    :param env_name: The environment name
    :param wrappers: Wrappers applied to the base environment **in the order
        they are provided**: that is, if wrappers=[W1, W2], the environment
        (before the optional auto-wrapping) will be W2(W1(base_env))
    :param autoreset: if True, wrap the environment into an AutoResetWrapper,
        defaults to False
    """
    env = make(env_name, **kwargs)
    for wrapper in wrappers:
        env = wrapper(env)

    if autoreset:
        env = AutoResetWrapper(env)
    return env


def record_video(env: Env, policy: Agent, path: str):
    """Record a video for a given gymnasium environment and a BBRL agent

    :param env: The environment (created with `render_mode="rgb_array"`)
    :param policy: The BBRL agent
    :param path: The path of the video
    """
    # Tries to get the non temporal agent.
    if isinstance(policy, TemporalAgent):
        policy = policy.agent

    # Creates the containing folder if needed
    path = Path(path)

    path.parent.mkdir(exist_ok=True, parents=True)

    with torch.no_grad():
        workspace = Workspace()
        obs, _ = env.reset()
        workspace.set("env/env_obs", 0, torch.Tensor(obs).unsqueeze(0))
        t = 0
        done = False
        video_recorder = VideoRecorder(env, str(path.resolve()), enabled=True)
        video_recorder.capture_frame()

        while not done:
            workspace.set("env/env_obs", t, torch.Tensor(obs).unsqueeze(0))
            policy(t=t, workspace=workspace)
            action = workspace.get("action", t).squeeze(0).numpy()
            obs, reward, terminated, truncated, info = env.step(action)
            video_recorder.capture_frame()
            done = terminated or truncated
            t += 1

        video_recorder.close()


def _convert_action(action: Union[Dict,Tensor]) -> Union[int, np.ndarray]:
    if isinstance(action, dict):
        return {key: _convert_action(value) for key, value in action.items()}
    if len(action.size()) == 0:
        action = action.item()
        assert isinstance(action, int)
    else:
        action = np.array(action.tolist())
    return action


def _format_frame(
    frame: Union[Dict[str, Tensor], List[Tensor], np.ndarray, Tensor, bool, int, float]
) -> Union[Tensor, Dict[str, Tensor]]:
    if isinstance(frame, Dict):
        r = {}
        for k in frame:
            r[k] = _format_frame(frame[k])
        return r
    elif isinstance(frame, List):
        t = torch.tensor(frame).unsqueeze(0)
        if t.dtype == torch.float64 or t.dtype == torch.float32:
            t = t.float()
        else:
            t = t.long()
        return t
    elif isinstance(frame, np.ndarray):
        t = torch.from_numpy(frame).unsqueeze(0)
        if t.dtype == torch.float64 or t.dtype == torch.float32:
            t = t.float()
        else:
            t = t.long()
        return t
    elif isinstance(frame, Tensor):
        return frame.unsqueeze(0)
    elif isinstance(frame, bool):
        return torch.tensor([frame]).bool()
    elif isinstance(frame, int):
        return torch.tensor([frame]).long()
    elif isinstance(frame, float):
        return torch.tensor([frame]).float()

    else:
        try:
            # Check if it is a LazyFrame from OpenAI Baselines
            o = torch.from_numpy(frame.__array__()).unsqueeze(0).float()
            return o
        except TypeError:
            assert False


def _torch_type(d: Dict[str, Tensor]) -> Dict[str, Tensor]:
    return {k: d[k].float() if torch.is_floating_point(d[k]) else d[k] for k in d}


def _torch_cat_dict(d: List[Dict[str, Tensor]]) -> Dict[str, Tensor]:
    r = {}
    for k in d[0]:
        r[k] = torch.cat([dd[k] for dd in d], dim=0)
    return r


class GymAgent(TimeAgent, SeedableAgent, SerializableAgent, ABC):
    default_seed = 42

    def __init__(
        self,
        *args,
        input_string: str = "action",
        output_string: str = "env/",
        reward_at_t: bool = False,
        include_last_state: bool = True,
        **kwargs,
    ):
        """Initialize the generic GymAgent

        :param input_string: The name of the variable containing the action,
            defaults to "action"
        :param output_string: The prefix for variables set by the environment,
            defaults to "env/"
        :param reward_at_t: The reward for transitioning from $s_t$ to $s_{t+1}$
            is $r_t$ if reward_at_t is True, and $r_{t+1}$ otherwise (default
            False).
        :param include_last_state: By default (False), the final state is not
            included when using an auto-reset environment. Setting to True allows
            to preserve it.
        """
        super().__init__(*args, **kwargs)

        self.reward_at_t = reward_at_t
        self.include_last_state = include_last_state
        self.ghost_params: nn.Parameter = nn.Parameter(torch.Tensor())

        self.input: str = input_string
        self.output: str = output_string
        self._timestep_from_reset: int = 0
        self._nb_reset: int = 1

        self.observation_space: Optional[Space[ObsType]] = None
        self.action_space: Optional[Space[ActType]] = None

    def forward(self, t: int, *args, **kwargs) -> None:
        if self._seed is None:
            self.seed(self.default_seed)
        if t == 0:
            self._timestep_from_reset = 1
            self._nb_reset += 1
        else:
            self._timestep_from_reset += 1

    def set_obs(self, observations: Dict[str, Tensor], t: int) -> None:
        for k in observations:
            obs = observations[k].to(self.ghost_params.device)
            if self.reward_at_t and k in ["reward", "cumulated_reward"]:
                if t > 0:
                    self.set(
                        (self.output + k, t - 1),
                        obs,
                    )

                # Just use 0 for reward at $t$ for now
                if k == "reward":
                    obs = torch.zeros_like(obs)
            try:
                self.set(
                    (self.output + k, t),
                    obs,
                )
            except Exception:
                logging.error("Error while setting %s", self.output + k)
                raise

    def get_observation_space(self) -> Space[ObsType]:
        """Return the observation space of the environment"""
        if self.observation_space is None:
            raise ValueError("The observation space is not defined")
        return self.observation_space

    def get_action_space(self) -> Space[ActType]:
        """Return the action space of the environment"""
        if self.action_space is None:
            raise ValueError("The action space is not defined")
        return self.action_space

    def get_obs_and_actions_sizes(self) -> Union[int, Tuple[int]]:
        obs_space = self.get_observation_space()
        act_space = self.get_action_space()

        def parse_space(space):
            if len(space.shape) > 0:
                if len(space.shape) > 1:
                    warnings.warn(
                        "Multi dimensional space, be careful, a tuple (shape) "
                        "is returned, maybe youd like to flatten or simplify it first"
                    )
                    return space.shape
                return space.shape[0]
            else:
                return space.n

        return parse_space(obs_space), parse_space(act_space)

    def is_continuous_action(self):
        return isinstance(self.action_space, gym.spaces.Box)

    def is_discrete_action(self):
        return isinstance(self.action_space, gym.spaces.Discrete)

    def is_continuous_state(self):
        return isinstance(self.observation_space, gym.spaces.Box)

    def is_discrete_state(self):
        return isinstance(self.observation_space, gym.spaces.Discrete)


class ParallelGymAgent(GymAgent):
    """Create an Agent from a gymnasium environment

    To create an auto-reset ParallelGymAgent, use the gymnasium
    `AutoResetWrapper` in the make_env_fn
    """

    def __init__(
        self,
        make_env_fn: Callable[[Optional[Dict[str, Any]]], Env],
        num_envs: int,
        make_env_args: Union[Dict[str, Any], None] = None,
        *args,
        **kwargs,
    ):
        """Create an agent from a Gymnasium environment

        Args:
            make_env_fn ([function that returns a gymnasium.Env]): The function
            to create a single gymnasium environments

            num_envs ([int]): The number of environments to create, defaults to
            1

            make_env_args (dict): The arguments of the function that creates a
            gymnasium.Env

            input_string (str, optional): [the name of the action variable in
            the workspace]. Defaults to "action".

            output_string (str, optional): [the output prefix of the
            environment]. Defaults to "env/".
        """
        super().__init__(*args, **kwargs)
        assert num_envs > 0, "n_envs must be > 0"

        self.make_env_fn: Callable[[], Env] = make_env_fn
        self.num_envs: int = num_envs

        self.envs: List[Env] = []
        self.cumulated_reward: Dict[int, float] = {}

        self._timestep: Tensor
        self._is_autoreset: bool = False
        self._last_frame = [None for _ in range(num_envs)]

        args: Dict[str, Any] = make_env_args if make_env_args is not None else {}
        self._initialize_envs(num_envs=num_envs, make_env_args=args)

    def _initialize_envs(self, num_envs: int, make_env_args: Dict[str, Any]):
        self.envs = [self.make_env_fn(**make_env_args) for _ in range(num_envs)]
        self._timestep = torch.zeros(len(self.envs), dtype=torch.long)
        self.observation_space = self.envs[0].observation_space
        self.action_space = self.envs[0].action_space

        # Check if we have an autoreset wrapper somewhere
        _env = self.envs[0]
        while isinstance(_env, Wrapper) and not self._is_autoreset:
            self._is_autoreset = isinstance(_env, AutoResetWrapper)
            _env = _env.env

        if not self._is_autoreset:
            # Do not include last state if not auto-reset
            self.include_last_state = False

    @staticmethod
    def _flatten_value(value: Dict[str, Any]):
        """Flatten nested dict structures with concatenating keys"""
        ret = {}
        for key, value in value.items():
            if isinstance(value, Tensor):
                ret[key] = value
            elif isinstance(value, dict):
                for subkey, subvalue in ParallelGymAgent._flatten_value(value).items():
                    ret[f"{key}/{subkey}"] = subvalue
            else:
                raise ValueError(
                    f"Observation component must be a torch.Tensor or a dict, not {type(observation)}"
                )                

        return ret

    @staticmethod
    def _format_frame(frame):
        observation = _format_frame(frame)

        if isinstance(observation, Tensor):
            return {"env_obs": observation}

        if isinstance(observation, dict):
            return {f"env_obs/{key}": value for key, value in ParallelGymAgent._flatten_value(observation).items()}

        raise ValueError(
            f"Observation must be a torch.Tensor or a dict, not {type(observation)}"
        )

    def _format_obs(
        self, k: int, obs, info, *, terminated=False, truncated=False, reward=0
    ):
        observation: Union[Tensor, Dict[str, Tensor]] = ParallelGymAgent._format_frame(
            obs
        )

        done = terminated or truncated

        if done and self.include_last_state:
            # Create a new frame to be inserted after this step,
            # containing the first observation of the next episode
            self._last_frame[k] = {
                **observation,
                "terminated": torch.tensor([False]),
                "truncated": torch.tensor([False]),
                "done": torch.tensor([False]),
                "reward": torch.tensor([0]).float(),
                "cumulated_reward": torch.tensor([0]).float(),
                "timestep": torch.tensor([0]),
            }
            # Use the final observation instead
            observation = ParallelGymAgent._format_frame(info["final_observation"])

        ret: Dict[str, Tensor] = {
            **observation,
            "terminated": torch.tensor([terminated]),
            "truncated": torch.tensor([truncated]),
            "done": torch.tensor([done]),
            "reward": torch.tensor([reward]).float(),
            "cumulated_reward": torch.tensor([self.cumulated_reward[k]]),
            "timestep": torch.tensor([self._timestep[k]]),
        }

        # Resets the cumulated reward and timestep
        if done and self._is_autoreset:
            self.cumulated_reward[k] = 0.0
            if self._is_autoreset and self.include_last_state:
                self._timestep[k] = 0
            else:
                self._timestep[k] = 1

        return _torch_type(ret)

    def _reset(self, k: int) -> Dict[str, Tensor]:
        """Resets the kth environment

        :param k: The environment index
        :raises ValueError: if the returned observation is not a torch Tensor or
            a dict
        :return: The first observation
        """
        env: Env = self.envs[k]

        self._timestep[k] = 0
        self.cumulated_reward[k] = 0.0

        # Computes a new seed for this environment
        s: int = self._timestep_from_reset * self.num_envs * self._nb_reset * self._seed
        s += (k + 1) * (self._timestep[k].item() + 1 if self._is_autoreset else 1)

        return self._format_obs(k, *env.reset(seed=s))

    def _step(self, k: int, action: Tensor):
        env = self.envs[k]

        action: Union[int, np.ndarray[int]] = _convert_action(action)
        obs, reward, terminated, truncated, info = env.step(action)

        self._timestep[k] += 1
        self.cumulated_reward[k] += reward

        return self._format_obs(
            k, obs, info, terminated=terminated, truncated=truncated, reward=reward
        )

    def forward(self, t: int = 0, **kwargs) -> None:
        """Do one step by reading the `action` at t-1
        If t==0, environments are reset
        If render is True, then the output of env.render() is written as env/rendering
        """
        super().forward(t, **kwargs)

        observations = []
        if t == 0:
            for k, env in enumerate(self.envs):
                observations.append(self._reset(k))
                self._last_frame[k] = None
        else:
            if self.input in self.workspace.variables:
                # Action is a tensor
                action = self.get((self.input, t - 1))
                assert action.size()[0] == self.num_envs, f"Incompatible number of envs ({action.shape[0]} vs {self.num_envs})"
            else:
                # Action is a dictionary
                action = {}
                prefix = f"{self.input}/"
                len_prefix = len(prefix)
                for varname in self.workspace.variables:
                    if not varname.startswith(prefix):
                        continue
                    keys = varname[len_prefix:].split("/")
                    current = action
                    for key in keys[:-1]:
                        current = current.setdefault(key, {})
                    current[keys[-1]] = self.get((varname, t - 1))
                
            def dict_slice(k: int, object):
                if isinstance(object, dict):
                    return {key: dict_slice(k, value) for key, value in object.items()}
                return object[k]
                
            for k, env in enumerate(self.envs):
                if self._last_frame[k] is None:
                    if isinstance(action, dict):
                        frame = self._step(k, dict_slice(k, action))
                    else:
                        frame = self._step(k, action[k])
                else:
                    # Use last frame
                    frame = self._last_frame[k]
                    self._last_frame[k] = None

                observations.append(frame)

                # Reproduce the last frame if over (but with 0 reward)
                if not self._is_autoreset and frame["done"]:
                    self._last_frame[k] = {key: value for key, value in frame.items()}
                    self._last_frame[k]["reward"] = torch.Tensor([0.0])

        self.set_obs(observations=_torch_cat_dict(observations), t=t)


class VecGymAgent(GymAgent):
    """Multi-process

    Use gymnasium VecEnv for multi-process support
    This constrains the environment to be of the auto-reset "type"
    """

    def __init__(
        self,
        make_envs_fn: Callable[[Optional[Dict[str, Any]]], VectorEnv],
        vec_env_args: Optional[Dict[str, Any]] = None,
        *args,
        **kwargs,
    ):
        super().__init__(*args, **kwargs)

        args: Dict[str, Any] = vec_env_args or {}
        self.envs: VectorEnv = make_envs_fn(**args)

        self.observation_space = self.envs.single_observation_space
        self.action_space = self.envs.single_action_space

        self.cumulated_reward: Tensor = torch.zeros(self.envs.num_envs)

    def forward(self, t: int, **kwargs) -> None:
        super().forward(t, **kwargs)

        if t == 0:
            s: int = self._seed * self._nb_reset
            obs, infos = self.envs.reset(seed=s)
            terminated = torch.tensor([False] * self.envs.num_envs)
            truncated = torch.tensor([False] * self.envs.num_envs)
            rewards = torch.tensor([0.0] * self.envs.num_envs)
            self.cumulated_reward = torch.zeros(self.envs.num_envs)
        else:
            action = self.get((self.input, t - 1))
            assert (
                action.size()[0] == self.envs.num_envs
            ), "Incompatible number of actions"
            converted_action: Union[int, np.ndarray[int]] = _convert_action(action)
            obs, rewards, terminated, truncated, infos = self.envs.step(
                converted_action
            )
            rewards = torch.tensor(rewards).float()
            terminated = torch.tensor(terminated)
            truncated = torch.tensor(truncated)
            self.cumulated_reward = self.cumulated_reward + rewards

        observation: Union[Tensor, Dict[str, Tensor]] = _format_frame(obs)

        if not isinstance(observation, Tensor):
            raise ValueError("Observation can't be an OrderedDict in a VecEnv")

        ret: Dict[str, Tensor] = {
            "env_obs": observation.squeeze(0),
            "terminated": terminated,
            "truncated": truncated,
            "done": terminated or truncated,
            "reward": rewards,
            "cumulated_reward": self.cumulated_reward,
        }
        self.set_obs(observations=ret, t=t)

----!@#$----
agents\README.md
# bbrl.agents

We propose a list of agents to reuse (see Documentation in the code)

## utils

* Agents: Execute multiple agents sequentially
* TemporalAgent: Execute one agent over multiple timesteps
* CopyAgent: An agent to create copies of variables
* PrintAgent: An agent that prints variables

## remote

* RemoteAgent: A single agent in a single process
* NRemoteAgent: A single agent parallelized over multiple processes

## gyma and gymb

* GymAgent: An agent based on an openAI gym environment
* AutoResetGymAgent: The same, but with an autoreset when reaching terminal states
* NoAutoResetGymAgent: The same as the GymAgent

## brax_wrapper

* AutoResetBraxAgent: An agent based on a BRAX environment with autoreset
* NoAutoResetBraxAgent: An agent based on a BRAX environment without autoreset

## dataloader

* ShuffledDatasetAgent: An agent to read random batches in a torch.utils.data.Dataset
* DataLoaderAgent: An agent to do one pass over a complete dataset (based on a DataLoader)

## asynchronous

* AsynchronousAgent: it is used to execute any agent asynchronously, the agent creating its own workspace at each
  execution

1. `agent=AsynchronousAgent(my_agent)`
2. `agent(**execution_arguments)` (not workspace provided)
3. `if not agent.is_running(): workspace=agent.get_workspace()`

----!@#$----
agents\remote.py
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
#

import copy
import time

import torch
import torch.multiprocessing as mp

from bbrl.agents.agent import Agent
from bbrl.workspace import Workspace, _SplitSharedWorkspace


# Unused???
def f(agent, in_queue, out_queue, seed, verbose):
    """The function that is executed in a single process"""
    out_queue.put("ok")
    running = True
    old_workspace = None
    if verbose:
        print("Seeding remote agent with ", seed)
    agent.seed(seed)
    while running:
        command = in_queue.get()
        if command[0] == "go_new_workspace":
            _, workspace, kwargs = command
            old_workspace = workspace
            agent(workspace, **kwargs)
            out_queue.put("ok")
        elif command[0] == "go_reuse_workspace":
            _, _, kwargs = command
            agent(old_workspace, **kwargs)
            out_queue.put("ok")
        elif command[0] == "exit":
            out_queue.put("ok")
            return
        elif command[0] == "eval_mode":
            agent.eval()
            out_queue.put("ok")
        elif command[0] == "train_mode":
            agent.train()
            out_queue.put("ok")


class RemoteAgent(Agent):
    """It corresponds to an agent that is executed in another process

    Args:
        Agent ([bbrl.Agent]): the agent ot execute in another process
    """

    def __init__(self, agent, name=None, verbose=False):
        super().__init__(name=name, verbose=verbose)
        self.agent = agent
        self._is_running = False
        self.process = None
        self.last_workspace = None
        self.train_mode = True

    def get_by_name(self, n):
        if self._name == n:
            return [self] + self.agent.get_by_name(n)
        else:
            return self.agent.get_by_name(n)

    def forward(self, **kwargs):
        raise NotImplementedError

    def _create_process(self):
        if self.verbose:
            print("[RemoteAgent] starting process...")
        self.i_queue = mp.Queue()
        self.o_queue = mp.Queue()
        self.i_queue.cancel_join_thread()
        self.o_queue.cancel_join_thread()
        self.process = mp.Process(
            target=f, args=(self.agent, self.i_queue, self.o_queue, self._seed)
        )
        self.process.daemon = False
        self.process.start()

    def __call__(self, workspace, **kwargs):
        with torch.no_grad():
            assert (
                workspace.is_shared
            ), "You must use a shared workspace when using a Remote Agent"
            if self.process is None:
                self._create_process()
                self.train(self.train_mode)
            if not workspace == self.last_workspace:
                self.i_queue.put(("go_new_workspace", workspace, kwargs))
                self.last_workspace = workspace
                r = self.o_queue.get()
                assert r == "ok"
            else:
                self.i_queue.put(("go_reuse_workspace", workspace, kwargs))
                r = self.o_queue.get()
                assert r == "ok"

    def _asynchronous_call(self, workspace, **kwargs):
        """Non-blocking forward. To use together with `is_running`"""
        with torch.no_grad():
            self._is_running = True
            assert (
                workspace.is_shared
            ), "You must use a shared workspace when using a Remote Agent"
            if self.process is None:
                self._create_process()
            if not workspace == self.last_workspace:
                self.i_queue.put(("go_new_workspace", workspace, kwargs))
                self.last_workspace = workspace
            else:
                self.i_queue.put(("go_reuse_workspace", workspace, kwargs))

    def train(self, f=True):
        self.train_mode = f
        if self.process is None:
            return
        if f:
            self.i_queue.put(("train_mode",))
            a = self.o_queue.get()
            assert a == "ok"
        else:
            self.eval()

    def eval(self):
        self.train_mode = False
        if self.process is None:
            return
        self.i_queue.put(("eval_mode",))
        a = self.o_queue.get()
        assert a == "ok"

    def seed(self, _seed):
        self._seed = _seed

    def _running_queue(self):
        return self.o_queue

    def is_running(self):
        if self._is_running:
            try:
                r = self.o_queue.get(False)
                assert r == "ok"
                self._is_running = False
            except:
                pass
        return self._is_running

    def close(self):
        if self.process is None:
            return

        if self.verbose:
            print("[RemoteAgent] closing process")
        self.i_queue.put(("exit",))
        self.o_queue.get()
        time.sleep(0.1)
        self.process.terminate()
        self.process.join()
        self.i_queue.close()
        self.o_queue.close()
        time.sleep(0.1)
        del self.i_queue
        del self.o_queue
        self.process = None

    def __del__(self):
        self.close()


class NRemoteAgent(Agent):
    """Multiple agents executed in different processes. Use the `NRemoteAgent.create` function to create such an agent"""

    def __init__(self, agents, batch_dims):
        super().__init__()
        self.agents = agents
        self.batch_dims = batch_dims

    def get_by_name(self, name):
        r = []
        if self._name == name:
            r = [self]
        for a in self.agents:
            r = r + a.get_by_name(name)
        return r

    @staticmethod
    def create(agent, num_processes=0, time_size=None, **extra_kwargs):
        """Returns a NRemote agent with num_processes copies of agent in different processes
        Also returns the specific workspace to use with such an agent

        Args:
            agent ([bbrl.Agent]): The agent to execute in multiple processes
            num_processes (int, optional): Number of processes to create. If 0, then no processes are created (for debugging). Defaults to 0.
            time_size ([type], optional): If specified, it forces the created Workspace to have this particular time_size. Defaults to None.

        Returns:
            [bbrl.Agent,bbrl.SharedWorkspace]: The NRemoteAgent and the corresponding workspace
        """
        agent.seed(0)
        if num_processes == 0:
            workspace = Workspace()
            _agent = copy.deepcopy(agent)
            agent(workspace, **extra_kwargs)
            shared_workspace = workspace._convert_to_shared_workspace(
                n_repeat=1, time_size=time_size
            )
            return _agent, shared_workspace

        workspace = Workspace()
        agents = [copy.deepcopy(agent) for t in range(num_processes)]
        agent(workspace, **extra_kwargs)
        b = workspace.batch_size()
        batch_dims = [(k * b, k * b + b) for k, a in enumerate(agents)]
        shared_workspace = workspace._convert_to_shared_workspace(
            n_repeat=num_processes, time_size=time_size
        )
        agents = [RemoteAgent(a) for a in agents]
        return NRemoteAgent(agents, batch_dims), shared_workspace

    def __call__(self, workspace, **kwargs):
        assert workspace.is_shared
        for k in range(len(self.agents)):
            _workspace = _SplitSharedWorkspace(workspace, self.batch_dims[k])
            self.agents[k]._asynchronous_call(_workspace, **kwargs)
        for a in self.agents:
            ok = a._running_queue().get()
            assert ok == "ok"

    def seed(self, seed, inc=1):
        s = seed
        for a in self.agents:
            a.seed(s)
            s += inc

    def _asynchronous_call(self, workspace, **kwargs):
        assert workspace.is_shared
        for k in range(len(self.agents)):
            _workspace = _SplitSharedWorkspace(workspace, self.batch_dims[k])
            self.agents[k]._asynchronous_call(_workspace, **kwargs)

    def is_running(self):
        for a in self.agents:
            if a.is_running():
                return True
        return False

    def train(self, f=True):
        for a in self.agents:
            a.train(f)

    def eval(self):
        for a in self.agents:
            a.eval()

    def close(self):
        for a in self.agents:
            a.close()

----!@#$----
agents\seeding.py
# coding=utf-8
#
# Copyright © Sorbonne University
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
#
from abc import ABC
from typing import Optional

from bbrl.agents import Agent


class SeedableAgent(Agent, ABC):
    """
    `SeedableAgent` is used as a convention to represent agents that
    are seeded (not mandatory)
    """

    def __init__(self, seed: Optional[int] = None, **kwargs):
        super().__init__(**kwargs)
        self._seed = seed

    def seed(self, seed: int):
        """Provide a seed to this agent. Useful if the agent is stochastic.

        Args:
            seed (int): The seed to use
        """
        if self._seed is not None:
            raise Exception(
                "Your {self} agent is already seeded with {seed}. You cannot seed it again with {new_seed}\n"
                "If you want to seed it again, une one of SAgentLast, SAgentMean, SAgentSum".format(
                    self=self.__class__.__name__,
                    seed=self._seed,
                    new_seed=seed,
                )
            )
        self._seed = seed
        return self


class SeedableAgentLast(SeedableAgent, ABC):
    """
    `SeedableAgentLast` is used as a convention to represent agents that
    are seeded (not mandatory) and that use the last seed provided."""

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def seed(self, seed: int):
        """Provide a seed to this agent. Useful is the agent is stochastic.

        Args:
            seed (int): The seed to use
        """
        self._seed = seed


class SeedableAgentSum(SeedableAgent, ABC):
    """
    `SeedableAgentSum` is used as a convention to represent agents that
    are seeded (not mandatory) and that use the sum of all seeds provided.
    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._seeds = []

    def seed(self, seed: int) -> None:
        """Provide a seed to this agent. Useful is the agent is stochastic.

        Args:
            seed (int): The seed to use
        """
        self._seeds.append(seed)
        self._seed = sum(self._seeds)


class SeedableAgentMean(SeedableAgentSum, ABC):
    """
    `SeedableAgentMean` is used as a convention to represent agents that
    are seeded (not mandatory) and that use the mean of all seeds provided.
    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def seed(self, seed: int):
        """Provide a seed to this agent. Useful is the agent is stochastic.

        Args:
            seed (int): The seed to use
        """
        super().seed(seed)
        self._seed = int(sum(self._seeds) / len(self._seeds))

----!@#$----
agents\utils.py
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
#

import torch
import torch.nn as nn

from bbrl.agents.agent import Agent


class Agents(Agent):
    """An agent that contains multiple agents that will be executed sequentially

    Args:
        Agent ([bbrl.Agent]): The agents
    """

    def __init__(self, *agents, name=None):
        """Creates the agent from multiple agents

        Args:
            name ([str], optional): [name of the resulting agent]. Defaults to None.
        """
        super().__init__(name=name)
        for a in agents:
            assert isinstance(a, Agent)
        self.agents = nn.ModuleList(agents)

    def __call__(self, workspace, **kwargs):
        for a in self.agents:
            a(workspace, **kwargs)

    def forward(self, **kwargs):
        raise NotImplementedError

    def seed(self, seed):
        for a in self.agents:
            a.seed(seed)

    def __getitem__(self, k):
        return self.agents[k]

    def get_by_name(self, n):
        r = []
        for a in self.agents:
            r = r + a.get_by_name(n)
        if n == self._name:
            r = r + [self]
        return r


class TemporalAgent(Agent):
    """Execute one Agent over multiple timesteps"""

    def __init__(self, agent, name=None):
        """The agent to transform to a temporal agent

        Args:
            agent ([bbrl.Agent]): The agent to encapsulate
            name ([str], optional): Name of the agent
        """
        super().__init__(name=name)
        self.agent = agent

    def __call__(self, workspace, t=0, n_steps=None, stop_variable=None, **kwargs):
        """Execute the agent starting at time t, for n_steps

        Args:
            workspace ([bbrl.Workspace]):
            t (int, optional): The starting timestep. Defaults to 0.
            n_steps ([type], optional): The number of steps. Defaults to None.
            stop_variable ([type], optional): if True everywhere (at time t), execution is stopped.
            Defaults to None = not used.
        """

        assert n_steps is not None or stop_variable is not None
        _t = t
        while True:
            self.agent(workspace, t=_t, **kwargs)
            if stop_variable is not None:
                s = workspace.get(stop_variable, _t)
                if s.all():
                    break
            _t += 1
            if n_steps is not None:
                if _t >= t + n_steps:
                    break

    def forward(self, **kwargs):
        raise NotImplementedError

    def seed(self, seed):
        self.agent.seed(seed)

    def get_by_name(self, n):
        r = self.agent.get_by_name(n)
        if n == self._name:
            r = r + [self]
        return r


class CopyTAgent(Agent):
    """An agent that copies a variable"""

    def __init__(self, input_name, output_name, detach=False, name=None):
        """
        Args:
        input_name ([str]): The variable to copy from
        output_name ([str]): The variable to copy to
        detach ([bool]): copy with detach if True
        """
        super().__init__(name=name)
        self.input_name = input_name
        self.output_name = output_name
        self.detach = detach

    def forward(self, t=None, **kwargs):
        """
        Args:
            t ([type], optional): if not None, copy at time t. Defaults to None.
        """
        if t is None:
            x = self.get(self.input_name)
            if not self.detach:
                self.set(self.output_name, x)
            else:
                self.set((self.output_name, t), x.detach())
        else:
            x = self.get((self.input_name, t))
            if not self.detach:
                self.set((self.output_name, t), x)
            else:
                self.set((self.output_name, t), x.detach())


class PrintAgent(Agent):
    """An agent to generate print in the console (mainly for debugging)
    It can be passed a list of strings corresponding to the variables to print
    or if nothing is passed, it prints all the existing variables in the workspace
    """

    def __init__(self, *names, name=None):
        """
        Args:
            names ([str], optional): The variables to print
        """
        super().__init__(name=name)
        self.names = names

    def reset(self):
        self.names = ()

    def forward(self, t, **kwargs):
        if self.names == ():
            self.names = self.workspace.keys()
        for n in self.names:
            try:
                value = self.get((n, t))
                print(n, " = ", value)
            except KeyError as e:
                print("Be sure to:")
                print(" — use the correct variable name / time step / workspace")
                print(
                    " — your print agent is called after the variable is written by another agent at the same time step"
                )
                if t == 0:
                    print(
                        f" — if you use r_t representation, your key {n} is not yet written at time t=0"
                    )
                raise KeyError("Variable ", n, " not found", e)


class EpisodesDone(Agent):
    """
    If done is encountered at time t, then done=True for all timeteps t'>=t
    It allows to simulate a single episode agent based on an autoreset agent
    """

    def __init__(self, in_var="env/done", out_var="env/done"):
        super().__init__()
        self.in_var = in_var
        self.out_var = out_var

    def forward(self, t, **kwargs):
        d = self.get((self.in_var, t))
        if t == 0:
            self.state = torch.zeros_like(d).bool()
        self.state = torch.logical_or(self.state, d)
        self.set((self.out_var, t), self.state)

----!@#$----
agents\__init__.py
from .agent import Agent, TimeAgent, SerializableAgent
from .dataloader import DataLoaderAgent, ShuffledDatasetAgent
from .remote import NRemoteAgent, RemoteAgent
from .utils import Agents, CopyTAgent, PrintAgent, TemporalAgent, EpisodesDone
from .seeding import SeedableAgent

----!@#$----
utils\chrono.py
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
#

import time


class Chrono:
    """
    Description: Class to display time spent in human format rather than seconds
    """

    def __init__(self):
        self.name = "Chrono"
        self.start = time.time()

    def stop(self):
        stop = time.time()
        dif = stop - self.start
        difstring = ""
        if dif > 3600:
            heures = int(dif / 3600)
            difstring = str(heures) + "h "
            dif = dif - (heures * 3600)
        if dif > 60:
            minutes = int(dif / 60)
            difstring = difstring + str(minutes) + "mn "
            dif = dif - (minutes * 60)
        difstring = difstring + str(int(dif)) + "s "
        dif = int((dif - int(dif)) * 1000)
        difstring = difstring + str(dif) + "ms"
        print("Time :", difstring)

----!@#$----
utils\distributions.py
""" Probability distributions
This file has been imported from Stable baselines 3 and adapted to the BBRL context """

import numpy as np

from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional, Tuple, Union

import gym
import torch as th
from gym import spaces
from torch import nn
from torch.distributions import Bernoulli, Categorical, Normal


def get_action_dim(action_space: spaces.Space) -> int:
    """
    Get the dimension of the action space.

    :param action_space:
    :return:
    """
    if isinstance(action_space, spaces.Box):
        return int(np.prod(action_space.shape))
    elif isinstance(action_space, spaces.Discrete):
        # Action is an int
        return 1
    elif isinstance(action_space, spaces.MultiDiscrete):
        # Number of discrete actions
        return int(len(action_space.nvec))
    elif isinstance(action_space, spaces.MultiBinary):
        # Number of binary actions
        return int(action_space.n)
    else:
        raise NotImplementedError(f"{action_space} action space is not supported")


class Distribution(ABC):
    """Abstract base class for distributions."""

    def __init__(self):
        super().__init__()
        self.distribution = None

    @abstractmethod
    def proba_distribution_net(
        self, *args, **kwargs
    ) -> Union[nn.Module, Tuple[nn.Module, nn.Parameter]]:
        """Create the layers and parameters that represent the distribution.

        Subclasses must define this, but the arguments and return type vary between
        concrete classes."""

    @abstractmethod
    def proba_distribution(self, *args, **kwargs) -> "Distribution":
        """Set parameters of the distribution.

        :return: self
        """

    @abstractmethod
    def log_prob(self, x: th.Tensor) -> th.Tensor:
        """
        Returns the log likelihood

        :param x: the taken action
        :return: The log likelihood of the distribution
        """

    @abstractmethod
    def entropy(self) -> Optional[th.Tensor]:
        """
        Returns Shannon's entropy of the probability

        :return: the entropy, or None if no analytical form is known
        """

    @abstractmethod
    def sample(self) -> th.Tensor:
        """
        Returns a sample from the probability distribution

        :return: the stochastic action
        """

    @abstractmethod
    def mode(self) -> th.Tensor:
        """
        Returns the most likely action (deterministic output)
        from the probability distribution

        :return: the stochastic action
        """

    def get_actions(self, deterministic: bool = False) -> th.Tensor:
        """
        Return actions according to the probability distribution.

        :param deterministic:
        :return:
        """
        if deterministic:
            return self.mode()
        return self.sample()

    @abstractmethod
    def actions_from_params(self, *args, **kwargs) -> th.Tensor:
        """
        Returns samples from the probability distribution
        given its parameters.

        :return: actions
        """

    @abstractmethod
    def log_prob_from_params(self, *args, **kwargs) -> Tuple[th.Tensor, th.Tensor]:
        """
        Returns samples and the associated log probabilities
        from the probability distribution given its parameters.

        :return: actions and log prob
        """


def sum_independent_dims(tensor: th.Tensor) -> th.Tensor:
    """
    Continuous actions are usually considered to be independent,
    so we can sum components of the ``log_prob`` or the entropy.

    :param tensor: shape: (n_batch, n_actions) or (n_batch,)
    :return: shape: (n_batch,)
    """
    if len(tensor.shape) > 1:
        tensor = tensor.sum(dim=1)
    else:
        tensor = tensor.sum()
    return tensor


class DiagGaussianDistribution(Distribution):
    """
    Gaussian distribution with diagonal covariance matrix, for continuous actions.

    :param action_dim:  Dimension of the action space.
    """

    def __init__(self, action_dim: int):
        super().__init__()
        self.action_dim = action_dim
        self.mean_actions = None
        self.log_std = None

    def proba_distribution_net(
        self, latent_dim: int, log_std_init: float = 0.0
    ) -> Tuple[nn.Module, nn.Parameter]:
        """
        Create the layers and parameter that represent the distribution:
        one output will be the mean of the Gaussian, the other parameter will be the
        standard deviation (log std in fact to allow negative values)

        :param latent_dim: Dimension of the last layer of the policy (before the action layer)
        :param log_std_init: Initial value for the log standard deviation
        :return:
        """
        mean_actions = nn.Linear(latent_dim, self.action_dim)
        # TODO: allow action dependent std
        log_std = nn.Parameter(
            th.ones(self.action_dim) * log_std_init, requires_grad=True
        )
        return mean_actions, log_std

    def make_distribution(
        self, mean_actions: th.Tensor, log_std: th.Tensor
    ) -> "DiagGaussianDistribution":
        """
        Create the distribution given its parameters (mean, std)

        :param mean_actions:
        :param log_std:
        :return:
        """
        action_std = th.ones_like(mean_actions) * log_std.exp()
        self.distribution = Normal(mean_actions, action_std)
        return self

    def log_prob(self, actions: th.Tensor) -> th.Tensor:
        """
        Get the log probabilities of actions according to the distribution.
        Note that you must first call the ``proba_distribution()`` method.

        :param actions:
        :return:
        """
        log_prob = self.distribution.log_prob(actions)
        return sum_independent_dims(log_prob)

    def entropy(self) -> th.Tensor:
        return sum_independent_dims(self.distribution.entropy())

    def sample(self) -> th.Tensor:
        # Reparametrization trick to pass gradients
        return self.distribution.rsample()

    def mode(self) -> th.Tensor:
        return self.distribution.mean

    def actions_from_params(
        self, mean_actions: th.Tensor, log_std: th.Tensor, deterministic: bool = False
    ) -> th.Tensor:
        # Update the proba distribution
        self.proba_distribution(mean_actions, log_std)
        return self.get_actions(deterministic=deterministic)

    def log_prob_from_params(
        self, mean_actions: th.Tensor, log_std: th.Tensor
    ) -> Tuple[th.Tensor, th.Tensor]:
        """
        Compute the log probability of taking an action
        given the distribution parameters.

        :param mean_actions:
        :param log_std:
        :return:
        """
        actions = self.actions_from_params(mean_actions, log_std)
        log_prob = self.log_prob(actions)
        return actions, log_prob


class SquashedDiagGaussianDistribution(DiagGaussianDistribution):
    """
    Gaussian distribution with diagonal covariance matrix, followed by a squashing function (tanh) to ensure bounds.

    :param action_dim: Dimension of the action space.
    :param epsilon: small value to avoid NaN due to numerical imprecision.
    """

    def __init__(self, action_dim: int, epsilon: float = 1e-6):
        super().__init__(action_dim)
        # Avoid NaN (prevents division by zero or log of zero)
        self.epsilon = epsilon
        self.gaussian_actions = None

    def proba_distribution(
        self, mean_actions: th.Tensor, log_std: th.Tensor
    ) -> "SquashedDiagGaussianDistribution":
        super().proba_distribution(mean_actions, log_std)
        return self

    def log_prob(
        self, actions: th.Tensor, gaussian_actions: Optional[th.Tensor] = None
    ) -> th.Tensor:
        # Inverse tanh
        # Naive implementation (not stable): 0.5 * torch.log((1 + x) / (1 - x))
        # We use numpy to avoid numerical instability
        if gaussian_actions is None:
            # It will be clipped to avoid NaN when inversing tanh
            gaussian_actions = TanhBijector.inverse(actions)

        # Log likelihood for a Gaussian distribution
        log_prob = super().log_prob(gaussian_actions)
        # Squash correction (from original SAC implementation)
        # this comes from the fact that tanh is bijective and differentiable
        log_prob -= th.sum(th.log(1 - actions**2 + self.epsilon), dim=1)
        return log_prob

    def entropy(self) -> Optional[th.Tensor]:
        # No analytical form,
        # entropy needs to be estimated using -log_prob.mean()
        raise Exception("Call to entropy in squashed Diag Gaussian distribution")
        return None

    def sample(self) -> th.Tensor:
        # Reparametrization trick to pass gradients
        self.gaussian_actions = super().sample()
        return th.tanh(self.gaussian_actions)

    def mode(self) -> th.Tensor:
        self.gaussian_actions = super().mode()
        # Squash the output
        return th.tanh(self.gaussian_actions)

    def log_prob_from_params(
        self, mean_actions: th.Tensor, log_std: th.Tensor
    ) -> Tuple[th.Tensor, th.Tensor]:
        action = self.actions_from_params(mean_actions, log_std)
        log_prob = self.log_prob(action, self.gaussian_actions)
        return action, log_prob

    def get_ten_samples(self) -> List:
        action_list = []
        for i in range(10):
            action = self.sample()
            action_list.append(action)
        return action_list


class CategoricalDistribution(Distribution):
    """
    Categorical distribution for discrete actions.

    :param action_dim: Number of discrete actions
    """

    def __init__(self, action_dim: int):
        super().__init__()
        self.action_dim = action_dim

    def proba_distribution_net(self, latent_dim: int) -> nn.Module:
        """
        Create the layer that represents the distribution:
        it will be the logits of the Categorical distribution.
        You can then get probabilities using a softmax.

        :param latent_dim: Dimension of the last layer
            of the policy network (before the action layer)
        :return:
        """
        action_logits = nn.Linear(latent_dim, self.action_dim)
        return action_logits

    def proba_distribution(self, action_logits: th.Tensor) -> "CategoricalDistribution":
        self.distribution = Categorical(logits=action_logits)
        return self

    def log_prob(self, actions: th.Tensor) -> th.Tensor:
        return self.distribution.log_prob(actions)

    def entropy(self) -> th.Tensor:
        return self.distribution.entropy()

    def sample(self) -> th.Tensor:
        return self.distribution.sample()

    def mode(self) -> th.Tensor:
        return th.argmax(self.distribution.probs, dim=1)

    def actions_from_params(
        self, action_logits: th.Tensor, deterministic: bool = False
    ) -> th.Tensor:
        # Update the proba distribution
        self.proba_distribution(action_logits)
        return self.get_actions(deterministic=deterministic)

    def log_prob_from_params(
        self, action_logits: th.Tensor
    ) -> Tuple[th.Tensor, th.Tensor]:
        actions = self.actions_from_params(action_logits)
        log_prob = self.log_prob(actions)
        return actions, log_prob


class MultiCategoricalDistribution(Distribution):
    """
    MultiCategorical distribution for multi discrete actions.

    :param action_dims: List of sizes of discrete action spaces
    """

    def __init__(self, action_dims: List[int]):
        super().__init__()
        self.action_dims = action_dims

    def proba_distribution_net(self, latent_dim: int) -> nn.Module:
        """
        Create the layer that represents the distribution:
        it will be the logits (flattened) of the MultiCategorical distribution.
        You can then get probabilities using a softmax on each sub-space.

        :param latent_dim: Dimension of the last layer
            of the policy network (before the action layer)
        :return:
        """

        action_logits = nn.Linear(latent_dim, sum(self.action_dims))
        return action_logits

    def proba_distribution(
        self, action_logits: th.Tensor
    ) -> "MultiCategoricalDistribution":
        self.distribution = [
            Categorical(logits=split)
            for split in th.split(action_logits, tuple(self.action_dims), dim=1)
        ]
        return self

    def log_prob(self, actions: th.Tensor) -> th.Tensor:
        # Extract each discrete action and compute log prob for their respective distributions
        return th.stack(
            [
                dist.log_prob(action)
                for dist, action in zip(self.distribution, th.unbind(actions, dim=1))
            ],
            dim=1,
        ).sum(dim=1)

    def entropy(self) -> th.Tensor:
        return th.stack([dist.entropy() for dist in self.distribution], dim=1).sum(
            dim=1
        )

    def sample(self) -> th.Tensor:
        return th.stack([dist.sample() for dist in self.distribution], dim=1)

    def mode(self) -> th.Tensor:
        return th.stack(
            [th.argmax(dist.probs, dim=1) for dist in self.distribution], dim=1
        )

    def actions_from_params(
        self, action_logits: th.Tensor, deterministic: bool = False
    ) -> th.Tensor:
        # Update the proba distribution
        self.proba_distribution(action_logits)
        return self.get_actions(deterministic=deterministic)

    def log_prob_from_params(
        self, action_logits: th.Tensor
    ) -> Tuple[th.Tensor, th.Tensor]:
        actions = self.actions_from_params(action_logits)
        log_prob = self.log_prob(actions)
        return actions, log_prob


class BernoulliDistribution(Distribution):
    """
    Bernoulli distribution for MultiBinary action spaces.

    :param action_dim: Number of binary actions
    """

    def __init__(self, action_dims: int):
        super().__init__()
        self.action_dims = action_dims

    def proba_distribution_net(self, latent_dim: int) -> nn.Module:
        """
        Create the layer that represents the distribution:
        it will be the logits of the Bernoulli distribution.

        :param latent_dim: Dimension of the last layer
            of the policy network (before the action layer)
        :return:
        """
        action_logits = nn.Linear(latent_dim, self.action_dims)
        return action_logits

    def proba_distribution(self, action_logits: th.Tensor) -> "BernoulliDistribution":
        self.distribution = Bernoulli(logits=action_logits)
        return self

    def log_prob(self, actions: th.Tensor) -> th.Tensor:
        return self.distribution.log_prob(actions).sum(dim=1)

    def entropy(self) -> th.Tensor:
        return self.distribution.entropy().sum(dim=1)

    def sample(self) -> th.Tensor:
        return self.distribution.sample()

    def mode(self) -> th.Tensor:
        return th.round(self.distribution.probs)

    def actions_from_params(
        self, action_logits: th.Tensor, deterministic: bool = False
    ) -> th.Tensor:
        # Update the proba distribution
        self.proba_distribution(action_logits)
        return self.get_actions(deterministic=deterministic)

    def log_prob_from_params(
        self, action_logits: th.Tensor
    ) -> Tuple[th.Tensor, th.Tensor]:
        actions = self.actions_from_params(action_logits)
        log_prob = self.log_prob(actions)
        return actions, log_prob


class StateDependentNoiseDistribution(Distribution):
    """
    Distribution class for using generalized State Dependent Exploration (gSDE).
    Paper: https://arxiv.org/abs/2005.05719

    It is used to create the noise exploration matrix and
    compute the log probability of an action with that noise.

    :param action_dim: Dimension of the action space.
    :param full_std: Whether to use (n_features x n_actions) parameters
        for the std instead of only (n_features,)
    :param use_expln: Use ``expln()`` function instead of ``exp()`` to ensure
        a positive standard deviation (cf paper). It allows to keep variance
        above zero and prevent it from growing too fast. In practice, ``exp()`` is usually enough.
    :param squash_output: Whether to squash the output using a tanh function,
        this ensures bounds are satisfied.
    :param learn_features: Whether to learn features for gSDE or not.
        This will enable gradients to be backpropagated through the features
        ``latent_sde`` in the code.
    :param epsilon: small value to avoid NaN due to numerical imprecision.
    """

    def __init__(
        self,
        action_dim: int,
        full_std: bool = True,
        use_expln: bool = False,
        squash_output: bool = False,
        learn_features: bool = False,
        epsilon: float = 1e-6,
    ):
        super().__init__()
        self.action_dim = action_dim
        self.latent_sde_dim = None
        self.mean_actions = None
        self.log_std = None
        self.weights_dist = None
        self.exploration_mat = None
        self.exploration_matrices = None
        self._latent_sde = None
        self.use_expln = use_expln
        self.full_std = full_std
        self.epsilon = epsilon
        self.learn_features = learn_features
        if squash_output:
            self.bijector = TanhBijector(epsilon)
        else:
            self.bijector = None

    def get_std(self, log_std: th.Tensor) -> th.Tensor:
        """
        Get the standard deviation from the learned parameter
        (log of it by default). This ensures that the std is positive.

        :param log_std:
        :return:
        """
        if self.use_expln:
            # From gSDE paper, it allows to keep variance
            # above zero and prevent it from growing too fast
            below_threshold = th.exp(log_std) * (log_std <= 0)
            # Avoid NaN: zeros values that are below zero
            safe_log_std = log_std * (log_std > 0) + self.epsilon
            above_threshold = (th.log1p(safe_log_std) + 1.0) * (log_std > 0)
            std = below_threshold + above_threshold
        else:
            # Use normal exponential
            std = th.exp(log_std)

        if self.full_std:
            return std
        # Reduce the number of parameters:
        return th.ones(self.latent_sde_dim, self.action_dim).to(log_std.device) * std

    def sample_weights(self, log_std: th.Tensor, batch_size: int = 1) -> None:
        """
        Sample weights for the noise exploration matrix,
        using a centered Gaussian distribution.

        :param log_std:
        :param batch_size:
        """
        std = self.get_std(log_std)
        self.weights_dist = Normal(th.zeros_like(std), std)
        # Reparametrization trick to pass gradients
        self.exploration_mat = self.weights_dist.rsample()
        # Pre-compute matrices in case of parallel exploration
        self.exploration_matrices = self.weights_dist.rsample((batch_size,))

    def proba_distribution_net(
        self,
        latent_dim: int,
        log_std_init: float = -2.0,
        latent_sde_dim: Optional[int] = None,
    ) -> Tuple[nn.Module, nn.Parameter]:
        """
        Create the layers and parameter that represent the distribution:
        one output will be the deterministic action, the other parameter will be the
        standard deviation of the distribution that control the weights of the noise matrix.

        :param latent_dim: Dimension of the last layer of the policy (before the action layer)
        :param log_std_init: Initial value for the log standard deviation
        :param latent_sde_dim: Dimension of the last layer of the features extractor
            for gSDE. By default, it is shared with the policy network.
        :return:
        """
        # Network for the deterministic action, it represents the mean of the distribution
        mean_actions_net = nn.Linear(latent_dim, self.action_dim)
        # When we learn features for the noise, the feature dimension
        # can be different between the policy and the noise network
        self.latent_sde_dim = latent_dim if latent_sde_dim is None else latent_sde_dim
        # Reduce the number of parameters if needed
        log_std = (
            th.ones(self.latent_sde_dim, self.action_dim)
            if self.full_std
            else th.ones(self.latent_sde_dim, 1)
        )
        # Transform it to a parameter so it can be optimized
        log_std = nn.Parameter(log_std * log_std_init, requires_grad=True)
        # Sample an exploration matrix
        self.sample_weights(log_std)
        return mean_actions_net, log_std

    def proba_distribution(
        self, mean_actions: th.Tensor, log_std: th.Tensor, latent_sde: th.Tensor
    ) -> "StateDependentNoiseDistribution":
        """
        Create the distribution given its parameters (mean, std)

        :param mean_actions:
        :param log_std:
        :param latent_sde:
        :return:
        """
        # Stop gradient if we don't want to influence the features
        self._latent_sde = latent_sde if self.learn_features else latent_sde.detach()
        variance = th.mm(self._latent_sde**2, self.get_std(log_std) ** 2)
        self.distribution = Normal(mean_actions, th.sqrt(variance + self.epsilon))
        return self

    def log_prob(self, actions: th.Tensor) -> th.Tensor:
        if self.bijector is not None:
            gaussian_actions = self.bijector.inverse(actions)
        else:
            gaussian_actions = actions
        # log likelihood for a gaussian
        log_prob = self.distribution.log_prob(gaussian_actions)
        # Sum along action dim
        log_prob = sum_independent_dims(log_prob)

        if self.bijector is not None:
            # Squash correction (from original SAC implementation)
            log_prob -= th.sum(
                self.bijector.log_prob_correction(gaussian_actions), dim=1
            )
        return log_prob

    def entropy(self) -> Optional[th.Tensor]:
        if self.bijector is not None:
            # No analytical form,
            # entropy needs to be estimated using -log_prob.mean()
            return None
        return sum_independent_dims(self.distribution.entropy())

    def sample(self) -> th.Tensor:
        noise = self.get_noise(self._latent_sde)
        actions = self.distribution.mean + noise
        if self.bijector is not None:
            return self.bijector.forward(actions)
        return actions

    def mode(self) -> th.Tensor:
        actions = self.distribution.mean
        if self.bijector is not None:
            return self.bijector.forward(actions)
        return actions

    def get_noise(self, latent_sde: th.Tensor) -> th.Tensor:
        latent_sde = latent_sde if self.learn_features else latent_sde.detach()
        # Default case: only one exploration matrix
        if len(latent_sde) == 1 or len(latent_sde) != len(self.exploration_matrices):
            return th.mm(latent_sde, self.exploration_mat)
        # Use batch matrix multiplication for efficient computation
        # (batch_size, n_features) -> (batch_size, 1, n_features)
        latent_sde = latent_sde.unsqueeze(1)
        # (batch_size, 1, n_actions)
        noise = th.bmm(latent_sde, self.exploration_matrices)
        return noise.squeeze(1)

    def actions_from_params(
        self,
        mean_actions: th.Tensor,
        log_std: th.Tensor,
        latent_sde: th.Tensor,
        deterministic: bool = False,
    ) -> th.Tensor:
        # Update the proba distribution
        self.proba_distribution(mean_actions, log_std, latent_sde)
        return self.get_actions(deterministic=deterministic)

    def log_prob_from_params(
        self, mean_actions: th.Tensor, log_std: th.Tensor, latent_sde: th.Tensor
    ) -> Tuple[th.Tensor, th.Tensor]:
        actions = self.actions_from_params(mean_actions, log_std, latent_sde)
        log_prob = self.log_prob(actions)
        return actions, log_prob


class TanhBijector:
    """
    Bijective transformation of a probability distribution
    using a squashing function (tanh)
    TODO: use Pyro instead (https://pyro.ai/)

    :param epsilon: small value to avoid NaN due to numerical imprecision.
    """

    def __init__(self, epsilon: float = 1e-6):
        super().__init__()
        self.epsilon = epsilon

    @staticmethod
    def forward(x: th.Tensor) -> th.Tensor:
        return th.tanh(x)

    @staticmethod
    def atanh(x: th.Tensor) -> th.Tensor:
        """
        Inverse of Tanh

        Taken from Pyro: https://github.com/pyro-ppl/pyro
        0.5 * torch.log((1 + x ) / (1 - x))
        """
        return 0.5 * (x.log1p() - (-x).log1p())

    @staticmethod
    def inverse(y: th.Tensor) -> th.Tensor:
        """
        Inverse tanh.

        :param y:
        :return:
        """
        eps = th.finfo(y.dtype).eps
        # Clip the action to avoid NaN
        return TanhBijector.atanh(y.clamp(min=-1.0 + eps, max=1.0 - eps))

    def log_prob_correction(self, x: th.Tensor) -> th.Tensor:
        # Squash correction (from original SAC implementation)
        return th.log(1.0 - th.tanh(x) ** 2 + self.epsilon)


def make_proba_distribution(
    action_space: gym.spaces.Space,
    use_sde: bool = False,
    dist_kwargs: Optional[Dict[str, Any]] = None,
) -> Distribution:
    """
    Return an instance of Distribution for the correct type of action space

    :param action_space: the input action space
    :param use_sde: Force the use of StateDependentNoiseDistribution
        instead of DiagGaussianDistribution
    :param dist_kwargs: Keyword arguments to pass to the probability distribution
    :return: the appropriate Distribution object
    """
    if dist_kwargs is None:
        dist_kwargs = {}

    if isinstance(action_space, spaces.Box):
        assert len(action_space.shape) == 1, "Error: the action space must be a vector"
        cls = StateDependentNoiseDistribution if use_sde else DiagGaussianDistribution
        return cls(get_action_dim(action_space), **dist_kwargs)
    elif isinstance(action_space, spaces.Discrete):
        return CategoricalDistribution(action_space.n, **dist_kwargs)
    elif isinstance(action_space, spaces.MultiDiscrete):
        return MultiCategoricalDistribution(action_space.nvec, **dist_kwargs)
    elif isinstance(action_space, spaces.MultiBinary):
        return BernoulliDistribution(action_space.n, **dist_kwargs)
    else:
        raise NotImplementedError(
            "Error: probability distribution, not implemented for action space"
            f"of type {type(action_space)}."
            " Must be of type Gym Spaces: Box, Discrete, MultiDiscrete or MultiBinary."
        )


def kl_divergence(dist_true: Distribution, dist_pred: Distribution) -> th.Tensor:
    """
    Wrapper for the PyTorch implementation of the full form KL Divergence

    :param dist_true: the p distribution
    :param dist_pred: the q distribution
    :return: KL(dist_true||dist_pred)
    """
    # KL Divergence for different distribution types is out of scope
    assert (
        dist_true.__class__ == dist_pred.__class__
    ), "Error: input distributions should be the same type"

    # MultiCategoricalDistribution is not a PyTorch Distribution subclass
    # so we need to implement it ourselves!
    if isinstance(dist_pred, MultiCategoricalDistribution):
        assert (
            dist_pred.action_dims == dist_true.action_dims
        ), "Error: distributions must have the same input space"
        return th.stack(
            [
                th.distributions.kl_divergence(p, q)
                for p, q in zip(dist_true.distribution, dist_pred.distribution)
            ],
            dim=1,
        ).sum(dim=1)

    # Use the PyTorch kl_divergence implementation
    else:
        return th.distributions.kl_divergence(
            dist_true.distribution, dist_pred.distribution
        )

----!@#$----
utils\functional.py
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
#

import torch


def _index(tensor_3d, tensor_2d):
    x, y, z = tensor_3d.size()
    t = tensor_3d.reshape(x * y, z)
    tt = tensor_2d.reshape(x * y)
    v = t[torch.arange(x * y), tt]
    v = v.reshape(x, y)
    return v


def cumulated_reward(reward, done):
    T, B = done.size()
    done = done.detach().clone()

    v_done, index_done = done.float().max(0)
    assert v_done.eq(
        1.0
    ).all(), "[agents.rl.functional.cumulated_reward] Computing cumulated reward over unfinished trajectories"
    arange = torch.arange(T, device=done.device).unsqueeze(-1).repeat(1, B)
    index_done = index_done.unsqueeze(0).repeat(T, 1)

    mask = arange.le(index_done)
    reward = (reward * mask.float()).sum(0)
    return reward.mean().item()


def temporal_difference(critic, reward, must_bootstrap, discount_factor):
    target = discount_factor * critic[1:].detach() * must_bootstrap.float() + reward[1:]
    td = target - critic[:-1]
    to_add = torch.zeros(1, td.size()[1]).to(td.device)
    td = torch.cat([td, to_add], dim=0)
    return td


def doubleqlearning_temporal_difference(
    q, action, q_target, reward, must_bootstrap, discount_factor
):
    action_max = q.max(-1)[1]
    q_target_max = _index(q_target, action_max).detach()[1:]

    mb = must_bootstrap.float()
    target = reward[1:] + discount_factor * q_target_max * mb

    q = _index(q, action)[:-1]
    td = target - q
    to_add = torch.zeros(1, td.size()[1], device=td.device)
    td = torch.cat([td, to_add], dim=0)
    return td


def old_gae(critic, reward, must_bootstrap, discount_factor, gae_coef):
    mb = must_bootstrap.float()
    td = reward[1:] + discount_factor * mb * critic[1:].detach() - critic[:-1]
    # handling td0 case
    if gae_coef == 0.0:
        return td

    td_shape = td.shape[0]
    gae_val = td[-1]
    gaes = [gae_val]
    for t in range(td_shape - 2, -1, -1):
        gae_val = td[t] + discount_factor * gae_coef * mb[:-1][t] * gae_val
        gaes.append(gae_val)
    gaes = list([g.unsqueeze(0) for g in reversed(gaes)])
    gaes = torch.cat(gaes, dim=0)
    return gaes

def gae(reward, next_critic, must_bootstrap, critic, discount_factor, gae_coef):
    mb = must_bootstrap.int()
    td = reward + discount_factor * next_critic.detach() * mb - critic
    # handling td0 case
    if gae_coef == 0.0:
        return td

    td_shape = td.shape[0]
    gae_val = td[-1]
    gaes = [gae_val]
    for t in range(td_shape - 2, -1, -1):
        gae_val = td[t] + discount_factor * gae_coef * mb[t] * gae_val
        gaes.append(gae_val)
    gaes = list([g.unsqueeze(0) for g in reversed(gaes)])
    gaes = torch.cat(gaes, dim=0)
    return gaes


def compute_reinforce_loss(
    reward, action_probabilities, baseline, action, done, discount_factor
):
    batch_size = reward.size()[1]

    # Find the first occurrence of done for each element in the batch
    v_done, trajectories_length = done.float().max(0)
    trajectories_length += 1
    assert v_done.eq(1.0).all()
    max_trajectories_length = trajectories_length.max().item()
    # Shorten trajectories for faster computation
    reward = reward[:max_trajectories_length]
    action_probabilities = action_probabilities[:max_trajectories_length]
    baseline = baseline[:max_trajectories_length]
    action = action[:max_trajectories_length]

    # Create a binary mask to mask useless values (of size max_trajectories_length x batch_size)
    arange = (
        torch.arange(max_trajectories_length, device=done.device)
        .unsqueeze(-1)
        .repeat(1, batch_size)
    )
    mask = arange.lt(
        trajectories_length.unsqueeze(0).repeat(max_trajectories_length, 1)
    )
    reward = reward * mask

    # Compute discounted cumulated reward
    cum_reward = [torch.zeros_like(reward[-1])]
    for t in range(max_trajectories_length - 1, 0, -1):
        cum_reward.append(discount_factor + cum_reward[-1] + reward[t])
    cum_reward.reverse()
    cum_reward = torch.cat([c.unsqueeze(0) for c in cum_reward])

    # baseline loss
    g = baseline - cum_reward
    baseline_loss = (g) ** 2
    baseline_loss = (baseline_loss * mask).mean()

    # policy loss
    log_probabilities = _index(action_probabilities, action).log()
    policy_loss = log_probabilities * -g.detach()
    policy_loss = policy_loss * mask
    policy_loss = policy_loss.mean()

    # entropy loss
    entropy = torch.distributions.Categorical(action_probabilities).entropy() * mask
    entropy_loss = entropy.mean()

    return {
        "baseline_loss": baseline_loss,
        "policy_loss": policy_loss,
        "entropy_loss": entropy_loss,
    }

----!@#$----
utils\functionalb.py
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
#

import torch


def _index(tensor_3d, tensor_2d):
    x, y, z = tensor_3d.size()
    t = tensor_3d.reshape(x * y, z)
    tt = tensor_2d.reshape(x * y)
    v = t[torch.arange(x * y), tt]
    v = v.reshape(x, y)
    return v


def cumulated_reward(reward, done):
    timesteps, batch_size = done.size()
    done = done.detach().clone()

    v_done, index_done = done.float().max(0)
    assert v_done.eq(
        1.0
    ).all(), "[agents.rl.functional.cumulated_reward] Computing cumulated reward over unfinished trajectories"
    arange = (
        torch.arange(timesteps, device=done.device).unsqueeze(-1).repeat(1, batch_size)
    )
    index_done = index_done.unsqueeze(0).repeat(timesteps, 1)

    mask = arange.le(index_done)
    reward = (reward * mask.float()).sum(0)
    return reward.mean().item()


def temporal_difference(critic, reward, must_bootstrap, discount_factor):
    target = reward[:-1] + discount_factor * critic[1:].detach() * (
        must_bootstrap.float()
    )
    td = target - critic[:-1]
    to_add = torch.zeros(1, td.size()[1]).to(td.device)
    td = torch.cat([td, to_add], dim=0)
    return td


def doubleqlearning_temporal_difference(
    q, action, q_target, reward, must_bootstrap, discount_factor
):
    action_max = q.max(-1)[1]
    q_target_max = _index(q_target, action_max).detach()[1:]

    target = reward[:-1] + discount_factor * q_target_max * (must_bootstrap.float())

    q = _index(q, action)[:-1]
    td = target - q
    to_add = torch.zeros(1, td.size()[1], device=td.device)
    td = torch.cat([td, to_add], dim=0)
    return td


def gae(critic, reward, must_bootstrap, discount_factor, gae_coef):
    mb = must_bootstrap.float()
    td = reward[:-1] + discount_factor * critic[1:].detach() * mb - critic[:-1]
    # handling td0 case
    if gae_coef == 0.0:
        return td

    td_shape = td.shape[0]
    gae_val = td[-1]
    gae_vals = [gae_val]
    for t in range(td_shape - 2, -1, -1):
        gae_val = td[t] + discount_factor * gae_coef * mb[:-1][t] * gae_val
        gae_vals.append(gae_val)
    gae_vals = list([g.unsqueeze(0) for g in reversed(gae_vals)])
    gae_vals = torch.cat(gae_vals, dim=0)
    return gae_vals


def compute_reinforce_loss(
    reward, action_probabilities, baseline, action, done, discount_factor
):
    batch_size = reward.size()[1]

    # Find the first occurrence of done for each element in the batch
    v_done, trajectories_length = done.float().max(0)
    trajectories_length += 1
    assert v_done.eq(1.0).all()
    max_trajectories_length = trajectories_length.max().item()
    # Shorten trajectories for faster computation
    reward = reward[:-max_trajectories_length]
    action_probabilities = action_probabilities[:max_trajectories_length]
    baseline = baseline[:max_trajectories_length]
    action = action[:max_trajectories_length]

    # Create a binary mask to mask useless values (of size max_trajectories_length x batch_size)
    arange = (
        torch.arange(max_trajectories_length, device=done.device)
        .unsqueeze(-1)
        .repeat(1, batch_size)
    )
    mask = arange.lt(
        trajectories_length.unsqueeze(0).repeat(max_trajectories_length, 1)
    )
    reward = reward * mask

    # Compute discounted cumulated reward
    cum_reward = [torch.zeros_like(reward[-1])]
    for t in range(max_trajectories_length - 1, 0, -1):
        cum_reward.append(discount_factor + cum_reward[-1] + reward[t])
    cum_reward.reverse()
    cum_reward = torch.cat([c.unsqueeze(0) for c in cum_reward])

    # baseline loss
    g = baseline - cum_reward
    baseline_loss = g**2
    baseline_loss = (baseline_loss * mask).mean()

    # policy loss
    log_probabilities = _index(action_probabilities, action).log()
    policy_loss = log_probabilities * -g.detach()
    policy_loss = policy_loss * mask
    policy_loss = policy_loss.mean()

    # entropy loss
    entropy = torch.distributions.Categorical(action_probabilities).entropy() * mask
    entropy_loss = entropy.mean()

    return {
        "baseline_loss": baseline_loss,
        "policy_loss": policy_loss,
        "entropy_loss": entropy_loss,
    }

----!@#$----
utils\logger.py
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
#

import bz2
import pickle
import time
from typing import Optional, Union

import numpy as np

from omegaconf import DictConfig
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm

try:
    import wandb

    class WandbLogger:
        """A wandb logger"""

        def __init__(
            self,
            project: Optional[str] = None,
            group: Optional[str] = None,
            job_type: Optional[str] = None,
            tags: str = "",
            every_n_seconds: bool = None,
            verbose: bool = False,
            log_loss: bool = False,
            **kwargs
        ) -> None:
            wandb.login()
            wandb.init(
                project=project, group=group, job_type=job_type, tags=tags.split("_")
            )
            self.logs = {}
            self.every_n_seconds = every_n_seconds
            self.save_time = -float("inf")
            self.verbose = verbose

        def _to_dict(self, h: Union[dict, DictConfig]) -> dict:
            if isinstance(h, dict) or isinstance(h, DictConfig):
                return {k: self._to_dict(v) for k, v in h.items()}
            else:
                return h

        def save_hps(self, hps, verbose=True) -> None:
            print(hps)
            wandb.config.update(self._to_dict(hps))

        def get_logger(self, prefix):
            return self

        def message(self, msg, from_name="") -> None:
            print("[", from_name, "]: ", msg)

        def add_images(self, name, value, iteration) -> None:
            pass

        def add_figure(self, name, value, iteration) -> None:
            pass

        def add_scalar(self, name, value, iteration) -> None:
            if self.verbose:
                print("['" + name + "' at " + str(iteration) + "] = " + str(value))
            self.logs[name] = value
            self.logs["iteration"] = iteration
            wandb.log(self.logs, commit=True)
            self.logs = {}

        def add_video(self, name, value, iteration, fps=10) -> None:
            pass

        def add_html(self, name, value) -> None:
            wandb.log({name: wandb.Html(value)})

        def close(self, exit_code=0) -> None:
            wandb.finish(exit_code=exit_code)

except ImportError:

    class WandbLogger:
        def __init__(self, *args, **kwargs):
            raise RuntimeError("wandb not installed, please do not use WandbLogger")


class TFPrefixLogger:
    def __init__(self, prefix, logger):
        self.logger = logger
        self.prefix = prefix

    def add_images(self, name, value, iteration):
        self.logger.add_images(self.prefix + name, value, iteration)

    def add_figure(self, name, value, iteration):
        self.logger.add_figure(self.prefix + name, value, iteration)

    def add_scalar(self, name, value, iteration):
        self.logger.add_scalar(self.prefix + name, value, iteration)

    def add_video(self, name, value, iteration, fps=10):
        self.logger.add_video(self.prefix + name, value, iteration, fps)

    def message(self, msg, from_name=""):
        self.logger.message(msg, from_name=self.prefix + from_name)

    def debug(self, msg, from_name=""):
        self.logger.debug(msg, from_name=self.prefix + from_name)

    def get_logger(self, prefix):
        return TFPrefixLogger(self.prefix + prefix, self.logger)

    def close(self):
        pass


class TFLogger(SummaryWriter):
    """A logger that stores informations both in tensorboard and CSV formats"""

    def __init__(
        self,
        log_dir=None,
        hps={},
        cache_size=10000,
        every_n_seconds=None,
        modulo=1,
        verbose=False,
        use_zip=True,
        save_tensorboard=True,
    ):
        SummaryWriter.__init__(self, log_dir=log_dir)
        self.save_tensorboard = save_tensorboard
        self.use_zip = use_zip
        self.save_every = cache_size
        self.modulo = modulo
        self.written_values = {}
        self.log_dir = log_dir
        self.every_n_seconds = every_n_seconds
        if self.every_n_seconds is None:
            print(
                "[Deprecated] bbrl.utils.logger: use 'every_n_seconds' instead of cache_size"
            )
        else:
            self.save_every = None
        self._start_time = time.time()
        self.verbose = verbose

        self.picklename = log_dir + "/db.pickle.bzip2"
        if not self.use_zip:
            self.picklename = log_dir + "/db.pickle"
        self.to_pickle = []
        if len(hps) > 0:
            f = open(log_dir + "/params.json", "wt")
            f.write(str(hps) + "\n")
            f.close()

            outfile = open(log_dir + "/params.pickle", "wb")
            pickle.dump(hps, outfile)
            outfile.close()
            self.add_text("Hyperparameters", str(hps))

    def _omegaconf_to_dict(self, hps):
        d = {}
        for k, v in hps.items():
            if isinstance(v, DictConfig):
                d[k] = self._omegaconf_to_dict(v)
            else:
                d[k] = v
        return d

    def _to_dict(self, h):
        if isinstance(h, dict):
            return {k: self._to_dict(v) for k, v in h.items()}
        if isinstance(h, DictConfig):
            return {k: self._to_dict(v) for k, v in h.items()}
        else:
            return h

    def save_hps(self, hps, verbose=True):
        hps = self._to_dict(hps)
        if verbose:
            print(hps)
        f = open(self.log_dir + "/params.json", "wt")
        f.write(str(hps) + "\n")
        f.close()

        outfile = open(self.log_dir + "/params.pickle", "wb")
        pickle.dump(hps, outfile)
        outfile.close()
        self.add_text("Hyperparameters", str(hps))

    def get_logger(self, prefix):
        return TFPrefixLogger(prefix, self)

    def message(self, msg, from_name=""):
        print("[", from_name, "]: ", msg)

    def debug(self, msg, from_name=""):
        print("[DEBUG] [", from_name, "]: ", msg)

    def _to_pickle(self, name, value, iteration):
        self.to_pickle.append((name, iteration, value))

        if self.every_n_seconds is not None:
            if time.time() - self._start_time > self.every_n_seconds:
                if self.use_zip:
                    f = bz2.BZ2File(self.picklename, "ab")
                    pickle.dump(self.to_pickle, f)
                    f.close()
                else:
                    f = open(self.picklename, "ab")
                    pickle.dump(self.to_pickle, f)
                    f.close()
                self._start_time = time.time()
                self.to_pickle = []
        else:
            if len(self.to_pickle) > self.save_every:
                if self.use_zip:
                    f = bz2.BZ2File(self.picklename, "ab")
                    pickle.dump(self.to_pickle, f)
                    f.close()
                else:
                    f = open(self.picklename, "ab")
                    pickle.dump(self.to_pickle, f)
                    f.close()
                self.to_pickle = []

    def add_images(self, name, value, iteration):
        iteration = int(iteration / self.modulo) * self.modulo
        if (name, iteration) in self.written_values:
            return
        else:
            self.written_values[(name, iteration)] = True

        self._to_pickle(name, value, iteration)
        if self.save_tensorboard:
            SummaryWriter.add_images(self, name, value, iteration)

    def add_figure(self, name, value, iteration):
        iteration = int(iteration / self.modulo) * self.modulo
        if (name, iteration) in self.written_values:
            return
        else:
            self.written_values[(name, iteration)] = True

        self._to_pickle(name, value, iteration)
        if self.save_tensorboard:
            SummaryWriter.add_figure(self, name, value, iteration)

    def add_scalar(self, name, value, iteration):
        iteration = int(iteration / self.modulo) * self.modulo
        if (name, iteration) in self.written_values:
            return
        else:
            self.written_values[(name, iteration)] = True

        self._to_pickle(name, value, iteration)
        if self.verbose:
            print("['" + name + "' at " + str(iteration) + "] = " + str(value))

        if isinstance(value, int) or isinstance(value, float):
            if self.save_tensorboard:
                SummaryWriter.add_scalar(self, name, value, iteration)

    def add_video(self, name, value, iteration, fps=10):
        iteration = int(iteration / self.modulo) * self.modulo
        if (name, iteration) in self.written_values:
            return
        else:
            self.written_values[(name, iteration)] = True

        self._to_pickle(name, value.numpy(), iteration)
        if self.save_tensorboard:
            SummaryWriter.add_video(self, name, value, iteration, fps=fps)

    def close(self):
        if len(self.to_pickle) > 0:
            if self.use_zip:
                f = bz2.BZ2File(self.picklename, "ab")
                pickle.dump(self.to_pickle, f)
                f.close()
            else:
                f = open(self.picklename, "ab")
                pickle.dump(self.to_pickle, f)
                f.close()
            self.to_pickle = []

        SummaryWriter.close(self)

        f = open(self.log_dir + "/done", "wt")
        f.write("Done\n")
        f.close()


class Log:
    def __init__(self, hps, values):
        self.hps = hps
        self.values = values
        max_length = max([len(v) for v in self.values])
        for k in values:
            while len(values[k]) < max_length:
                values[k].append(None)
        self.length = max_length

    def to_xy(self, name):
        assert name in self.values
        x, y = [], []
        for k, v in enumerate(self.values[name]):
            if v is not None:
                x.append(k)
                y.append(v)
        return x, y

    def to_dataframe(self, with_hps=False):
        import pandas as pd

        max_len = np.max([len(k) for v, k in self.values.items()])
        nv = {}
        for k, v in self.values.items():
            nnv = [None] * (max_len - len(v))
            nv[k] = v + nnv
        self.values = nv
        it = list(np.arange(max_len))
        d = {**self.values, **{"iteration": it}}
        _pd = pd.DataFrame(d)
        if with_hps:
            for k in self.hps:
                _pd["_hp/" + k] = self.hps[k]
        return _pd

    def get_at(self, name, iteration):
        return self.values[name][iteration]

    def get(self, name, keep_none=False):
        v = self.values[name]
        if not keep_none:
            return [k for k in v if k is not None]
        else:
            return v

    def replace_none_(self, name):
        v = self.values[name]
        last_v = None
        first_v = None
        r = []
        for k in range(len(v)):
            if v[k] is None:
                r.append(last_v)
            else:
                r.append(v[k])
                if last_v is None:
                    first_v = v[k]
                last_v = v[k]

        p = 0
        while r[p] is None:
            r[p] = first_v
            p += 1
        self.values[name] = r

    def max(self, name):
        v = self.values[name]
        vv = [k for k in v if k is not None]
        return np.max(vv)

    def min(self, name):
        v = self.values[name]
        vv = [k for k in v if k is not None]
        return np.min(vv)

    def argmin(self, name):
        v = self.values[name]
        vv = [k for k in v if k is not None]
        _max = np.max(vv)

        for k in range(len(v)):
            if v[k] is None:
                vv.append(_max + 1.0)
            else:
                vv.append(v[k])
        return np.argmin(vv)

    def argmax(self, name):
        v = self.values[name]
        vv = [k for k in v if k is not None]
        _min = np.min(vv)
        vv = []
        for k in range(len(v)):
            if v[k] is None:
                vv.append(_min - 1.0)
            else:
                vv.append(v[k])
        return np.argmax(vv)


class Logs:
    def __init__(self):
        self.logs = []
        self.hp_names = None
        self.filenames = []

    def _add(self, log):
        self.hp_names = {k: True for k in log.hps}
        for log in self.logs:
            for k in log.hps:
                if k not in log.hps:
                    log.hps[k] = "none"

        self.logs.append(log)

    def add(self, logs):
        if isinstance(logs, Log):
            self._add(logs)
        else:
            for log in logs:
                self._add(log)

    def max(self, function):
        alls = [function(log) for log in self.logs]
        idx = np.argmax(alls)
        return self.logs[idx]

    def columns(self):
        return list(self.logs[0].values)

    def hps(self):
        return list(self.hp_names)

    def size(self):
        return len(self.logs)

    def filter(self, hp_name, test_fn):
        logs = Logs()
        if not callable(test_fn):
            for log in self.logs:
                h = log.hps[hp_name]
                if h == test_fn:
                    logs.add(log)
        else:
            for log in self.logs:
                if test_fn(log.hps[hp_name]):
                    logs.add(log)
        return logs

    def unique_hps(self, name):
        r = {}
        for log in self.logs:
            v = log.hps[name]
            r[v] = 1
        return list(r.keys())

    def __len__(self):
        return len(self.logs)

    def to_dataframe(self):
        import pandas as pd

        rdf = None
        for log in tqdm(self.logs):
            df = log.to_dataframe(with_hps=True)
            if rdf is None:
                rdf = [df]
            else:
                rdf.append(df)
        return pd.concat(rdf)

    # def plot(self, y, x, hue=None, style=None, row=None, col=None, kind="line"):


def flattify(d):
    r = {}
    for k, v in d.items():
        if isinstance(v, dict):
            rr = flattify(v)
            rrr = {k + "/" + kk: rrr for kk, rrr in rr.items()}
            r = {**r, **rrr}
        elif isinstance(v, list):
            r[k] = str(v)
        else:
            r[k] = v
    return r


def read_log(directory, use_bz2=True, debug=False):
    if use_bz2:
        picklename = directory + "/db.pickle.bzip2"
        f = bz2.BZ2File(picklename, "rb")
    else:
        picklename = directory + "/db.pickle"
        f = open(picklename, "rb")
    values = {}

    try:
        while True:
            a = pickle.load(f)
            if a is not None:
                for name, iteration, value in a:
                    # print(name,iteration,value)
                    if debug:
                        print(name, value, type(value))
                    if isinstance(value, np.int64):
                        value = int(value)
                    if (
                        isinstance(value, int)
                        or isinstance(value, float)
                        or isinstance(value, str)
                    ):
                        if name not in values:
                            values[name] = []
                        while len(values[name]) < iteration + 1:
                            values[name].append(None)
                        values[name][iteration] = value
    except pickle.PickleError:
        f.close()
    f = open(directory + "/params.pickle", "rb")
    params = pickle.load(f)
    params = eval(str(params))
    params = flattify(params)
    f.close()
    log = Log(params, values)
    log.from_directory = directory
    # f=open(directory+"/fast.pickle","wb")
    # pickle.dump(log,f)
    # f.close()

    return log


def get_directories(directory, use_bz2=True):
    import os.path

    name = "db.pickle"
    if use_bz2:
        name = "db.pickle.bzip2"

    return [
        dirpath
        for dirpath, dirnames, filenames in os.walk(directory)
        if name in filenames
    ]


def read_directories(directories, use_bz2=True):
    logs = Logs()
    for dirpath in directories:
        log = read_log(dirpath, use_bz2)
        logs.add(log)
    print("Found %d logs" % logs.size())
    return logs


def read_directory(directory, use_bz2=True):
    import os.path

    logs = Logs()
    name = "db.pickle"
    if use_bz2:
        name = "db.pickle.bzip2"
    for dirpath, dirnames, filenames in os.walk(directory):
        if name in filenames:
            log = read_log(dirpath, use_bz2)
            logs.add(log)
    print("Found %d logs" % logs.size())
    return logs


def _create_col(df, hps, _name):
    vs = []
    for k, v in df.groupby(hps):
        n = {hps[i]: k[i] for i in range(len(hps))}
        v = v.copy()
        name = ",".join([str(k) + "=" + str(n[k]) for k in n])
        print(name)
        print(_name)
        v[_name] = name
        vs.append(v)
    import pandas as pd

    return pd.concat(vs)


def plot_dataframe(
    df, y, x="iteration", hue=None, style=None, row=None, col=None, kind="line"
):
    import seaborn as sns

    cols = [y, x]
    if isinstance(row, list):
        cols += row
    else:
        cols += [row]
    if isinstance(col, list):
        cols += col
    else:
        cols += [col]
    if isinstance(style, list):
        cols += style
    else:
        cols += [style]
    if isinstance(hue, list):
        cols += hue
    else:
        cols += [hue]
    cols = [c for c in cols if c is not None]
    df = df[cols].dropna()

    if isinstance(row, list):
        df = _create_col(df, row, "__row")
        row = "__row"
    if isinstance(col, list):
        df = _create_col(df, col, "__col")
        col = "__col"
    if isinstance(style, list):
        df = _create_col(df, style, "__style")
        style = "__style"
    if isinstance(hue, list):
        df = _create_col(df, hue, "__hue")
        hue = "__hue"

    # df = convert_iteration_to_steps(df)

    sns.relplot(x=x, y=y, hue=hue, style=style, row=row, col=col, data=df, kind=kind)

----!@#$----
utils\replay_buffer.py
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
#

import copy
import torch

from bbrl.workspace import Workspace


class ReplayBuffer:
    def __init__(self, max_size, device=torch.device("cpu")):
        self.max_size = int(max_size)
        self.variables = None
        self.position = 0
        self.is_full = False
        self.device = device

    def init_workspace(self, all_tensors):
        """
        Create an array to stores workspace based on the given all_tensors keys.
        shape of stores tensors : [key] => [self.max_size][time_size][key_dim]
        Makes a copy of the input content
        """

        if self.variables is None:
            self.variables = {}
            for k, v in all_tensors.items():
                s = list(v.size())
                s[1] = self.max_size
                _s = copy.deepcopy(s)
                s[0] = _s[1]
                s[1] = _s[0]

                tensor = torch.zeros(*s, dtype=v.dtype, device=self.device)
                self.variables[k] = tensor
            self.is_full = False
            self.position = 0

    def _insert(self, k, indexes, v):
        self.variables[k][indexes] = v.detach().moveaxis((0, 1), (1, 0))

    def put(self, workspace):
        """
        Add a the content of a workspace to the replay buffer.
        The given workspace must have keys of shape : [time_size][batch_size][key_dim]
        """

        new_data = {
            k: workspace.get_full(k).detach().to(self.device) for k in workspace.keys()
        }
        self.init_workspace(new_data)

        batch_size = None
        arange = None
        indexes = None

        for k, v in new_data.items():
            if batch_size is None:
                batch_size = v.size()[1]
                # print(f"{k}: batch size : {batch_size}")
                # print("pos", self.position)
            if self.position + batch_size < self.max_size:
                # The case where the batch can be inserted before the end of the replay buffer
                if indexes is None:
                    indexes = torch.arange(batch_size) + self.position
                    arange = torch.arange(batch_size)
                    self.position = self.position + batch_size
                indexes = indexes.to(dtype=torch.long, device=v.device)
                arange = arange.to(dtype=torch.long, device=v.device)
                # print("insertion standard:", indexes)
                # # print("v shape", v.detach().shape)
                self._insert(k, indexes, v)
            else:
                # The case where the batch cannot be inserted before the end of the replay buffer
                # A part is at the end, the other part is in the beginning
                self.is_full = True
                # the number of data at the end of the RB
                batch_end_size = self.max_size - self.position
                # the number of data at the beginning of the RB
                batch_begin_size = batch_size - batch_end_size
                if indexes is None:
                    # print(f"{k}: batch size : {batch_size}")
                    # print("pos", self.position)
                    # the part of the indexes at the end of the RB
                    indexes = torch.arange(batch_end_size) + self.position
                    arange = torch.arange(batch_end_size)
                    # the part of the indexes at the beginning of the RB
                    # print("insertion intermediate computed:", indexes)
                    indexes = torch.cat((indexes, torch.arange(batch_begin_size)), 0)
                    arange = torch.cat((arange, torch.arange(batch_begin_size)), 0)
                    # print("insertion full:", indexes)
                    self.position = batch_begin_size
                indexes = indexes.to(dtype=torch.long, device=v.device)
                arange = arange.to(dtype=torch.long, device=v.device)
                self._insert(k, indexes, v)

    def size(self):
        if self.is_full:
            return self.max_size
        else:
            return self.position

    def print_obs(self):
        print(f"position: {self.position}")
        print(self.variables["env/env_obs"])

    def get_shuffled(self, batch_size):
        who = torch.randint(
            low=0, high=self.size(), size=(batch_size,), device=self.device
        )
        workspace = Workspace()
        for k in self.variables:
            workspace.set_full(k, self.variables[k][who].transpose(0, 1))

        return workspace

    def to(self, device):
        n_vars = {k: v.to(device) for k, v in self.variables.items()}
        self.variables = n_vars

----!@#$----
utils\utils.py
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
#

import copy
from typing import Callable, List, Union

import torch

try:
    from gym.spaces import Box, Discrete
    from gym.wrappers import TimeLimit
except ModuleNotFoundError:

    class Box:
        pass

    class Discrete:
        pass

    class TimeLimit:
        pass


from omegaconf import DictConfig

from bbrl.agents.agent import Agent
from bbrl.workspace import Workspace
from bbrl import instantiate_class
from bbrl.agents.asynchronous import AsynchronousAgent


def get_env_dimensions(env) -> tuple:
    env = instantiate_class(env)
    obs_dim = env.observation_space.shape[0]
    if isinstance(env.action_space, Discrete):
        action_dim = env.action_space.n
        del env
        return obs_dim, action_dim

    elif isinstance(env.action_space, Box):
        action_dim = env.action_space.shape[0]
        max_action = env.action_space.high[0]
        del env
        return obs_dim, action_dim, max_action
    else:
        raise Exception(f"{type(env.action_space)} unknown")


def make_gym_env(max_episode_steps, env_name):
    import gym

    return TimeLimit(gym.make(env_name), max_episode_steps=max_episode_steps)


def soft_param_update(network_to_update, network, rho):
    for n_to_update, p_net in zip(network_to_update.parameters(), network.parameters()):
        n_to_update.data.copy_(rho * p_net.data + (1 - rho) * n_to_update.data)


# dict configs utils :
def key_path_in_dict(nested_dict: dict, key_path: str):
    """
    Check if a sequences of keys exists in a nested dict
    """
    try:
        keys = key_path.split(".")
        rv = nested_dict
        for key in keys:
            rv = rv[key]
        return True
    except KeyError:
        return False


def set_value_with_key_path(nested_dict: DictConfig, key_path: str, value):
    keys = key_path.split(".")
    for key in keys[:-1]:
        nested_dict = nested_dict[key]
    nested_dict[keys[-1]] = value


# Salina additions ####


# need to check if this function works well using cuda
def vector_to_parameters(vec: torch.Tensor, parameters) -> None:
    r"""Convert one vector to the parameters

    Args:
        vec (Tensor): a single vector represents the parameters of a model.
        parameters (Iterable[Tensor]): an iterator of Tensors that are the
            parameters of a model.
    """
    # Ensure vec of type Tensor
    if not isinstance(vec, torch.Tensor):
        raise TypeError(
            "expected torch.Tensor, but got: {}".format(torch.typename(vec))
        )

    # Pointer for slicing the vector for each parameter
    pointer = 0
    for param in parameters:
        # The length of the parameter
        num_param = param.numel()
        # Slice the vector, reshape it, and replace the old data of the parameter
        param.data.copy_(vec[pointer : pointer + num_param].view_as(param).data)

        # Increment the pointer
        pointer += num_param


# !!!! nRemoteParamAgent,  Not ready (WIP) !!!
class nRemoteParamAgent(Agent):
    """
    Class that allows to evaluate N (different) individuals with m processes
    The user have to provide:
       1/ the aquisition agent list or template
       2/ list of parameters for each of the individual of the pop
       3/ the function that apply the parameters to the acquisition agent
    This implementation is based on the  Asynchronous agents
    (I think another implementation could use the NRemote agent
    maybe by slicing the shared workspace to separate the experiences
    collected by each individual)
    """

    def __init__(self, acq_agent: Agent, n_process: int, name: str = "") -> None:
        """
        Implements a list of agent which are executed aynchronously in another process.
        Each agent can be parametrized by specific parameters and will returns it's own workspace.
        acq_agent : an instance of the agent that will be runned over each processes
        n_process :
        apply_params : a function f(acq_agent, param) => acq_agent
                       Allow to update each of the agent with a specific set of parameters.
        """
        super().__init__(name)
        self.async_agents = []
        self.n_process = n_process
        for i in range(n_process):
            async_agent = AsynchronousAgent(copy.deepcopy(acq_agent))
            self.async_agents.append(async_agent)

    def __call__(self, params: list, apply_params: Callable, **kwargs):
        self.workspaces = []
        nb_agent_to_launch = len(params)
        pool = []
        to_launch_id = 0

        def launch_agent(agent, to_launch_id):
            apply_params(agent, params[to_launch_id])
            agent(**kwargs)
            to_launch_id += 1
            pool.append(agent)

        for i in range(min(nb_agent_to_launch, self.n_process)):
            launch_agent(self.async_agents[i], i)

        while True:
            for agent in pool:
                if not agent.is_running():
                    workspace = agent.get_workspace()
                    if workspace:
                        self.workspaces.append(workspace)
                        if len(self.workspaces) == nb_agent_to_launch:
                            return
                    if to_launch_id < nb_agent_to_launch:
                        last_launched_id = len(self.workspaces) - 1
                        apply_params(agent, params[last_launched_id])
                        agent(**kwargs)
                        last_launched_id += 1
                    else:
                        pool.remove(agent)

    def get_workspaces(self) -> List[Workspace]:
        try:
            return self.workspaces
        except AttributeError:
            raise Exception(
                "The nRemoteParamAgent has not been called yet, workspaces have not been created"
            )

    def close(self) -> None:
        for a in self.async_agents:
            a.close()


# !!!! nRemoteDistinctAgents Not functionnal !!!
# To my knownledge you can't
# change the content of an async agent
class nRemoteDistinctAgents(Agent):
    """
    Class that allows to evaluate N (different) individuals with m processes
    Basic usage :
    remote = nRemoteDistinctAgents(n_process)
    remote(acq_agent_list,)
    The user have to provide:
        1/ a list of acqusition_agent that will be copied to remotes
    This implementation is based on the  Asynchronous agents
    (i think another implementation could use the Nremote agent
    maybe by slicing the shared workspace to separate the experiences
    collected by each individual)
    """

    def __init__(self, n_process: int, name: str = "") -> None:
        """
        Implements a list of agent which are executed aynchronously in another process.
        Each agent can be parametrized by specific parameters and will returns it's own
        workspace.
        acq_agent : an instance of the agent that will be runned over each processes
        n_process :
        apply_params : a function f(acq_agent, param) => acq_agent
                       Allow to update each of the agent with a specific set of parameters.
        """
        super().__init__(name)
        self.async_agents = []
        self.n_process = n_process
        for i in range(n_process):
            async_agent = AsynchronousAgent(None)
            self.async_agents.append(async_agent)

    def __call__(
        self, acq_agents: List[Agent], agents_args: Union[list, dict, None], **kwargs
    ):
        def get_agent_args(agent_id):
            if agents_args is None:
                args = {}
            elif isinstance(agents_args, dict):
                args = agents_args
            elif isinstance(agents_args, list):
                args = agents_args[agent_id]
            else:
                raise Exception("Unsupported")
            return args

        self.workspaces = []
        nb_agent_to_launch = len(acq_agents)
        to_launch_id = 0

        pool = []
        for _ in range(min(self.n_process, nb_agent_to_launch)):
            args = get_agent_args(to_launch_id)
            self.async_agents[to_launch_id].agent = acq_agents[to_launch_id]
            self.async_agents[to_launch_id](**args, **kwargs)
            pool.append(self.async_agents[to_launch_id])
            to_launch_id += 1

        while len(self.workspaces) < nb_agent_to_launch:
            j = 0
            for async_agent in pool:
                if not async_agent.is_running():
                    workspace = async_agent.get_workspace()
                    self.workspaces.append(workspace)
                    # print(f'process {j} finished total {len(self.workspaces)}/{nb_agent_to_launch}')
                    if len(self.workspaces) == nb_agent_to_launch:
                        return
                    if to_launch_id < nb_agent_to_launch:
                        async_agent.agent = acq_agents[to_launch_id]
                        args = get_agent_args(to_launch_id)
                        async_agent(**args, **kwargs)
                        # print(f'process {j} launched for agent {to_launch_id}')
                        to_launch_id += 1
                    else:
                        pool.remove(async_agent)
                j += 1

    def get_workspaces(self) -> List[Workspace]:
        try:
            return self.workspaces
        except AttributeError:
            raise Exception(
                "The nRemoteParamAgent has not been called yet, workspaces have not been created"
            )

    def close(self) -> None:
        for a in self.async_agents:
            a.close()


def is_vec_of_ones(vec) -> bool:
    # print(vec)
    # print(vec.shape)
    for subvec in vec:
        for i in subvec:
            # print(f"{i} ")
            if not i == 1:
                # print(f"{i} is not one")
                return False
    return True

----!@#$----
utils\__init__.py

----!@#$----
utils\gymnasium\distributions.py
""" Probability distributions
This file has been imported from Stable baselines 3 and adapted to the BBRL context """

import numpy as np

from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional, Tuple, Union

import gymnasium as gym
from gymnasium import spaces

import torch as th
from torch import nn
from torch.distributions import Bernoulli, Categorical, Normal


def get_action_dim(action_space: spaces.Space) -> int:
    """
    Get the dimension of the action space.

    :param action_space:
    :return:
    """
    if isinstance(action_space, spaces.Box):
        return int(np.prod(action_space.shape))
    elif isinstance(action_space, spaces.Discrete):
        # Action is an int
        return 1
    elif isinstance(action_space, spaces.MultiDiscrete):
        # Number of discrete actions
        return int(len(action_space.nvec))
    elif isinstance(action_space, spaces.MultiBinary):
        # Number of binary actions
        return int(action_space.n)
    else:
        raise NotImplementedError(f"{action_space} action space is not supported")


class Distribution(ABC):
    """Abstract base class for distributions."""

    def __init__(self):
        super().__init__()
        self.distribution = None

    @abstractmethod
    def proba_distribution_net(
        self, *args, **kwargs
    ) -> Union[nn.Module, Tuple[nn.Module, nn.Parameter]]:
        """Create the layers and parameters that represent the distribution.

        Subclasses must define this, but the arguments and return type vary between
        concrete classes."""

    @abstractmethod
    def proba_distribution(self, *args, **kwargs) -> "Distribution":
        """Set parameters of the distribution.

        :return: self
        """

    @abstractmethod
    def log_prob(self, x: th.Tensor) -> th.Tensor:
        """
        Returns the log likelihood

        :param x: the taken action
        :return: The log likelihood of the distribution
        """

    @abstractmethod
    def entropy(self) -> Optional[th.Tensor]:
        """
        Returns Shannon's entropy of the probability

        :return: the entropy, or None if no analytical form is known
        """

    @abstractmethod
    def sample(self) -> th.Tensor:
        """
        Returns a sample from the probability distribution

        :return: the stochastic action
        """

    @abstractmethod
    def mode(self) -> th.Tensor:
        """
        Returns the most likely action (deterministic output)
        from the probability distribution

        :return: the stochastic action
        """

    def get_actions(self, deterministic: bool = False) -> th.Tensor:
        """
        Return actions according to the probability distribution.

        :param deterministic:
        :return:
        """
        if deterministic:
            return self.mode()
        return self.sample()

    @abstractmethod
    def actions_from_params(self, *args, **kwargs) -> th.Tensor:
        """
        Returns samples from the probability distribution
        given its parameters.

        :return: actions
        """

    @abstractmethod
    def log_prob_from_params(self, *args, **kwargs) -> Tuple[th.Tensor, th.Tensor]:
        """
        Returns samples and the associated log probabilities
        from the probability distribution given its parameters.

        :return: actions and log prob
        """


def sum_independent_dims(tensor: th.Tensor) -> th.Tensor:
    """
    Continuous actions are usually considered to be independent,
    so we can sum components of the ``log_prob`` or the entropy.

    :param tensor: shape: (n_batch, n_actions) or (n_batch,)
    :return: shape: (n_batch,)
    """
    if len(tensor.shape) > 1:
        tensor = tensor.sum(dim=1)
    else:
        tensor = tensor.sum()
    return tensor


class DiagGaussianDistribution(Distribution):
    """
    Gaussian distribution with diagonal covariance matrix, for continuous actions.

    :param action_dim:  Dimension of the action space.
    """

    def __init__(self, action_dim: int):
        super().__init__()
        self.action_dim = action_dim
        self.mean_actions = None
        self.log_std = None

    def proba_distribution_net(
        self, latent_dim: int, log_std_init: float = 0.0
    ) -> Tuple[nn.Module, nn.Parameter]:
        """
        Create the layers and parameter that represent the distribution: one
        output will be the mean of the Gaussian, the other parameter will be the
        standard deviation (log std in fact to allow negative values)

        :param latent_dim: Dimension of the last layer of the policy (before the
            action layer)
        :param log_std_init: Initial value for the log standard deviation
        :return:
        """
        mean_actions = nn.Linear(latent_dim, self.action_dim)
        # TODO: allow action dependent std
        log_std = nn.Parameter(
            th.ones(self.action_dim) * log_std_init, requires_grad=True
        )
        return mean_actions, log_std

    def make_distribution(
        self, mean_actions: th.Tensor, log_std: th.Tensor
    ) -> "DiagGaussianDistribution":
        """
        Create the distribution given its parameters (mean, std)

        :param mean_actions:
        :param log_std:
        :return:
        """
        action_std = th.ones_like(mean_actions) * log_std.exp()
        self.distribution = Normal(mean_actions, action_std)
        return self

    def log_prob(self, actions: th.Tensor) -> th.Tensor:
        """
        Get the log probabilities of actions according to the distribution.
        Note that you must first call the ``proba_distribution()`` method.

        :param actions:
        :return:
        """
        log_prob = self.distribution.log_prob(actions)
        return sum_independent_dims(log_prob)

    def entropy(self) -> th.Tensor:
        return sum_independent_dims(self.distribution.entropy())

    def sample(self) -> th.Tensor:
        # Reparametrization trick to pass gradients
        return self.distribution.rsample()

    def mode(self) -> th.Tensor:
        return self.distribution.mean

    def actions_from_params(
        self, mean_actions: th.Tensor, log_std: th.Tensor, deterministic: bool = False
    ) -> th.Tensor:
        # Update the proba distribution
        self.proba_distribution(mean_actions, log_std)
        return self.get_actions(deterministic=deterministic)

    def log_prob_from_params(
        self, mean_actions: th.Tensor, log_std: th.Tensor
    ) -> Tuple[th.Tensor, th.Tensor]:
        """
        Compute the log probability of taking an action
        given the distribution parameters.

        :param mean_actions:
        :param log_std:
        :return:
        """
        actions = self.actions_from_params(mean_actions, log_std)
        log_prob = self.log_prob(actions)
        return actions, log_prob


class SquashedDiagGaussianDistribution(DiagGaussianDistribution):
    """
    Gaussian distribution with diagonal covariance matrix, followed by a
    squashing function (tanh) to ensure bounds.

    :param action_dim: Dimension of the action space.
    :param epsilon: small value to avoid NaN due to numerical imprecision.
    """

    def __init__(self, action_dim: int, epsilon: float = 1e-6):
        super().__init__(action_dim)
        # Avoid NaN (prevents division by zero or log of zero)
        self.epsilon = epsilon
        self.gaussian_actions = None

    def proba_distribution(
        self, mean_actions: th.Tensor, log_std: th.Tensor
    ) -> "SquashedDiagGaussianDistribution":
        super().proba_distribution(mean_actions, log_std)
        return self

    def log_prob(
        self, actions: th.Tensor, gaussian_actions: Optional[th.Tensor] = None
    ) -> th.Tensor:
        # Inverse tanh
        # Naive implementation (not stable): 0.5 * torch.log((1 + x) / (1 - x))
        # We use numpy to avoid numerical instability
        if gaussian_actions is None:
            # It will be clipped to avoid NaN when inversing tanh
            gaussian_actions = TanhBijector.inverse(actions)

        # Log likelihood for a Gaussian distribution
        log_prob = super().log_prob(gaussian_actions)
        # Squash correction (from original SAC implementation)
        # this comes from the fact that tanh is bijective and differentiable
        log_prob -= th.sum(th.log(1 - actions**2 + self.epsilon), dim=1)
        return log_prob

    def entropy(self) -> Optional[th.Tensor]:
        # No analytical form,
        # entropy needs to be estimated using -log_prob.mean()
        raise Exception("Call to entropy in squashed Diag Gaussian distribution")
        return None

    def sample(self) -> th.Tensor:
        # Reparametrization trick to pass gradients
        self.gaussian_actions = super().sample()
        return th.tanh(self.gaussian_actions)

    def mode(self) -> th.Tensor:
        self.gaussian_actions = super().mode()
        # Squash the output
        return th.tanh(self.gaussian_actions)

    def log_prob_from_params(
        self, mean_actions: th.Tensor, log_std: th.Tensor
    ) -> Tuple[th.Tensor, th.Tensor]:
        action = self.actions_from_params(mean_actions, log_std)
        log_prob = self.log_prob(action, self.gaussian_actions)
        return action, log_prob

    def get_ten_samples(self) -> List:
        action_list = []
        for i in range(10):
            action = self.sample()
            action_list.append(action)
        return action_list


class CategoricalDistribution(Distribution):
    """
    Categorical distribution for discrete actions.

    :param action_dim: Number of discrete actions
    """

    def __init__(self, action_dim: int):
        super().__init__()
        self.action_dim = action_dim

    def proba_distribution_net(self, latent_dim: int) -> nn.Module:
        """
        Create the layer that represents the distribution:
        it will be the logits of the Categorical distribution.
        You can then get probabilities using a softmax.

        :param latent_dim: Dimension of the last layer
            of the policy network (before the action layer)
        :return:
        """
        action_logits = nn.Linear(latent_dim, self.action_dim)
        return action_logits

    def proba_distribution(self, action_logits: th.Tensor) -> "CategoricalDistribution":
        self.distribution = Categorical(logits=action_logits)
        return self

    def log_prob(self, actions: th.Tensor) -> th.Tensor:
        return self.distribution.log_prob(actions)

    def entropy(self) -> th.Tensor:
        return self.distribution.entropy()

    def sample(self) -> th.Tensor:
        return self.distribution.sample()

    def mode(self) -> th.Tensor:
        return th.argmax(self.distribution.probs, dim=1)

    def actions_from_params(
        self, action_logits: th.Tensor, deterministic: bool = False
    ) -> th.Tensor:
        # Update the proba distribution
        self.proba_distribution(action_logits)
        return self.get_actions(deterministic=deterministic)

    def log_prob_from_params(
        self, action_logits: th.Tensor
    ) -> Tuple[th.Tensor, th.Tensor]:
        actions = self.actions_from_params(action_logits)
        log_prob = self.log_prob(actions)
        return actions, log_prob


class MultiCategoricalDistribution(Distribution):
    """
    MultiCategorical distribution for multi discrete actions.

    :param action_dims: List of sizes of discrete action spaces
    """

    def __init__(self, action_dims: List[int]):
        super().__init__()
        self.action_dims = action_dims

    def proba_distribution_net(self, latent_dim: int) -> nn.Module:
        """
        Create the layer that represents the distribution: it will be the logits
        (flattened) of the MultiCategorical distribution. You can then get
        probabilities using a softmax on each sub-space.

        :param latent_dim: Dimension of the last layer of the policy network
            (before the action layer)
        :return:
        """

        action_logits = nn.Linear(latent_dim, sum(self.action_dims))
        return action_logits

    def proba_distribution(
        self, action_logits: th.Tensor
    ) -> "MultiCategoricalDistribution":
        self.distribution = [
            Categorical(logits=split)
            for split in th.split(action_logits, tuple(self.action_dims), dim=1)
        ]
        return self

    def log_prob(self, actions: th.Tensor) -> th.Tensor:
        # Extract each discrete action and compute log prob for their respective
        # distributions
        return th.stack(
            [
                dist.log_prob(action)
                for dist, action in zip(self.distribution, th.unbind(actions, dim=1))
            ],
            dim=1,
        ).sum(dim=1)

    def entropy(self) -> th.Tensor:
        return th.stack([dist.entropy() for dist in self.distribution], dim=1).sum(
            dim=1
        )

    def sample(self) -> th.Tensor:
        return th.stack([dist.sample() for dist in self.distribution], dim=1)

    def mode(self) -> th.Tensor:
        return th.stack(
            [th.argmax(dist.probs, dim=1) for dist in self.distribution], dim=1
        )

    def actions_from_params(
        self, action_logits: th.Tensor, deterministic: bool = False
    ) -> th.Tensor:
        # Update the proba distribution
        self.proba_distribution(action_logits)
        return self.get_actions(deterministic=deterministic)

    def log_prob_from_params(
        self, action_logits: th.Tensor
    ) -> Tuple[th.Tensor, th.Tensor]:
        actions = self.actions_from_params(action_logits)
        log_prob = self.log_prob(actions)
        return actions, log_prob


class BernoulliDistribution(Distribution):
    """
    Bernoulli distribution for MultiBinary action spaces.

    :param action_dim: Number of binary actions
    """

    def __init__(self, action_dims: int):
        super().__init__()
        self.action_dims = action_dims

    def proba_distribution_net(self, latent_dim: int) -> nn.Module:
        """
        Create the layer that represents the distribution:
        it will be the logits of the Bernoulli distribution.

        :param latent_dim: Dimension of the last layer
            of the policy network (before the action layer)
        :return:
        """
        action_logits = nn.Linear(latent_dim, self.action_dims)
        return action_logits

    def proba_distribution(self, action_logits: th.Tensor) -> "BernoulliDistribution":
        self.distribution = Bernoulli(logits=action_logits)
        return self

    def log_prob(self, actions: th.Tensor) -> th.Tensor:
        return self.distribution.log_prob(actions).sum(dim=1)

    def entropy(self) -> th.Tensor:
        return self.distribution.entropy().sum(dim=1)

    def sample(self) -> th.Tensor:
        return self.distribution.sample()

    def mode(self) -> th.Tensor:
        return th.round(self.distribution.probs)

    def actions_from_params(
        self, action_logits: th.Tensor, deterministic: bool = False
    ) -> th.Tensor:
        # Update the proba distribution
        self.proba_distribution(action_logits)
        return self.get_actions(deterministic=deterministic)

    def log_prob_from_params(
        self, action_logits: th.Tensor
    ) -> Tuple[th.Tensor, th.Tensor]:
        actions = self.actions_from_params(action_logits)
        log_prob = self.log_prob(actions)
        return actions, log_prob


class StateDependentNoiseDistribution(Distribution):
    """
    Distribution class for using generalized State Dependent Exploration (gSDE).
    Paper: https://arxiv.org/abs/2005.05719

    It is used to create the noise exploration matrix and compute the log
    probability of an action with that noise.

    :param action_dim: Dimension of the action space.
    :param full_std: Whether to use (n_features x n_actions) parameters for the
        std instead of only (n_features,)
    :param use_expln: Use ``expln()`` function instead of ``exp()`` to ensure a
        positive standard deviation (cf paper). It allows to keep variance above
        zero and prevent it from growing too fast. In practice, ``exp()`` is
        usually enough.
    :param squash_output: Whether to squash the output using a tanh function,
        this ensures bounds are satisfied.
    :param learn_features: Whether to learn features for gSDE or not. This will
        enable gradients to be backpropagated through the features
        ``latent_sde`` in the code.
    :param epsilon: small value to avoid NaN due to numerical imprecision.
    """

    def __init__(
        self,
        action_dim: int,
        full_std: bool = True,
        use_expln: bool = False,
        squash_output: bool = False,
        learn_features: bool = False,
        epsilon: float = 1e-6,
    ):
        super().__init__()
        self.action_dim = action_dim
        self.latent_sde_dim = None
        self.mean_actions = None
        self.log_std = None
        self.weights_dist = None
        self.exploration_mat = None
        self.exploration_matrices = None
        self._latent_sde = None
        self.use_expln = use_expln
        self.full_std = full_std
        self.epsilon = epsilon
        self.learn_features = learn_features
        if squash_output:
            self.bijector = TanhBijector(epsilon)
        else:
            self.bijector = None

    def get_std(self, log_std: th.Tensor) -> th.Tensor:
        """
        Get the standard deviation from the learned parameter
        (log of it by default). This ensures that the std is positive.

        :param log_std:
        :return:
        """
        if self.use_expln:
            # From gSDE paper, it allows to keep variance
            # above zero and prevent it from growing too fast
            below_threshold = th.exp(log_std) * (log_std <= 0)
            # Avoid NaN: zeros values that are below zero
            safe_log_std = log_std * (log_std > 0) + self.epsilon
            above_threshold = (th.log1p(safe_log_std) + 1.0) * (log_std > 0)
            std = below_threshold + above_threshold
        else:
            # Use normal exponential
            std = th.exp(log_std)

        if self.full_std:
            return std
        # Reduce the number of parameters:
        return th.ones(self.latent_sde_dim, self.action_dim).to(log_std.device) * std

    def sample_weights(self, log_std: th.Tensor, batch_size: int = 1) -> None:
        """
        Sample weights for the noise exploration matrix,
        using a centered Gaussian distribution.

        :param log_std:
        :param batch_size:
        """
        std = self.get_std(log_std)
        self.weights_dist = Normal(th.zeros_like(std), std)
        # Reparametrization trick to pass gradients
        self.exploration_mat = self.weights_dist.rsample()
        # Pre-compute matrices in case of parallel exploration
        self.exploration_matrices = self.weights_dist.rsample((batch_size,))

    def proba_distribution_net(
        self,
        latent_dim: int,
        log_std_init: float = -2.0,
        latent_sde_dim: Optional[int] = None,
    ) -> Tuple[nn.Module, nn.Parameter]:
        """
        Create the layers and parameter that represent the distribution: one
        output will be the deterministic action, the other parameter will be the
        standard deviation of the distribution that control the weights of the
        noise matrix.

        :param latent_dim: Dimension of the last layer of the policy (before the
            action layer)
        :param log_std_init: Initial value for the log standard deviation
        :param latent_sde_dim: Dimension of the last layer of the features
            extractor for gSDE. By default, it is shared with the policy
            network.
        :return:
        """
        # Network for the deterministic action, it represents the mean of the
        # distribution
        mean_actions_net = nn.Linear(latent_dim, self.action_dim)
        # When we learn features for the noise, the feature dimension
        # can be different between the policy and the noise network
        self.latent_sde_dim = latent_dim if latent_sde_dim is None else latent_sde_dim
        # Reduce the number of parameters if needed
        log_std = (
            th.ones(self.latent_sde_dim, self.action_dim)
            if self.full_std
            else th.ones(self.latent_sde_dim, 1)
        )
        # Transform it to a parameter so it can be optimized
        log_std = nn.Parameter(log_std * log_std_init, requires_grad=True)
        # Sample an exploration matrix
        self.sample_weights(log_std)
        return mean_actions_net, log_std

    def proba_distribution(
        self, mean_actions: th.Tensor, log_std: th.Tensor, latent_sde: th.Tensor
    ) -> "StateDependentNoiseDistribution":
        """
        Create the distribution given its parameters (mean, std)

        :param mean_actions:
        :param log_std:
        :param latent_sde:
        :return:
        """
        # Stop gradient if we don't want to influence the features
        self._latent_sde = latent_sde if self.learn_features else latent_sde.detach()
        variance = th.mm(self._latent_sde**2, self.get_std(log_std) ** 2)
        self.distribution = Normal(mean_actions, th.sqrt(variance + self.epsilon))
        return self

    def log_prob(self, actions: th.Tensor) -> th.Tensor:
        if self.bijector is not None:
            gaussian_actions = self.bijector.inverse(actions)
        else:
            gaussian_actions = actions
        # log likelihood for a gaussian
        log_prob = self.distribution.log_prob(gaussian_actions)
        # Sum along action dim
        log_prob = sum_independent_dims(log_prob)

        if self.bijector is not None:
            # Squash correction (from original SAC implementation)
            log_prob -= th.sum(
                self.bijector.log_prob_correction(gaussian_actions), dim=1
            )
        return log_prob

    def entropy(self) -> Optional[th.Tensor]:
        if self.bijector is not None:
            # No analytical form,
            # entropy needs to be estimated using -log_prob.mean()
            return None
        return sum_independent_dims(self.distribution.entropy())

    def sample(self) -> th.Tensor:
        noise = self.get_noise(self._latent_sde)
        actions = self.distribution.mean + noise
        if self.bijector is not None:
            return self.bijector.forward(actions)
        return actions

    def mode(self) -> th.Tensor:
        actions = self.distribution.mean
        if self.bijector is not None:
            return self.bijector.forward(actions)
        return actions

    def get_noise(self, latent_sde: th.Tensor) -> th.Tensor:
        latent_sde = latent_sde if self.learn_features else latent_sde.detach()
        # Default case: only one exploration matrix
        if len(latent_sde) == 1 or len(latent_sde) != len(self.exploration_matrices):
            return th.mm(latent_sde, self.exploration_mat)
        # Use batch matrix multiplication for efficient computation
        # (batch_size, n_features) -> (batch_size, 1, n_features)
        latent_sde = latent_sde.unsqueeze(1)
        # (batch_size, 1, n_actions)
        noise = th.bmm(latent_sde, self.exploration_matrices)
        return noise.squeeze(1)

    def actions_from_params(
        self,
        mean_actions: th.Tensor,
        log_std: th.Tensor,
        latent_sde: th.Tensor,
        deterministic: bool = False,
    ) -> th.Tensor:
        # Update the proba distribution
        self.proba_distribution(mean_actions, log_std, latent_sde)
        return self.get_actions(deterministic=deterministic)

    def log_prob_from_params(
        self, mean_actions: th.Tensor, log_std: th.Tensor, latent_sde: th.Tensor
    ) -> Tuple[th.Tensor, th.Tensor]:
        actions = self.actions_from_params(mean_actions, log_std, latent_sde)
        log_prob = self.log_prob(actions)
        return actions, log_prob


class TanhBijector:
    """
    Bijective transformation of a probability distribution
    using a squashing function (tanh)
    TODO: use Pyro instead (https://pyro.ai/)

    :param epsilon: small value to avoid NaN due to numerical imprecision.
    """

    def __init__(self, epsilon: float = 1e-6):
        super().__init__()
        self.epsilon = epsilon

    @staticmethod
    def forward(x: th.Tensor) -> th.Tensor:
        return th.tanh(x)

    @staticmethod
    def atanh(x: th.Tensor) -> th.Tensor:
        """
        Inverse of Tanh

        Taken from Pyro: https://github.com/pyro-ppl/pyro
        0.5 * torch.log((1 + x ) / (1 - x))
        """
        return 0.5 * (x.log1p() - (-x).log1p())

    @staticmethod
    def inverse(y: th.Tensor) -> th.Tensor:
        """
        Inverse tanh.

        :param y:
        :return:
        """
        eps = th.finfo(y.dtype).eps
        # Clip the action to avoid NaN
        return TanhBijector.atanh(y.clamp(min=-1.0 + eps, max=1.0 - eps))

    def log_prob_correction(self, x: th.Tensor) -> th.Tensor:
        # Squash correction (from original SAC implementation)
        return th.log(1.0 - th.tanh(x) ** 2 + self.epsilon)


def make_proba_distribution(
    action_space: gym.spaces.Space,
    use_sde: bool = False,
    dist_kwargs: Optional[Dict[str, Any]] = None,
) -> Distribution:
    """
    Return an instance of Distribution for the correct type of action space

    :param action_space: the input action space
    :param use_sde: Force the use of StateDependentNoiseDistribution
        instead of DiagGaussianDistribution
    :param dist_kwargs: Keyword arguments to pass to the probability distribution
    :return: the appropriate Distribution object
    """
    if dist_kwargs is None:
        dist_kwargs = {}

    if isinstance(action_space, spaces.Box):
        assert len(action_space.shape) == 1, "Error: the action space must be a vector"
        cls = StateDependentNoiseDistribution if use_sde else DiagGaussianDistribution
        return cls(get_action_dim(action_space), **dist_kwargs)
    elif isinstance(action_space, spaces.Discrete):
        return CategoricalDistribution(action_space.n, **dist_kwargs)
    elif isinstance(action_space, spaces.MultiDiscrete):
        return MultiCategoricalDistribution(action_space.nvec, **dist_kwargs)
    elif isinstance(action_space, spaces.MultiBinary):
        return BernoulliDistribution(action_space.n, **dist_kwargs)
    else:
        raise NotImplementedError(
            "Error: probability distribution, not implemented for action space"
            f"of type {type(action_space)}."
            " Must be of type Gym Spaces: Box, Discrete, MultiDiscrete or MultiBinary."
        )


def kl_divergence(dist_true: Distribution, dist_pred: Distribution) -> th.Tensor:
    """
    Wrapper for the PyTorch implementation of the full form KL Divergence

    :param dist_true: the p distribution
    :param dist_pred: the q distribution
    :return: KL(dist_true||dist_pred)
    """
    # KL Divergence for different distribution types is out of scope
    assert (
        dist_true.__class__ == dist_pred.__class__
    ), "Error: input distributions should be the same type"

    # MultiCategoricalDistribution is not a PyTorch Distribution subclass
    # so we need to implement it ourselves!
    if isinstance(dist_pred, MultiCategoricalDistribution):
        assert (
            dist_pred.action_dims == dist_true.action_dims
        ), "Error: distributions must have the same input space"
        return th.stack(
            [
                th.distributions.kl_divergence(p, q)
                for p, q in zip(dist_true.distribution, dist_pred.distribution)
            ],
            dim=1,
        ).sum(dim=1)

    # Use the PyTorch kl_divergence implementation
    else:
        return th.distributions.kl_divergence(
            dist_true.distribution, dist_pred.distribution
        )

----!@#$----
utils\gymnasium\__init__.py

----!@#$----
visu\common.py
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
#
import os

import matplotlib.pyplot as plt


def final_show(save_figure, plot, directory, figure_name, x_label, y_label, title):
    """
    Finalize all plots, adding labels and putting the corresponding file in the
    specified directory
    :param save_figure: boolean stating whether the figure should be saved
    :param plot: whether the plot should be shown interactively
    :param figure_name: the name of the file where to save the figure
    :param x_label: label on the x axis
    :param y_label: label on the y axis
    :param title: title of the figure
    :param directory: the path where to save the picture
    :return: nothing
    """
    plt.xlabel(x_label)
    plt.ylabel(y_label)
    plt.title(title)

    if save_figure:
        if not os.path.exists(directory):
            os.makedirs(directory)
        filename = directory + figure_name
        plt.savefig(filename)

    if plot:
        plt.show()

    plt.close()

----!@#$----
visu\play.py
from pathlib import Path
import re
import gym
import torch


def load_agent(path: Path, prefix: str):
    """Loads an agent in the given folder with the highest reward

    Agent files should have the pattern '{prefix}_REWARD.agt'
    """
    argmax_r, max_r = None, float("-inf")
    for p in path.glob(f"{prefix}*.agt"):
        m = re.match(rf".*/{prefix}(-?\d+\.\d+)\.agt", str(p))
        r = float(m.group(1))
        if r > max_r:
            max_r = r
            argmax_r = p

    if argmax_r:
        print(f"Loading {argmax_r}")
        return torch.load(argmax_r)
    return None


def play(env: gym.Env, agent: torch.nn.Module):
    """Render the agent"""
    if agent is None:
        print("No agent")
        return

    sum_reward = 0.0

    try:
        print(agent)
        with torch.no_grad():
            obs = env.reset()
            env.render()
            done = False
            while not done:
                obs = torch.Tensor(obs)
                action = agent.predict_action(obs, False)
                obs, reward, done, info = env.step(action.numpy())
                sum_reward += reward
                env.render()
    finally:
        env.close()

    return reward

----!@#$----
visu\plot_critics.py
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
#

import warnings
from typing import Union

import matplotlib.pyplot as plt
import numpy as np
import random
import torch
from gymnasium import Space
from gymnasium.core import ActType

from bbrl import Agent
from bbrl.agents.gymnasium import GymAgent
from bbrl.visu.common import final_show
from bbrl.workspace import Workspace

# The plot critic actions below could probably be factored or at least reuse common subparts


# plot a DPDG-like critic.
# If the input_action is None, which cannot be the case with DDPG-like critic, a random action is drawn, which makes little sense.

def plot_critic(
    agent: Agent,
    env: GymAgent,
    best_reward,
    directory: str,
    env_name: Union[str, None] = None,
    plot: bool = False,
    save_fig: bool = True,
    definition: int = 100,
    input_action=None,
    var_name_obs: str = "env/env_obs",
    var_name_action: str = "action",
    var_name_value: str = "None",
    **kwargs,
) -> None:
    """
    Plot the critic of an agent
    :param Agent agent: the agent
    :param GymAgent env: the environment
    :param Tensor best_reward: the best reward
    :param str directory: the directory to save the figure
    :param str env_name: the name of the environment
    :param bool plot: if True, plot the figure
    :param bool save_fig: if True, save the figure
    :param int definition: the definition of the plot
    :param ActType action: the action to use if the agent is a q function
    :param str var_name_obs: the name of the observation variable
    :param str var_name_action: the name of the action variable
    :param kwargs: the arguments to be passed to the agent forward function
    :return: None
    """

    if env_name is None:
        env_name = env.envs[0].unwrapped.spec.id

    figure_name: str = f"critic_{env_name}_{best_reward}.png"

    if not agent.is_q_function and input_action is not None:
        warnings.warn("action is ignored for non q function agent")
    if agent.is_q_function and input_action is None:
        action_space: Space[ActType] = env.get_action_space()
        input_action = action_space.sample()   
        warnings.warn("trying to plot a Q critic without giving an action")
        
    assert (
        len(env.observation_space.shape) == 1
    ), "Nested observation space not supported"

    if env.observation_space.shape[0] < 2:
        msg = f"Observation space dim {env.observation_space.shape[0]}, should be >= 2"
        raise (ValueError(msg))

    state_min = env.observation_space.low
    state_max = env.observation_space.high

    # TODO: it would be better to determine the min and max from the available data...
    for i in range(len(state_min)):
        if state_min[i] == -np.inf:
            state_min[i] = -1e20
        if state_max[i] == np.inf:
            state_max[i] = 1e20

    workspace: Workspace = Workspace()

    all_obs = []
    for index_x, x in enumerate(np.linspace(state_min[0], state_max[0], definition)):
        for index_y, y in enumerate(
            np.linspace(state_min[1], state_max[1], definition)
        ):
            # create possible states to observe
            obs = [x, y]
            for i in range(2, env.observation_space.shape[0]):
                # generate randomness around mean
                z = random.random() - 0.5
                # z = np.random.uniform(state_min[i], state_max[i])
                obs = np.append(obs, z)
            all_obs.append(obs)
    all_obs = torch.tensor([all_obs], dtype=torch.float32)

    workspace.set_full(var_name_obs, all_obs, batch_dims=None)

    if agent.is_q_function:
        action = torch.tensor([[input_action for _ in range(definition**2)]])
        workspace.set_full(var_name_action, action, batch_dims=None)
        if var_name_value == "None":
            var_name_value: str = f"{agent.name}/q_values"
    else:
        if var_name_value == "None":
            var_name_value: str = f"{agent.name}/v_values"

    agent(workspace, t=0, **kwargs)
    data = workspace.get_full(var_name_value)
    portrait = (
        data
        .reshape(definition, definition)
        .detach()
        .numpy()
    )
    plt.figure(figsize=(10, 10))
    
    plt.imshow(
        portrait,
        cmap="inferno",
        extent=[
            state_min[0],
            state_max[0],
            state_min[1],
            state_max[1],
        ],
        aspect="auto",
    )

    directory += "/" + env_name + "_critics/"
    title = env_name + " critic"
    plt.colorbar(label="critic value")

    # Add a point at the center
    plt.scatter([0], [0])
    x_label, y_label = getattr(env.observation_space, "names", ["x", "y"])
    final_show(save_fig, plot, directory, figure_name, x_label, y_label, title)

# Plot a DQN-like critic.
# If input_action is "policy", it plots the actions with a different color for each action
# If input_action is "None", it plots the value of the best action
# Otherwise, input_action is a number and it plots the Q-value of the corresponding action
    
def plot_discrete_q(
    agent: Agent,
    env: GymAgent,
    best_reward,
    directory: str,
    env_name: Union[str, None] = None,
    plot: bool = False,
    save_fig: bool = True,
    definition: int = 100,
    input_action=None,
    var_name_obs: str = "env/env_obs",
    var_name_action: str = "action",
    var_name_value: str = "None",
    **kwargs,
) -> None:
    """
    Plot the critic of an agent
    :param Agent agent: the agent
    :param GymAgent env: the environment
    :param Tensor best_reward: the best reward
    :param str directory: the directory to save the figure
    :param str env_name: the name of the environment
    :param bool plot: if True, plot the figure
    :param bool save_fig: if True, save the figure
    :param int definition: the definition of the plot
    :param ActType input_action: the action to use if the agent is a q function. Use "policy" if you want to see the critic as a policy (display the chosen action)
    :param str var_name_obs: the name of the observation variable
    :param str var_name_action: the name of the action variable
    :param kwargs: the arguments to be passed to the agent forward function
    :return: None
    """

    if env_name is None:
        env_name = env.envs[0].unwrapped.spec.id

    figure_name: str = f"critic_{env_name}_{best_reward}.png"

    if not agent.is_q_function and input_action is not None:
        warnings.warn("action is ignored for non q function agent")

    assert (
        len(env.observation_space.shape) == 1
    ), "Nested observation space not supported"

    if env.observation_space.shape[0] < 2:
        msg = f"Observation space dim {env.observation_space.shape[0]}, should be >= 2"
        raise (ValueError(msg))

    state_min = env.observation_space.low
    state_max = env.observation_space.high

    # TODO: it would be better to determine the min and max from the available data...
    for i in range(len(state_min)):
        if state_min[i] == -np.inf:
            state_min[i] = -1e20
        if state_max[i] == np.inf:
            state_max[i] = 1e20

    workspace: Workspace = Workspace()

    all_obs = []
    for index_x, x in enumerate(np.linspace(state_min[0], state_max[0], definition)):
        for index_y, y in enumerate(
            np.linspace(state_min[1], state_max[1], definition)
        ):
            # create possible states to observe
            obs = [x, y]
            for i in range(2, env.observation_space.shape[0]):
                # generate randomness around mean
                z = random.random() - 0.5
                # z = np.random.uniform(state_min[i], state_max[i])
                obs = np.append(obs, z)
            all_obs.append(obs)
    all_obs = torch.tensor([all_obs], dtype=torch.float32)

    workspace.set_full(var_name_obs, all_obs, batch_dims=None)

    assert agent.is_q_function, "plot_discrete_q should only be called for Q functions"
    
    if var_name_value == "None":
        var_name_value: str = f"{agent.name}/q_values"

    agent(workspace, t=0, **kwargs)
    data = workspace.get_full(var_name_value)

    if input_action is None:
        q_values = data.max(dim=-1).values
    elif input_action == "policy":
        q_values = data.max(dim=-1).indices
    else:
        q_values = data[:, :, input_action]
        
    portrait = (
        q_values.reshape(definition, definition)
        .detach()
        .numpy()
    )

    plt.figure(figsize=(10, 10))
    plt.imshow(
        portrait,
        cmap="inferno",
        extent=[
            state_min[0],
            state_max[0],
            state_min[1],
            state_max[1],
        ],
        aspect="auto",
    )

    directory += "/" + env_name + "_critics/"
    title = env_name + " critic"
    plt.colorbar(label="critic value")

    # Add a point at the center
    plt.scatter([0], [0])
    x_label, y_label = getattr(env.observation_space, "names", ["x", "y"])
    final_show(save_fig, plot, directory, figure_name, x_label, y_label, title)

----!@#$----
visu\plot_policies.py
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
#
import random
from typing import Union

import matplotlib.pyplot as plt
import numpy as np
import random
import torch
from gymnasium.spaces import flatdim

from bbrl import Agent
from bbrl.agents.gymnasium import GymAgent
from bbrl.visu.common import final_show
from bbrl.workspace import Workspace


def plot_policy(
    actor: Agent,
    env: GymAgent,
    best_reward,
    directory: str,
    env_name: Union[str, None] = None,
    plot: bool = False,
    save_fig: bool = True,
    definition: int = 100,
    var_name_obs: str = "env/env_obs",
    var_name_action: str = "action",
    **kwargs,
) -> None:
    """
    Plot the policy of the agent
    :param Agent actor: the agent
    :param GymAgent env: the environment
    :param Tensor best_reward: the best reward
    :param str directory: the directory to save the figure
    :param str env_name: the name of the environment
    :param bool plot: if True, plot the figure
    :param bool save_fig: if True, save the figure
    :param int definition: the definition of the plot
    :param str var_name_obs: the name of the observation variable red by the agent
    :param str var_name_action: the name of the action variable written by the agent
    :param kwargs: the arguments to be passed to the actor forward function
    :return: None
    """
    if env_name is None:
        env_name = env.envs[0].unwrapped.spec.id

    assert (
        len(env.observation_space.shape) == 1
    ), "Nested observation space not supported"

    if env.observation_space.shape[0] < 2:
        msg = f"Observation space dim {env.observation_space.shape[0]}, should be >= 2"
        raise (ValueError(msg))

    state_min = env.observation_space.low
    state_max = env.observation_space.high

    for i in range(len(state_min)):
        if state_min[i] == -np.inf:
            state_min[i] = -1e20
        if state_max[i] == np.inf:
            state_max[i] = 1e20

    workspace = Workspace()

    if env.is_continuous_action():
        action_dim = flatdim(env.action_space)
    else:
        action_dim = 1

    all_obs = []
    for index_x, x in enumerate(np.linspace(state_min[0], state_max[0], definition)):
        for index_y, y in enumerate(
            np.linspace(state_min[1], state_max[1], definition)
        ):
            # create possible states to observe
            obs = [x, y]
            for i in range(2, env.observation_space.shape[0]):
                # generate randomness around mean
                z = random.random() - 0.5
                # z = random.uniform(state_min[i], state_max[i])
                obs.append(z)
            all_obs.append(obs)
    all_obs = torch.tensor([all_obs], dtype=torch.float32)

    workspace.set_full(var_name_obs, all_obs, batch_dims=None)

    # predictions ici de l'action selon la policy
    actor(workspace, t=0, **kwargs)

    # get the actions from the workspace
    portrait = (
        workspace.get_full(var_name_action)
        .reshape(definition, definition, action_dim)
        .detach()
        .numpy()
    )

    for dim in range(action_dim):
        portrait = portrait[:, :, dim]

        plt.figure(figsize=(10, 10))
        plt.imshow(
            portrait,
            cmap="inferno",
            extent=[state_min[0], state_max[0], state_min[1], state_max[1]],
            aspect="auto",
        )

        figure_name: str = f"policy_{env_name}_dim_{dim}_{best_reward}.png"

        title = "{} Actor (action dim: {})".format(env_name, dim)
        plt.colorbar(label="action")
        directory += "/" + env_name + "_policies/"
        # Add a point at the center
        plt.scatter([0], [0])
        x_label, y_label = getattr(env.observation_space, "names", ["x", "y"])
        final_show(save_fig, plot, directory, figure_name, x_label, y_label, title)

----!@#$----
visu\plot_trajectories.py
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
#
import numpy as np
import matplotlib.pyplot as plt
from bbrl.visu.common import final_show
from bbrl.workspace import Workspace

from typing import Union


def plot_trajectory(
        data,
        fig_index = 1,
        x_plot: Union[int, None] = 0,
        y_plot: Union[int, None] = 1,
        x_label: str = 'x',
        y_label: str = 'y',
        obs_key: str = 'env/obs',
        title: str = 'Trajectory',
        save_figure=True,
        plot=True
) -> None:

    if isinstance(data, Workspace):
        o = data.get_full(obs_key).numpy()
    elif isinstance(data, np.ndarray):
        o = data
    else:
        raise ValueError("data should be a Workspace or a numpy array")
    o = np.swapaxes(o, 1, 0)
    x = o[:, :, x_plot]
    y = o[:, :, y_plot]

    colors = []
    for c in range(o.shape[0]):
        colors += [c] * o.shape[1]

    plt.scatter(x, y, c=colors, cmap='viridis', alpha=0.7, s=1)
    figname = "trajectory_" + str(fig_index) + ".pdf"
    final_show(save_figure, plot, figname, x_label, y_label, title, "/plots/")

----!@#$----
visu\svpg_histograms.py
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
#

import matplotlib.pyplot as plt
import numpy as np

from bbrl.visu.common import final_show


def plot_histograms(
    rewards_list, labels, colors, title, directory, plot=True, save_figure=True
):
    n_bars = len(rewards_list)
    x = np.arange(len(rewards_list[0]))
    width = 0.5 / n_bars

    for i, rewards in enumerate(rewards_list):
        plt.bar(x + width * i, np.sort(rewards)[::-1], width=width, color=colors[i])

    plt.legend(labels=labels)
    plt.xticks([], [])

    figname = f"{title}-indep_vs_svpg.png"
    final_show(save_figure, plot, figname, "", "rewards", title, directory)

----!@#$----
visu\__init__.py

--END--