{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "addb807a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Outlook\n",
    "\n",
    "In this notebook, using BBRL, we code a version of the DQN algorithm with a\n",
    "replay buffer and a target network, using the AutoReset approach.\n",
    "\n",
    "To understand this code, you need to know more about \n",
    "[the BBRL interaction model](https://colab.research.google.com/drive/1_yp-JKkxh_P8Yhctulqm0IrLbE41oK1p?usp=sharing).\n",
    "Then you should run [a first example](https://colab.research.google.com/drive/1Ui481r47fNHCQsQfKwdoNEVrEiqAEokh?usp=sharing)\n",
    "to see how agents interact.\n",
    "\n",
    "You also need to understand [details about\n",
    "autoreset=True](https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5?usp=sharing).\n",
    "\n",
    "The DQN algorithm is explained in [this\n",
    "video](https://www.youtube.com/watch?v=CXwvOMJujZk) and you can also read [the\n",
    "corresponding slides](http://pages.isir.upmc.fr/~sigaud/teach/dqn.pdf).\n",
    "\n",
    "In this notebook, you will learn how to modify the previous notebook:\n",
    "\n",
    "- to use a replay buffer and an environment that resets\n",
    "- to use a target network for $Q$\n",
    "- to use a better estimation for the maximum (Double-DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f32a9d",
   "metadata": {},
   "source": [
    "## Installation and Imports\n",
    "\n",
    "### Installation\n",
    "\n",
    "The BBRL library is [here](https://github.com/osigaud/bbrl).\n",
    "\n",
    "We use OmegaConf to that makes it possible that by just defining the `def\n",
    "run_dqn(cfg):` function and then executing a long `params = {...}` variable at\n",
    "the bottom of this colab, the code is run with the parameters without calling\n",
    "an explicit main.\n",
    "\n",
    "More precisely, the code is run by calling\n",
    "\n",
    "`config=OmegaConf.create(params)`\n",
    "\n",
    "`run_dqn(config)`\n",
    "\n",
    "at the very bottom of the colab, after starting tensorboard.\n",
    "\n",
    "Below, we import standard python packages, pytorch packages and gymnasium\n",
    "environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc26d74",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Installs the necessary Python and system libraries\n",
    "try:\n",
    "    from easypip import easyimport, easyinstall, is_notebook\n",
    "except ModuleNotFoundError as e:\n",
    "    get_ipython().run_line_magic(\"pip\", \"install easypip\")\n",
    "    from easypip import easyimport, easyinstall, is_notebook\n",
    "\n",
    "easyinstall(\"bbrl>=0.2.2\")\n",
    "easyinstall(\"swig\")\n",
    "easyinstall(\"bbrl_gymnasium>=0.2.0\")\n",
    "easyinstall(\"bbrl_gymnasium[box2d]\")\n",
    "easyinstall(\"bbrl_gymnasium[classic_control]\")\n",
    "easyinstall(\"tensorboard\")\n",
    "easyinstall(\"moviepy\")\n",
    "easyinstall(\"box2d-kengz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b1682e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "from moviepy.editor import ipython_display as video_display\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Tuple, Optional\n",
    "from functools import partial\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "import bbrl_gymnasium\n",
    "\n",
    "import copy\n",
    "from abc import abstractmethod, ABC\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from time import strftime\n",
    "OmegaConf.register_new_resolver(\n",
    "    \"current_time\", lambda: strftime(\"%Y%m%d-%H%M%S\"), replace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d85d92",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Imports all the necessary classes and functions from BBRL\n",
    "from bbrl.agents.agent import Agent\n",
    "from bbrl import get_arguments, get_class, instantiate_class\n",
    "# The workspace is the main class in BBRL, this is where all data is collected and stored\n",
    "from bbrl.workspace import Workspace\n",
    "\n",
    "# Agents(agent1,agent2,agent3,...) executes the different agents the one after the other\n",
    "# TemporalAgent(agent) executes an agent over multiple timesteps in the workspace, \n",
    "# or until a given condition is reached\n",
    "from bbrl.agents import Agents, TemporalAgent\n",
    "\n",
    "# ParallelGymAgent is an agent able to execute a batch of gymnasium environments\n",
    "# with auto-resetting. These agents produce multiple variables in the workspace:\n",
    "# ’env/env_obs’, ’env/reward’, ’env/timestep’, ’env/terminated’,\n",
    "# 'env/truncated', 'env/done', ’env/cumulated_reward’, ... \n",
    "# \n",
    "# When called at timestep t=0, the environments are automatically reset. At\n",
    "# timestep t>0, these agents will read the ’action’ variable in the workspace at\n",
    "# time t − 1\n",
    "from bbrl.agents.gymnasium import GymAgent, ParallelGymAgent, make_env, record_video\n",
    "\n",
    "# Replay buffers are useful to store past transitions when training\n",
    "from bbrl.utils.replay_buffer import ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551ee403",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "lines_to_next_cell": 1,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Utility function for launching tensorboard\n",
    "# For Colab - otherwise, it is easier and better to launch tensorboard from\n",
    "# the terminal\n",
    "def setup_tensorboard(path):\n",
    "    path = Path(path)\n",
    "    answer = \"\"\n",
    "    if is_notebook():\n",
    "        if get_ipython().__class__.__module__ == \"google.colab._shell\":\n",
    "            answer = \"y\"\n",
    "        while answer not in [\"y\", \"n\"]:\n",
    "                answer = input(f\"Do you want to launch tensorboard in this notebook [y/n] \").lower()\n",
    "\n",
    "    if answer == \"y\":\n",
    "        get_ipython().run_line_magic(\"load_ext\", \"tensorboard\")\n",
    "        get_ipython().run_line_magic(\"tensorboard\", f\"--logdir {path.absolute()}\")\n",
    "    else:\n",
    "        import sys\n",
    "        import os\n",
    "        import os.path as osp\n",
    "        print(f\"Launch tensorboard from the shell:\\n{osp.dirname(sys.executable)}/tensorboard --logdir={path.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371a3018",
   "metadata": {},
   "source": [
    "As before, we start with a Random Agent and 3 instances of the CartPole environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995523d9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# We deal with 3 a single environment (random seed 2139)\n",
    "\n",
    "env_agent = ParallelGymAgent(partial(make_env, env_name='CartPole-v1'), 1).seed(2139)\n",
    "obs_size, action_dim = env_agent.get_obs_and_actions_sizes()\n",
    "print(f\"Environment: observation space in R^{obs_size} and action space R^{action_dim}\")\n",
    "\n",
    "class RandomAgent(Agent):\n",
    "    def __init__(self, action_dim):\n",
    "        super().__init__()\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "    def forward(self, t: int, choose_action=True, **kwargs):\n",
    "        \"\"\"An Agent can use self.workspace\"\"\"\n",
    "        obs = self.get((\"env/env_obs\", t))\n",
    "        action = torch.randint(0, self.action_dim, (len(obs), ))\n",
    "        self.set((\"action\", t), action)\n",
    "\n",
    "# Each agent will be run (in the order given when constructing Agents)\n",
    "agents = Agents(env_agent, RandomAgent(action_dim))\n",
    "t_agents = TemporalAgent(agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebc474c",
   "metadata": {},
   "source": [
    "Let us have a closer look at the content of the workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760af875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a new workspace\n",
    "workspace = Workspace() \n",
    "t_agents(workspace, stop_variable=\"env/done\")\n",
    "\n",
    "# We get the transitions: each tensor is transformed so\n",
    "# that: \n",
    "# - we have the value at time step t and t+1 (so all the tensors first dimension have a size of 2)\n",
    "# - there is no distinction between the different environments (here, there is just one environment run in parallel to make it easy)\n",
    "transitions = workspace.get_transitions()\n",
    "\n",
    "# You can see that each pair of actions in the transitions can be found in the workspace\n",
    "display(\"Observations (first 3)\", workspace[\"env/env_obs\"][:3, 0])\n",
    "\n",
    "display(\"Transitions of actions (first 3)\")\n",
    "for t in range(3):\n",
    "    display(f'(s_{t}, s_{t+1})')\n",
    "    display(transitions[\"env/env_obs\"][:, t])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75d430c",
   "metadata": {},
   "source": [
    "Note that if we were using more than 1 environment (say N), we would have to watch in the transition every N lines, since transitions are stored one environment after the other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51be673e",
   "metadata": {},
   "source": [
    "## The replay buffer\n",
    "\n",
    "Differently from the previous case, we use a replace buffer that stores the a\n",
    "set of transitions $(s_t, a_t, r_t, s_{t+1})$\n",
    "Finally, the replay buffer keeps slices [:, i, ...] of the transition\n",
    "workspace (here at most 100 transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b94ad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "rb = ReplayBuffer(max_size=100)\n",
    "\n",
    "# We add the transitions to the buffer....\n",
    "rb.put(transitions)\n",
    "\n",
    "# And sample from them here we get 3 tuples (s_t, s_{t+1})\n",
    "rb.get_shuffled(3)[\"env/env_obs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eca761",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "A transition workspace is still a workspace... this is quite\n",
    " handy since each transition can be seen as a mini-episode of two time steps;\n",
    " we can use our agents on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34c70ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just as a reference\n",
    "\n",
    "display(transitions[\"action\"])\n",
    "\n",
    "t_random_agent = TemporalAgent(RandomAgent(action_dim))\n",
    "t_random_agent(transitions, t=0, n_steps=2)\n",
    "\n",
    "# Here, the action tensor will have been overwritten by the new actions\n",
    "display(transitions[\"action\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4124f7",
   "metadata": {},
   "source": [
    "## Definition of agents\n",
    "\n",
    "### The critic agent\n",
    "\n",
    "The [DQN](https://daiwk.github.io/assets/dqn.pdf) algorithm is a critic only\n",
    "algorithm. Thus we just need a Critic agent (which will also be used to output\n",
    "actions) and an Environment agent. We reuse the `DiscreteQAgent` class that we\n",
    "have already explained in the previous notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c72eb1",
   "metadata": {},
   "source": [
    "The function below builds a multi-layer perceptron where the size of each layer is given in the `size` list.\n",
    "We also specify the activation function of neurons at each layer and optionally a different activation function for the final layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eb625a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "def build_mlp(sizes, activation, output_activation=nn.Identity()):\n",
    "    \"\"\"Helper function to build a multi-layer perceptron (function from $\\mathbb R^n$ to $\\mathbb R^p$)\n",
    "    \n",
    "    Args:\n",
    "        sizes (List[int]): the number of neurons at each layer\n",
    "        activation (nn.Module): a PyTorch activation function (after each layer but the last)\n",
    "        output_activation (nn.Module): a PyTorch activation function (last layer)\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    for j in range(len(sizes) - 1):\n",
    "        act = activation if j < len(sizes) - 2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j + 1]), act]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2760d27",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class DiscreteQAgent(Agent):\n",
    "    def __init__(self, state_dim, hidden_layers, action_dim):\n",
    "        super().__init__()\n",
    "        self.model = build_mlp(\n",
    "            [state_dim] + list(hidden_layers) + [action_dim], activation=nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, t, choose_action=True, **kwargs):\n",
    "        obs = self.get((\"env/env_obs\", t))\n",
    "        q_values = self.model(obs)\n",
    "        self.set((\"q_values\", t), q_values)\n",
    "\n",
    "        # Sets the action\n",
    "        if choose_action:\n",
    "            action = q_values.argmax(1)\n",
    "            self.set((\"action\", t), action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4299c558",
   "metadata": {},
   "source": [
    "### Creating an Exploration method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4244dd5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "As Q-learning, DQN needs some exploration to prevent too early convergence.\n",
    "Here we will use the simple $\\epsilon$-greedy exploration method. The method\n",
    "is implemented as an agent which chooses an action based on the Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f29b3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGreedyActionSelector(Agent):\n",
    "    def __init__(self, epsilon):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, t, **kwargs):\n",
    "        q_values = self.get((\"q_values\", t))\n",
    "        nb_actions = q_values.size()[1]\n",
    "        size = q_values.size()[0]\n",
    "        is_random = torch.rand(size).lt(self.epsilon).float()\n",
    "        random_action = torch.randint(low=0, high=nb_actions, size=(size,))\n",
    "        max_action = q_values.max(1)[1]\n",
    "        action = is_random * random_action + (1 - is_random) * max_action\n",
    "        action = action.long()\n",
    "        self.set((\"action\", t), action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454f248f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Training and evaluation environments\n",
    "\n",
    "We build two environments: one for training and another one for evaluation.\n",
    "\n",
    "For training, it is more efficient to use an autoreset agent, as we do not\n",
    "want to waste time if the task is done in an environment sooner than in the\n",
    "others.\n",
    "\n",
    "By contrast, for evaluation, we just need to perform a fixed number of\n",
    "episodes (for statistics), thus it is more convenient to use a\n",
    "noautoreset agent with a set of environments and just run one episode in\n",
    "each environment. Thus we can use the `env/done` stop variable and take the\n",
    "average over the cumulated reward of all environments.\n",
    "\n",
    "See [this\n",
    "notebook](https://colab.research.google.com/drive/1Ui481r47fNHCQsQfKwdoNEVrEiqAEokh?usp=sharing)\n",
    "for explanations about agents and environment agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a83f636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from bbrl.agents.gymnasium import make_env, GymAgent, ParallelGymAgent\n",
    "from functools import partial\n",
    "\n",
    "def get_env_agents(cfg, *, autoreset=True, include_last_state=True) -> Tuple[GymAgent, GymAgent]:\n",
    "    # Returns a pair of environments (train / evaluation) based on a configuration `cfg`\n",
    "    \n",
    "    # Train environment\n",
    "    train_env_agent = ParallelGymAgent(\n",
    "        partial(make_env, cfg.gym_env.env_name, autoreset=autoreset),\n",
    "        cfg.algorithm.n_envs, \n",
    "        include_last_state=include_last_state\n",
    "    ).seed(cfg.algorithm.seed)\n",
    "\n",
    "    # Test environment\n",
    "    eval_env_agent = ParallelGymAgent(\n",
    "        partial(make_env, cfg.gym_env.env_name), \n",
    "        cfg.algorithm.nb_evals,\n",
    "        include_last_state=include_last_state\n",
    "    ).seed(cfg.algorithm.seed)\n",
    "\n",
    "    return train_env_agent, eval_env_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657d46cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dqn_agent(cfg, train_env_agent, eval_env_agent):\n",
    "    obs_size, act_size = train_env_agent.get_obs_and_actions_sizes()\n",
    "\n",
    "    # Get the two agents (critic and target critic)\n",
    "    critic = DiscreteQAgent(obs_size, cfg.algorithm.architecture.hidden_size, act_size)\n",
    "    target_critic = copy.deepcopy(critic)\n",
    "\n",
    "    # Builds the train agent that will produce transitions\n",
    "    explorer = EGreedyActionSelector(cfg.algorithm.epsilon)\n",
    "    tr_agent = Agents(train_env_agent, critic, explorer)\n",
    "    train_agent = TemporalAgent(tr_agent)\n",
    "\n",
    "    # Creates two temporal agents just for \"replaying\" some parts\n",
    "    # of the transition buffer    \n",
    "    q_agent = TemporalAgent(critic)\n",
    "    target_q_agent = TemporalAgent(target_critic)\n",
    "\n",
    "\n",
    "    # Get an agent that is executed on a complete workspace\n",
    "    ev_agent = Agents(eval_env_agent, critic)\n",
    "    eval_agent = TemporalAgent(ev_agent)\n",
    "\n",
    "    return train_agent, eval_agent, q_agent, target_q_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50689925",
   "metadata": {},
   "source": [
    "### The Logger class\n",
    "\n",
    "The logger is in charge of collecting statistics during the training\n",
    "process.\n",
    "\n",
    "Having logging provided under the hood is one of the features allowing you\n",
    "to save time when using RL libraries like BBRL.\n",
    "\n",
    "In these notebooks, the logger is defined as `bbrl.utils.logger.TFLogger` so as\n",
    "to use a tensorboard visualisation (see the parameters part `params = { \"logger\":{ ...` below).\n",
    "\n",
    "Note that the BBRL Logger is also saving the log in a readable format such\n",
    "that you can use `Logger.read_directories(...)` to read multiple logs, create\n",
    "a dataframe, and analyze many experiments afterward in a notebook for\n",
    "instance. The code for the different kinds of loggers is available in the\n",
    "[bbrl/utils/logger.py](https://github.com/osigaud/bbrl/blob/master/src/bbrl/utils/logger.py)\n",
    "file.\n",
    "\n",
    "`instantiate_class` is an inner BBRL mechanism. The\n",
    "`instantiate_class`function is available in the\n",
    "[`bbrl/__init__.py`](https://github.com/osigaud/bbrl/blob/master/src/bbrl/__init__.py)\n",
    "file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c59091",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "lines_to_next_cell": 2,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from bbrl import instantiate_class\n",
    "\n",
    "class Logger():\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        self.logger = instantiate_class(cfg.logger)\n",
    "\n",
    "    def add_log(self, log_string, loss, steps):\n",
    "        self.logger.add_scalar(log_string, loss.item(), steps)\n",
    "\n",
    "    # A specific function for RL algorithms having a critic, an actor and an entropy losses\n",
    "    def log_losses(self, critic_loss, entropy_loss, actor_loss, steps):\n",
    "        self.add_log(\"critic_loss\", critic_loss, steps)\n",
    "        self.add_log(\"entropy_loss\", entropy_loss, steps)\n",
    "        self.add_log(\"actor_loss\", actor_loss, steps)\n",
    "\n",
    "    def log_reward_losses(self, rewards, nb_steps):\n",
    "        self.add_log(\"reward/mean\", rewards.mean(), nb_steps)\n",
    "        self.add_log(\"reward/max\", rewards.max(), nb_steps)\n",
    "        self.add_log(\"reward/min\", rewards.min(), nb_steps)\n",
    "        self.add_log(\"reward/median\", rewards.median(), nb_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de247d98",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Setup the optimizers\n",
    "\n",
    "We use a single optimizer to tune the parameters of the actor (in the\n",
    "prob_agent part) and the critic (in the critic_agent part). It would be\n",
    "possible to have two optimizers which would work separately on the parameters\n",
    "of each component agent, but it would be more complicated because updating the\n",
    "actor requires the gradient of the critic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f309e97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the optimizer over the q agent\n",
    "def setup_optimizers(cfg, q_agent):\n",
    "    optimizer_args = get_arguments(cfg.optimizer)\n",
    "    parameters = q_agent.parameters()\n",
    "    optimizer = get_class(cfg.optimizer)(parameters, **optimizer_args)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbabb39",
   "metadata": {},
   "source": [
    "### Compute critic loss\n",
    "\n",
    "Detailed explanations of the function to compute the critic loss when using\n",
    "`autoreset=False` are given in [this\n",
    "notebook](http://master-dac.isir.upmc.fr/rld/rl/03-1-dqn-introduction.student.ipynb).\n",
    "The case where we use `autoreset=True` is very similar, but we need to\n",
    "specify that we use the first part of the Q-values (`q_values[0]`) for\n",
    "representing $Q(s_t,a_t)$ and the second part (`q_values[1]`) for representing\n",
    "$Q(s_{t+1},a)$, as these values are stored into a transition model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e91b84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_critic_loss(cfg, reward, must_bootstrap, q_values, target_q_values, action):\n",
    "\n",
    "    # To be completed...\n",
    "\n",
    "    #Adapt from the previous notebook and adapt to our case (target Q network)\n",
    "    #Don't forget that we deal with transitions (and not episodes)\n",
    "    assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "\n",
    "    # Compute critic loss (no need to use must_bootstrap here since we are dealing with \"full\" transitions)\n",
    "    mse = nn.MSELoss()\n",
    "    critic_loss = mse(target, qvals)\n",
    "    return critic_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645a2283",
   "metadata": {},
   "source": [
    "## Main training loop\n",
    "\n",
    "Note that everything about the shared workspace between all the agents is\n",
    "completely hidden under the hood. This results in a gain of productivity, at\n",
    "the expense of having to dig into the BBRL code if you want to understand the\n",
    "details, change the multiprocessing model, etc.\n",
    "\n",
    "### Agent execution\n",
    "\n",
    "This is the tricky part with BBRL, the one we need to understand in detail.\n",
    "The difficulty lies in the copy of the last step and the way to deal with the\n",
    "n_steps return.\n",
    "\n",
    "The call to `train_agent(workspace, t=1, n_steps=cfg.algorithm.n_timesteps -\n",
    "1, stochastic=True)` makes the agent run a number of steps in the workspace.\n",
    "In practice, it calls the\n",
    "[`__call__(...)`](https://github.com/osigaud/bbrl/blob/master/src/bbrl/agents/agent.py#L59)\n",
    "function which makes a forward pass of the agent network using the workspace\n",
    "data and updates the workspace accordingly.\n",
    "\n",
    "Now, if we start at the first epoch (`epoch=0`), we start from the first step\n",
    "(`t=0`). But when subsequently we perform the next epochs (`epoch>0`), we must\n",
    "not forget to cover the transition at the border between the previous epoch\n",
    "and the current epoch. To avoid this risk, we copy the information from the\n",
    "last time step of the previous epoch into the first time step of the next\n",
    "epoch.\n",
    "\n",
    "Note that we `optimizer.zero_grad()`, `loss.backward()` and `optimizer.step()`\n",
    "lines. `optimizer.zero_grad()` is necessary to cancel all the gradients\n",
    "computed at the previous iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426a1b4f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def run_dqn(cfg, compute_critic_loss):\n",
    "    # 1)  Build the  logger\n",
    "    logger = Logger(cfg)\n",
    "    best_reward = float('-inf')\n",
    "\n",
    "    # 2) Create the environment agents\n",
    "    train_env_agent, eval_env_agent = get_env_agents(cfg)\n",
    "\n",
    "    # 3) Create the DQN-like Agent\n",
    "    train_agent, eval_agent, q_agent, target_q_agent = create_dqn_agent(\n",
    "        cfg, train_env_agent, eval_env_agent\n",
    "    )\n",
    "\n",
    "    # 5) Configure the workspace to the right dimension\n",
    "    # Note that no parameter is needed to create the workspace.\n",
    "    # In the training loop, calling the agent() and critic_agent()\n",
    "    # will take the workspace as parameter\n",
    "    train_workspace = Workspace()  # Used for training\n",
    "    rb = ReplayBuffer(max_size=cfg.algorithm.buffer_size)\n",
    "\n",
    "    # 6) Configure the optimizer over the dqn agent\n",
    "    optimizer = setup_optimizers(cfg, q_agent)\n",
    "    nb_steps = 0\n",
    "    last_eval_step = 0\n",
    "    last_critic_update_step = 0\n",
    "    best_agent = eval_agent.agent.agents[1]\n",
    "\n",
    "    # 7) Training loop\n",
    "    pbar = tqdm(range(cfg.algorithm.max_epochs))\n",
    "    for epoch in pbar:\n",
    "        # Execute the agent in the workspace\n",
    "        if epoch > 0:\n",
    "            train_workspace.zero_grad()\n",
    "            train_workspace.copy_n_last_steps(1)\n",
    "            train_agent(\n",
    "                train_workspace, t=1, n_steps=cfg.algorithm.n_steps, stochastic=True\n",
    "            )\n",
    "        else:\n",
    "            train_agent(\n",
    "                train_workspace, t=0, n_steps=cfg.algorithm.n_steps, stochastic=True\n",
    "            )\n",
    "\n",
    "        # Get the transitions\n",
    "        transition_workspace = train_workspace.get_transitions()\n",
    "\n",
    "        action = transition_workspace[\"action\"]\n",
    "        nb_steps += action[0].shape[0]\n",
    "        \n",
    "        # Adds the transitions to the workspace\n",
    "        rb.put(transition_workspace)\n",
    "        if rb.size() > cfg.algorithm.learning_starts:\n",
    "            for _ in range(cfg.algorithm.n_updates):\n",
    "                rb_workspace = rb.get_shuffled(cfg.algorithm.batch_size)\n",
    "\n",
    "                # The q agent needs to be executed on the rb_workspace workspace (gradients are removed in workspace)\n",
    "                q_agent(rb_workspace, t=0, n_steps=2, choose_action=False)\n",
    "                q_values, terminated, reward, action = rb_workspace[\n",
    "                    \"q_values\", \"env/terminated\", \"env/reward\", \"action\"\n",
    "                ]\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    target_q_agent(rb_workspace, t=0, n_steps=2, stochastic=True)\n",
    "                target_q_values = rb_workspace[\"q_values\"]\n",
    "\n",
    "                # Determines whether values of the critic should be propagated\n",
    "                must_bootstrap = ~terminated[1]\n",
    "\n",
    "                # Compute critic loss\n",
    "                # FIXME: homogénéiser les notations (soit tranche temporelle, soit rien)\n",
    "                critic_loss = compute_critic_loss(\n",
    "                    cfg, reward, must_bootstrap, q_values, target_q_values[1], action\n",
    "                )\n",
    "                # Store the loss for tensorboard display\n",
    "                logger.add_log(\"critic_loss\", critic_loss, nb_steps)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(q_agent.parameters(), cfg.algorithm.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                if nb_steps - last_critic_update_step > cfg.algorithm.target_critic_update:\n",
    "                    last_critic_update_step = nb_steps\n",
    "                    target_q_agent.agent = copy.deepcopy(q_agent.agent)\n",
    "\n",
    "        # Evaluate the current policy\n",
    "        if nb_steps - last_eval_step > cfg.algorithm.eval_interval:\n",
    "            last_eval_step = nb_steps\n",
    "            eval_workspace = Workspace()\n",
    "            eval_agent(\n",
    "                eval_workspace, t=0, stop_variable=\"env/done\", choose_action=True\n",
    "            )\n",
    "            rewards = eval_workspace[\"env/cumulated_reward\"][-1]\n",
    "            mean = rewards.mean()\n",
    "            logger.log_reward_losses(rewards, nb_steps)\n",
    "            pbar.set_description(f\"nb steps: {nb_steps}, reward: {mean:.3f}\")\n",
    "            if cfg.save_best and mean > best_reward:\n",
    "                best_reward = mean\n",
    "                best_agent = copy.deepcopy(eval_agent.agent.agents[1])\n",
    "                directory = \"./dqn_critic/\"\n",
    "                if not os.path.exists(directory):\n",
    "                    os.makedirs(directory)\n",
    "                filename = directory + \"dqn0_\" + str(mean.item()) + \".agt\"\n",
    "                eval_agent.save_model(filename)\n",
    "\n",
    "    return best_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950031ca",
   "metadata": {},
   "source": [
    "## Definition of the parameters\n",
    "\n",
    "The logger is defined as `bbrl.utils.logger.TFLogger` so as to use a\n",
    "tensorboard visualisation.\n",
    "\n",
    "### Launching tensorboard to visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbe7c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_tensorboard(\"./tblogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db6c86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "  \"save_best\": False,\n",
    "  \"logger\":{\n",
    "    \"classname\": \"bbrl.utils.logger.TFLogger\",\n",
    "    \"log_dir\": \"./tblogs/dqn-buffer-\" + str(time.time()),\n",
    "    \"cache_size\": 10000,\n",
    "    \"every_n_seconds\": 10,\n",
    "    \"verbose\": False,    \n",
    "    },\n",
    "\n",
    "  \"algorithm\":{\n",
    "    \"seed\": 4,\n",
    "    \"max_grad_norm\": 0.5,\n",
    "    \"epsilon\": 0.02,\n",
    "    \"n_envs\": 8,\n",
    "    \"n_steps\": 32,\n",
    "    \"n_updates\": 32,\n",
    "    \"eval_interval\": 2000,\n",
    "    \"learning_starts\": 2000,\n",
    "    \"nb_evals\": 10,\n",
    "    \"buffer_size\": 1e6,\n",
    "    \"batch_size\": 256,\n",
    "    \"target_critic_update\": 5000,\n",
    "    \"max_epochs\": 3500,\n",
    "    \"discount_factor\": 0.99,\n",
    "    \"architecture\":{\"hidden_size\": [64, 64]},\n",
    "  },\n",
    "  \"gym_env\":{\n",
    "    \"env_name\": \"CartPole-v1\",\n",
    "  },\n",
    "  \"optimizer\":\n",
    "  {\n",
    "    \"classname\": \"torch.optim.Adam\",\n",
    "    \"lr\": 1e-3,\n",
    "  }\n",
    "}\n",
    "\n",
    "config=OmegaConf.create(params)\n",
    "torch.manual_seed(config.algorithm.seed)\n",
    "best_agent = run_dqn(config, compute_critic_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c46770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "env = make_env(config.gym_env.env_name, render_mode=\"rgb_array\")\n",
    "record_video(env, best_agent, \"videos/dqn-full.mp4\")\n",
    "video_display(\"videos/dqn-full.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847eb335",
   "metadata": {},
   "source": [
    "## Coding Exercise: Double DQN (DDQN)\n",
    "\n",
    "In DQN, the same network is responsible for selecting and estimating the best\n",
    "next action (in the TD-target) and that may lead to over-estimation: the\n",
    "action which q-value is over-estimated will be chosen more often. As a result,\n",
    "training is slower.\n",
    "\n",
    "To reduce over-estimation, double q-learning (and then DDQN) was proposed. It\n",
    "decouples the action selection from the value estimation.\n",
    "\n",
    "Concretely, in DQN, the target value in the critic loss (used to update the Q\n",
    "critic) for a sample at time $t$ is defined as:\n",
    "\n",
    "$$Y^{DQN}_{t} = r_{t+1} + \\gamma{Q}\\left(s_{t+1}, \\arg\\max_{a}Q\\left(s_{t+1},\n",
    "a; \\mathbb{\\theta}_{target}\\right); \\mathbb{\\theta}_{target}\\right)$$\n",
    "\n",
    "where the target network `target_q_agent` with parameters\n",
    "$\\mathbb{\\theta}_{target}$ is used for both action selection and estimation,\n",
    "and can therefore be rewritten:\n",
    "\n",
    "$$Y^{DQN}_{t} = r_{t+1} + \\gamma \\max_{a}{Q}\\left(s_{t+1}, a;\n",
    "\\mathbb{\\theta}_{target}\\right)$$\n",
    "\n",
    "Instead, DDQN uses the online critic `q_agent` with parameters\n",
    "$\\mathbb{\\theta}_{online}$ to select the action, whereas it uses the target\n",
    "network `target_q_agent` to estimate the associated Q-values:\n",
    "\n",
    "$$Y^{DDQN}_{t} = r_{t+1} + \\gamma{Q}\\left(s_{t+1}, \\arg\\max_{a}Q\\left(s_{t+1},\n",
    "a; \\mathbb{\\theta}_{online}\\right); \\mathbb{\\theta}_{target}\\right)$$\n",
    "\n",
    "The goal in this exercise is for you to write the update method for `DDQN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8be5f5b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def compute_ddqn_loss(cfg, reward, must_bootstrap, q_values, target_q_values, action):\n",
    "    # To be completed...\n",
    "\n",
    "    Implement the double DQN loss\n",
    "    assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "\n",
    "    # Compute critic loss\n",
    "    mse = nn.MSELoss()\n",
    "    critic_loss = mse(target, qvals)\n",
    "    return critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd4e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "  \"save_best\": False,\n",
    "  \"logger\":{\n",
    "    \"classname\": \"bbrl.utils.logger.TFLogger\",\n",
    "    \"log_dir\": \"./tblogs/ddqn-buffer-\" + str(time.time()),\n",
    "    \"cache_size\": 10000,\n",
    "    \"every_n_seconds\": 10,\n",
    "    \"verbose\": False,    \n",
    "    },\n",
    "\n",
    "  \"algorithm\":{\n",
    "    \"seed\": 4,\n",
    "    \"max_grad_norm\": 0.5,\n",
    "    \"epsilon\": 0.02,\n",
    "    \"n_envs\": 8,\n",
    "    \"n_steps\": 32,\n",
    "    \"n_updates\": 32,\n",
    "    \"eval_interval\": 2000,\n",
    "    \"learning_starts\": 2000,\n",
    "    \"nb_evals\": 10,\n",
    "    \"buffer_size\": 1e6,\n",
    "    \"batch_size\": 256,\n",
    "    \"target_critic_update\": 5000,\n",
    "    \"max_epochs\": 3500,\n",
    "    \"discount_factor\": 0.99,\n",
    "    \"architecture\":{\"hidden_size\": [128, 128]},\n",
    "  },\n",
    "  \"gym_env\":{\n",
    "    \"env_name\": \"CartPole-v1\",\n",
    "  },\n",
    "  \"optimizer\":\n",
    "  {\n",
    "    \"classname\": \"torch.optim.Adam\",\n",
    "    \"lr\": 1e-3,\n",
    "  }\n",
    "}\n",
    "\n",
    "config=OmegaConf.create(params)\n",
    "torch.manual_seed(config.algorithm.seed)\n",
    "best_agent = run_dqn(config, compute_ddqn_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5806e46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "env = make_env(config.gym_env.env_name, render_mode=\"rgb_array\")\n",
    "record_video(env, best_agent, \"videos/dqn-double.mp4\")\n",
    "video_display(\"videos/dqn-double.mp4\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
