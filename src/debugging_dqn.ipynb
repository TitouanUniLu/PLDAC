{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[easypip] Installing bbrl_gymnasium>=0.2.0\n",
      "[easypip] Installing bbrl_gymnasium[box2d]\n",
      "[easypip] Installing bbrl_gymnasium[classic_control]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from easypip import easyimport, easyinstall, is_notebook\n",
    "except ModuleNotFoundError as e:\n",
    "    get_ipython().run_line_magic(\"pip\", \"install easypip\")\n",
    "    from easypip import easyimport, easyinstall, is_notebook\n",
    "\n",
    "easyinstall(\"bbrl>=0.2.2\")\n",
    "easyinstall(\"swig\")\n",
    "easyinstall(\"bbrl_gymnasium>=0.2.0\")\n",
    "easyinstall(\"bbrl_gymnasium[box2d]\")\n",
    "easyinstall(\"bbrl_gymnasium[classic_control]\")\n",
    "easyinstall(\"tensorboard\")\n",
    "easyinstall(\"moviepy\")\n",
    "easyinstall(\"box2d-kengz\")\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "from moviepy.editor import ipython_display as video_display\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Tuple, Optional\n",
    "from functools import partial\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "import bbrl_gymnasium\n",
    "\n",
    "import copy\n",
    "from abc import abstractmethod, ABC\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from time import strftime\n",
    "OmegaConf.register_new_resolver(\n",
    "    \"current_time\", lambda: strftime(\"%Y%m%d-%H%M%S\"), replace=True\n",
    ")\n",
    "# Imports all the necessary classes and functions from BBRL\n",
    "from bbrl.agents.agent import Agent\n",
    "from bbrl import get_arguments, get_class, instantiate_class\n",
    "# The workspace is the main class in BBRL, this is where all data is collected and stored\n",
    "from bbrl.workspace import Workspace\n",
    "\n",
    "# Agents(agent1,agent2,agent3,...) executes the different agents the one after the other\n",
    "# TemporalAgent(agent) executes an agent over multiple timesteps in the workspace, \n",
    "# or until a given condition is reached\n",
    "from bbrl.agents import Agents, TemporalAgent\n",
    "\n",
    "# ParallelGymAgent is an agent able to execute a batch of gymnasium environments\n",
    "# with auto-resetting. These agents produce multiple variables in the workspace:\n",
    "# ’env/env_obs’, ’env/reward’, ’env/timestep’, ’env/terminated’,\n",
    "# 'env/truncated', 'env/done', ’env/cumulated_reward’, ... \n",
    "# \n",
    "# When called at timestep t=0, the environments are automatically reset. At\n",
    "# timestep t>0, these agents will read the ’action’ variable in the workspace at\n",
    "# time t − 1\n",
    "from bbrl.agents.gymnasium import GymAgent, ParallelGymAgent, make_env, record_video\n",
    "\n",
    "# Replay buffers are useful to store past transitions when training\n",
    "from bbrl.utils.replay_buffer import ReplayBuffer\n",
    "# Utility function for launching tensorboard\n",
    "# For Colab - otherwise, it is easier and better to launch tensorboard from\n",
    "# the terminal\n",
    "def setup_tensorboard(path):\n",
    "    path = Path(path)\n",
    "    answer = \"\"\n",
    "    if is_notebook():\n",
    "        if get_ipython().__class__.__module__ == \"google.colab._shell\":\n",
    "            answer = \"y\"\n",
    "        while answer not in [\"y\", \"n\"]:\n",
    "                answer = input(f\"Do you want to launch tensorboard in this notebook [y/n] \").lower()\n",
    "\n",
    "    if answer == \"y\":\n",
    "        get_ipython().run_line_magic(\"load_ext\", \"tensorboard\")\n",
    "        get_ipython().run_line_magic(\"tensorboard\", f\"--logdir {path.absolute()}\")\n",
    "    else:\n",
    "        import sys\n",
    "        import os\n",
    "        import os.path as osp\n",
    "        print(f\"Launch tensorboard from the shell:\\n{osp.dirname(sys.executable)}/tensorboard --logdir={path.absolute()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "def build_mlp(sizes, activation, output_activation=nn.Identity()):\n",
    "    \"\"\"Helper function to build a multi-layer perceptron (function from $\\mathbb R^n$ to $\\mathbb R^p$)\n",
    "    \n",
    "    Args:\n",
    "        sizes (List[int]): the number of neurons at each layer\n",
    "        activation (nn.Module): a PyTorch activation function (after each layer but the last)\n",
    "        output_activation (nn.Module): a PyTorch activation function (last layer)\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    for j in range(len(sizes) - 1):\n",
    "        act = activation if j < len(sizes) - 2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j + 1]), act]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteQAgent(Agent):\n",
    "    def __init__(self, state_dim, hidden_layers, action_dim):\n",
    "        super().__init__()\n",
    "        self.model = build_mlp(\n",
    "            [state_dim] + list(hidden_layers) + [action_dim], activation=nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, t, choose_action=True, **kwargs):\n",
    "        #print('ENTERING DISCRETEQAGENT AT TIME ', t)\n",
    "        obs = self.get((\"env/env_obs\", t))\n",
    "        #print('obs', obs)\n",
    "        #print(obs.shape)\n",
    "        q_values = self.model(obs)\n",
    "        self.set((\"q_values\", t), q_values)\n",
    "\n",
    "        # Sets the action\n",
    "        if choose_action:\n",
    "            action = q_values.argmax(1)\n",
    "            #print('action', action)\n",
    "            #print(action.shape)\n",
    "            self.set((\"action\", t), action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSRSIZE = 4\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(4, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)        \n",
    "        self.fc1 = nn.Linear(16 * 18 * 18, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 4)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))        \n",
    "        x = x.view(-1, 16 * 18 * 18)  \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2  \n",
    "import numpy as np\n",
    "\n",
    "class ImageAgent(Agent):\n",
    "    def __init__(self, env_agent):\n",
    "        super().__init__()\n",
    "        self.env_agent = env_agent\n",
    "        self.cnn = SimpleCNN()\n",
    "        self.image_buffer = [\n",
    "            [torch.zeros(84, 84) for _ in range(4)]\n",
    "            for _ in range(self.env_agent.num_envs)\n",
    "        ]\n",
    "\n",
    "    def forward(self, t: int, **kwargs):\n",
    "        features = []\n",
    "        total_loss = 0.0\n",
    "        for env_index in range(self.env_agent.num_envs):\n",
    "            image = self.env_agent.envs[env_index].render()\n",
    "            processed_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "            processed_image = cv2.resize(\n",
    "                processed_image, (84, 84)\n",
    "            )  \n",
    "            #plt.imshow(processed_image)\n",
    "            #plt.show()\n",
    "            processed_image_tensor = torch.tensor(processed_image, dtype=float)\n",
    "\n",
    "            self.image_buffer[env_index].pop(0)\n",
    "            self.image_buffer[env_index].append(processed_image_tensor)\n",
    "\n",
    "            stacked_frames = np.stack(self.image_buffer[env_index], axis=0)\n",
    "            input_tensor = (\n",
    "                torch.tensor(stacked_frames, dtype=torch.float32).unsqueeze(0) / 255.0\n",
    "            )\n",
    "\n",
    "            self.cnn.train()  # Set the CNN to training mode\n",
    "\n",
    "            cnn_output = self.cnn(input_tensor).squeeze(0)\n",
    "\n",
    "            real_observations = self.get((\"env/env_obs\", t))[env_index]\n",
    "\n",
    "            loss = F.mse_loss(cnn_output, real_observations)  # Compute the MSE loss\n",
    "\n",
    "            # Backpropagation\n",
    "            self.cnn.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.cnn.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            features.append(cnn_output)\n",
    "        \n",
    "        # print(f' loss at time {t}: {total_loss}')\n",
    "        # total_loss_tensor = torch.tensor(total_loss, requires_grad=True)\n",
    "            \n",
    "\n",
    "        features_tensor = torch.stack(features)\n",
    "        self.set((\"env/features\", t), features_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGreedyActionSelector(Agent):\n",
    "    def __init__(self, epsilon):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, t, **kwargs):\n",
    "        q_values = self.get((\"q_values\", t))\n",
    "        nb_actions = q_values.size()[1]\n",
    "        size = q_values.size()[0]\n",
    "        is_random = torch.rand(size).lt(self.epsilon).float()\n",
    "        random_action = torch.randint(low=0, high=nb_actions, size=(size,))\n",
    "        max_action = q_values.max(1)[1]\n",
    "        action = is_random * random_action + (1 - is_random) * max_action\n",
    "        action = action.long()\n",
    "        self.set((\"action\", t), action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from bbrl.agents.gymnasium import make_env, GymAgent, ParallelGymAgent\n",
    "from functools import partial\n",
    "\n",
    "def get_env_agents(cfg, *, autoreset=True, include_last_state=True) -> Tuple[GymAgent, GymAgent]:\n",
    "    # Returns a pair of environments (train / evaluation) based on a configuration `cfg`\n",
    "    \n",
    "    # Train environment\n",
    "    train_env_agent = ParallelGymAgent(\n",
    "        partial(make_env, cfg.gym_env.env_name, render_mode=\"rgb_array\", autoreset=autoreset),\n",
    "        cfg.algorithm.n_envs, \n",
    "        include_last_state=include_last_state\n",
    "    ).seed(cfg.algorithm.seed)\n",
    "\n",
    "    # Test environment\n",
    "    eval_env_agent = ParallelGymAgent(\n",
    "        partial(make_env, cfg.gym_env.env_name, render_mode=\"rgb_array\"), \n",
    "        cfg.algorithm.nb_evals,\n",
    "        include_last_state=include_last_state\n",
    "    ).seed(cfg.algorithm.seed)\n",
    "\n",
    "    return train_env_agent, eval_env_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dqn_agent(cfg, train_env_agent, eval_env_agent):\n",
    "    obs_size, act_size = train_env_agent.get_obs_and_actions_sizes()\n",
    "    train_image_agent = ImageAgent(train_env_agent)\n",
    "    eval_image_agent = ImageAgent(eval_env_agent)\n",
    "    # Get the two agents (critic and target critic)\n",
    "    critic = DiscreteQAgent(obs_size, cfg.algorithm.architecture.hidden_size, act_size)\n",
    "    target_critic = copy.deepcopy(critic)\n",
    "\n",
    "    # Builds the train agent that will produce transitions\n",
    "    explorer = EGreedyActionSelector(cfg.algorithm.epsilon)\n",
    "    tr_agent = Agents(train_env_agent, train_image_agent, critic, explorer)\n",
    "    train_agent = TemporalAgent(tr_agent)\n",
    "\n",
    "    # Creates two temporal agents just for \"replaying\" some parts\n",
    "    # of the transition buffer    \n",
    "    q_agent = TemporalAgent(critic)\n",
    "    target_q_agent = TemporalAgent(target_critic)\n",
    "\n",
    "\n",
    "    # Get an agent that is executed on a complete workspace\n",
    "    ev_agent = Agents(eval_env_agent, eval_image_agent, critic)\n",
    "    eval_agent = TemporalAgent(ev_agent)\n",
    "\n",
    "    return train_agent, eval_agent, q_agent, target_q_agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bbrl import instantiate_class\n",
    "\n",
    "class Logger():\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        self.logger = instantiate_class(cfg.logger)\n",
    "\n",
    "    def add_log(self, log_string, loss, steps):\n",
    "        self.logger.add_scalar(log_string, loss.item(), steps)\n",
    "\n",
    "    def log_losses(self, critic_loss, entropy_loss, actor_loss, steps):\n",
    "        self.add_log(\"critic_loss\", critic_loss, steps)\n",
    "        self.add_log(\"entropy_loss\", entropy_loss, steps)\n",
    "        self.add_log(\"actor_loss\", actor_loss, steps)\n",
    "\n",
    "    def log_reward_losses(self, rewards, nb_steps):\n",
    "        self.add_log(\"reward/mean\", rewards.mean(), nb_steps)\n",
    "        self.add_log(\"reward/max\", rewards.max(), nb_steps)\n",
    "        self.add_log(\"reward/min\", rewards.min(), nb_steps)\n",
    "        self.add_log(\"reward/median\", rewards.median(), nb_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_optimizers(cfg, q_agent):\n",
    "    optimizer_args = get_arguments(cfg.optimizer)\n",
    "    parameters = q_agent.parameters()\n",
    "    optimizer = get_class(cfg.optimizer)(parameters, **optimizer_args)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_critic_loss(cfg, reward, must_bootstrap, q_values, target_q_values, action):\n",
    "    q_values_for_actions = q_values.gather(2, action.unsqueeze(-1)).squeeze(-1)\n",
    "    \n",
    "    # Compute the max Q-value for the next state, but not for the last timestep\n",
    "    next_q_values = q_values[1:].max(dim=2)[0]\n",
    "    # Compute the expected Q-values (target) for the current state and action\n",
    "    # Assuming next_q_values and must_bootstrap are correctly aligned and one step \"ahead\" of reward\n",
    "    target_q_values = reward[1:] + cfg[\"algorithm\"][\"discount_factor\"] * next_q_values * must_bootstrap\n",
    "\n",
    "    \n",
    "    # Compute the loss as the mean squared error between the current and target Q-values\n",
    "    loss = F.mse_loss(q_values_for_actions[:-1], target_q_values)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dqn(cfg, compute_critic_loss):\n",
    "    # 1)  Build the  logger\n",
    "    logger = Logger(cfg)\n",
    "    best_reward = float('-inf')\n",
    "\n",
    "    # 2) Create the environment agents\n",
    "    train_env_agent, eval_env_agent = get_env_agents(cfg)\n",
    "\n",
    "    # 3) Create the DQN-like Agent\n",
    "    train_agent, eval_agent, q_agent, target_q_agent = create_dqn_agent(\n",
    "        cfg, train_env_agent, eval_env_agent\n",
    "    )\n",
    "\n",
    "    # 5) Configure the workspace to the right dimension\n",
    "    # Note that no parameter is needed to create the workspace.\n",
    "    # In the training loop, calling the agent() and critic_agent()\n",
    "    # will take the workspace as parameter\n",
    "    train_workspace = Workspace()  # Used for training\n",
    "    rb = ReplayBuffer(max_size=cfg.algorithm.buffer_size)\n",
    "\n",
    "    # 6) Configure the optimizer over the dqn agent\n",
    "    optimizer = setup_optimizers(cfg, q_agent)\n",
    "    nb_steps = 0\n",
    "    last_eval_step = 0\n",
    "    last_critic_update_step = 0\n",
    "    best_agent = eval_agent.agent.agents[1]\n",
    "\n",
    "    # 7) Training loop\n",
    "    print('Entering training...')\n",
    "    pbar = tqdm(range(cfg.algorithm.max_epochs))\n",
    "    for epoch in pbar:\n",
    "        # Execute the agent in the workspace\n",
    "        if epoch > 0:\n",
    "            train_workspace.zero_grad()\n",
    "            train_workspace.copy_n_last_steps(1)\n",
    "            train_agent(\n",
    "                train_workspace, t=1, n_steps=cfg.algorithm.n_steps, stochastic=True\n",
    "            )\n",
    "        else:\n",
    "            #print('executing train agent')\n",
    "            train_agent(\n",
    "                train_workspace, t=0, n_steps=cfg.algorithm.n_steps, stochastic=True\n",
    "            )\n",
    "            #print('executed train agent at time 0')\n",
    "\n",
    "        # Get the transitions\n",
    "        transition_workspace = train_workspace.get_transitions()\n",
    "        #print(transition_workspace)\n",
    "\n",
    "        action = transition_workspace[\"action\"]\n",
    "        nb_steps += action[0].shape[0]\n",
    "        \n",
    "        # Adds the transitions to the workspace\n",
    "        rb.put(transition_workspace)\n",
    "        #print('rb.size() ', rb.size())\n",
    "        #print('cfg.algorithm.learning_starts ', cfg.algorithm.learning_starts)\n",
    "        if rb.size() > cfg.algorithm.learning_starts:\n",
    "            for _ in range(cfg.algorithm.n_updates):\n",
    "                rb_workspace = rb.get_shuffled(cfg.algorithm.batch_size)\n",
    "\n",
    "                # The q agent needs to be executed on the rb_workspace workspace (gradients are removed in workspace)\n",
    "                q_agent(rb_workspace, t=0, n_steps=2, choose_action=False)\n",
    "                q_values, terminated, reward, action = rb_workspace[\n",
    "                    \"q_values\", \"env/terminated\", \"env/reward\", \"action\"\n",
    "                ]\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    target_q_agent(rb_workspace, t=0, n_steps=2, stochastic=True)\n",
    "                target_q_values = rb_workspace[\"q_values\"]\n",
    "\n",
    "                # Determines whether values of the critic should be propagated\n",
    "                must_bootstrap = ~terminated[1]\n",
    "\n",
    "                # Compute critic loss\n",
    "                # FIXME: homogénéiser les notations (soit tranche temporelle, soit rien)\n",
    "                critic_loss = compute_critic_loss(\n",
    "                    cfg, reward, must_bootstrap, q_values, target_q_values[1], action\n",
    "                )\n",
    "                # Store the loss for tensorboard display\n",
    "                logger.add_log(\"critic_loss\", critic_loss, nb_steps)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(q_agent.parameters(), cfg.algorithm.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                if nb_steps - last_critic_update_step > cfg.algorithm.target_critic_update:\n",
    "                    last_critic_update_step = nb_steps\n",
    "                    target_q_agent.agent = copy.deepcopy(q_agent.agent)\n",
    "\n",
    "        # Evaluate the current policy\n",
    "        if nb_steps - last_eval_step > cfg.algorithm.eval_interval:\n",
    "            last_eval_step = nb_steps\n",
    "            eval_workspace = Workspace()\n",
    "            eval_agent(\n",
    "                eval_workspace, t=0, stop_variable=\"env/done\", choose_action=True\n",
    "            )\n",
    "            rewards = eval_workspace[\"env/cumulated_reward\"][-1]\n",
    "            mean = rewards.mean()\n",
    "            logger.log_reward_losses(rewards, nb_steps)\n",
    "            pbar.set_description(f\"nb steps: {nb_steps}, reward: {mean:.3f}\")\n",
    "            if cfg.save_best and mean > best_reward:\n",
    "                best_reward = mean\n",
    "                best_agent = copy.deepcopy(eval_agent.agent.agents[1])\n",
    "                directory = \"./dqn_critic/\"\n",
    "                if not os.path.exists(directory):\n",
    "                    os.makedirs(directory)\n",
    "                filename = directory + \"dqn0_\" + str(mean.item()) + \".agt\"\n",
    "                eval_agent.save_model(filename)\n",
    "\n",
    "    return best_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a7f1618c1d548aa8174fa8739c87ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing train agent\n",
      "executed train agent at time 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [55]\u001b[0m, in \u001b[0;36m<cell line: 41>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m config\u001b[38;5;241m=\u001b[39mOmegaConf\u001b[38;5;241m.\u001b[39mcreate(params)\n\u001b[0;32m     40\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(config\u001b[38;5;241m.\u001b[39malgorithm\u001b[38;5;241m.\u001b[39mseed)\n\u001b[1;32m---> 41\u001b[0m best_agent \u001b[38;5;241m=\u001b[39m \u001b[43mrun_dqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_critic_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Visualization\u001b[39;00m\n\u001b[0;32m     43\u001b[0m env \u001b[38;5;241m=\u001b[39m make_env(config\u001b[38;5;241m.\u001b[39mgym_env\u001b[38;5;241m.\u001b[39menv_name, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[1;32mIn [42]\u001b[0m, in \u001b[0;36mrun_dqn\u001b[1;34m(cfg, compute_critic_loss)\u001b[0m\n\u001b[0;32m     34\u001b[0m     train_workspace\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     35\u001b[0m     train_workspace\u001b[38;5;241m.\u001b[39mcopy_n_last_steps(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m     \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_workspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstochastic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexecuting train agent\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\titouan\\anaconda3\\lib\\site-packages\\bbrl\\agents\\utils.py:79\u001b[0m, in \u001b[0;36mTemporalAgent.__call__\u001b[1;34m(self, workspace, t, n_steps, stop_variable, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m _t \u001b[38;5;241m=\u001b[39m t\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent(workspace, t\u001b[38;5;241m=\u001b[39m_t, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_variable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     81\u001b[0m         s \u001b[38;5;241m=\u001b[39m workspace\u001b[38;5;241m.\u001b[39mget(stop_variable, _t)\n",
      "File \u001b[1;32mc:\\Users\\titouan\\anaconda3\\lib\\site-packages\\bbrl\\agents\\utils.py:31\u001b[0m, in \u001b[0;36mAgents.__call__\u001b[1;34m(self, workspace, **kwargs)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, workspace, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents:\n\u001b[1;32m---> 31\u001b[0m         a(workspace, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\titouan\\anaconda3\\lib\\site-packages\\bbrl\\agents\\agent.py:75\u001b[0m, in \u001b[0;36mAgent.__call__\u001b[1;34m(self, workspace, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m workspace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Agent.__call__] workspace must not be None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkspace \u001b[38;5;241m=\u001b[39m workspace\n\u001b[1;32m---> 75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkspace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Input \u001b[1;32mIn [54]\u001b[0m, in \u001b[0;36mImageAgent.forward\u001b[1;34m(self, t, **kwargs)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 45\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     48\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\titouan\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\titouan\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#setup_tensorboard('/tblogs') # \"\"\n",
    "params={\n",
    "  \"save_best\": False,\n",
    "  \"logger\":{\n",
    "    \"classname\": \"bbrl.utils.logger.TFLogger\",\n",
    "    \"log_dir\": \"./tblogs/dqn-buffer-\" + str(time.time()),\n",
    "    \"cache_size\": 10000,\n",
    "    \"every_n_seconds\": 10,\n",
    "    \"verbose\": False,    \n",
    "    },\n",
    "\n",
    "  \"algorithm\":{\n",
    "    \"seed\": 4,\n",
    "    \"max_grad_norm\": 0.5,\n",
    "    \"epsilon\": 0.02,\n",
    "    \"n_envs\": 8,\n",
    "    \"n_steps\": 32,\n",
    "    \"n_updates\": 32,\n",
    "    \"eval_interval\": 2000,\n",
    "    \"learning_starts\": 2000,\n",
    "    \"nb_evals\": 10,\n",
    "    \"buffer_size\": 1e6,\n",
    "    \"batch_size\": 256,\n",
    "    \"target_critic_update\": 5000,\n",
    "    \"max_epochs\": 3500,\n",
    "    \"discount_factor\": 0.99,\n",
    "    \"architecture\":{\"hidden_size\": [64, 64]},\n",
    "  },\n",
    "  \"gym_env\":{\n",
    "    \"env_name\": \"CartPole-v1\",\n",
    "  },\n",
    "  \"optimizer\":\n",
    "  {\n",
    "    \"classname\": \"torch.optim.Adam\",\n",
    "    \"lr\": 1e-3,\n",
    "  }\n",
    "}\n",
    "\n",
    "config=OmegaConf.create(params)\n",
    "torch.manual_seed(config.algorithm.seed)\n",
    "best_agent = run_dqn(config, compute_critic_loss)\n",
    "# Visualization\n",
    "env = make_env(config.gym_env.env_name, render_mode=\"rgb_array\")\n",
    "record_video(env, best_agent, \"videos/dqn-full.mp4\")\n",
    "video_display(\"videos/dqn-full.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
