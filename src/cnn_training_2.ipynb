{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F  \n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import gymnasium \n",
    "import mon_env\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "sequence_length = 4  # Number of images in each sequence\n",
    "num_episodes = 4000   # Number of episodes for data collection\n",
    "\n",
    "\n",
    "# Environment Setup\n",
    "env = gymnasium.make('MonCartPole-v1', render_mode = 'rgb_array')\n",
    "data_images = []\n",
    "data_states = []\n",
    "\n",
    "# Transformer les images et les convertir en tenseurs\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((60, 135)),\n",
    "    transforms.Grayscale()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cart location for centering image crop\n",
    "def get_cart_location(screen_width):\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "# Cropping, downsampling (and Grayscaling) image\n",
    "def get_screen():\n",
    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "    screen = env.render().transpose((2, 0, 1))\n",
    "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width * 0.6)\n",
    "    cart_location = get_cart_location(screen_width)\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return transform(screen.transpose(1,2,0)).squeeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\titouan\\anaconda3\\lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.x_threshold to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.x_threshold` for environment variables or `env.get_wrapper_attr('x_threshold')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\titouan\\anaconda3\\lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.state to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.state` for environment variables or `env.get_wrapper_attr('state')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\titouan\\AppData\\Local\\Temp\\ipykernel_9668\\1438530302.py:26: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  data_states = torch.tensor(data_states, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Collecter les données\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    observation = env.reset()[0] \n",
    "    images = [torch.zeros(60, 135) for _ in range(sequence_length)] # Initialisation avec images noires\n",
    "    for t in range(1000):\n",
    "        img = env.render()\n",
    "        tensor_image = get_screen() # shape : 60x135\n",
    "        # if t == 5:\n",
    "        #     fig, axes = plt.subplots(1, 2, figsize=(15, 5))  \n",
    "        #     axes[0].imshow(tensor_image)\n",
    "        \n",
    "        images.append(tensor_image)\n",
    "        sequence_tensor = torch.stack(images[-sequence_length:], dim=0) # shape : 4 x 60 x 135 (si sequence_length = 4)\n",
    "        data_images.append(sequence_tensor)\n",
    "        data_states.append(observation)\n",
    "\n",
    "        action = env.action_space.sample()  \n",
    "        observation, reward, done, info, _ = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Convert data_states to a tensor \n",
    "data_states = torch.tensor(data_states, dtype=torch.float32)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = TensorDataset(torch.stack(data_images), data_states)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Dans le dataloader : \n",
    "# images et labels associés. shape des images : 10 x 4 x 60 x 135\n",
    "#                            shape des labels : 10 x 4 (10 = batch_size, 4 car (x,x_dot,theta,theta_dot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CartPoleCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(sequence_length, 16, kernel_size=5, stride=1, padding=2),  # Input: 4 gray images, output: 16 channels, 60x135\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                 # Output size: 30x67\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                 # Output size: 15x33\n",
    "            # nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            # nn.ReLU(),\n",
    "            # nn.MaxPool2d(kernel_size=2, stride=2)                  # Output size: 7x16\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(15840, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,4)    # x, x_dot, theta, theta_dot\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output for the fully connected layers\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "# Instanciation du modèle\n",
    "model = CartPoleCNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 4]) torch.Size([10, 4, 60, 135])\n",
      "tensor([ 0.0369, -0.2094,  0.0320,  0.2687])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x2160 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for images, labels in dataloader:\n",
    "    print(labels.shape,images.shape) # (batch_size,4), (batch_size,sequence_length,60,135)\n",
    "    fig,axes = plt.subplots(1,sequence_length,figsize = (20,30))\n",
    "    for i in range(sequence_length):\n",
    "        axes[i].imshow(images[0][i])\n",
    "    print(labels[0])\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.026881594210863113, time: 0.09806489944458008\n",
      "Epoch 2, Loss: 0.04815106466412544, time: 0.09857416152954102\n",
      "Epoch 3, Loss: 0.03136144205927849, time: 0.09023237228393555\n",
      "Epoch 4, Loss: 0.06454367935657501, time: 0.07785439491271973\n",
      "Epoch 5, Loss: 0.03731458634138107, time: 0.12148833274841309\n",
      "Epoch 6, Loss: 0.03273656219244003, time: 0.12516570091247559\n",
      "Epoch 7, Loss: 0.02347732149064541, time: 0.08801412582397461\n",
      "Epoch 8, Loss: 0.028955023735761642, time: 0.08293724060058594\n",
      "Epoch 9, Loss: 0.0288323275744915, time: 0.08696961402893066\n",
      "Epoch 10, Loss: 0.031219303607940674, time: 0.08836197853088379\n",
      "Epoch 11, Loss: 0.028614094480872154, time: 0.08151459693908691\n",
      "Epoch 12, Loss: 0.02447572723031044, time: 0.10570263862609863\n",
      "Epoch 13, Loss: 0.032761044800281525, time: 0.09353804588317871\n",
      "Epoch 14, Loss: 0.03336870297789574, time: 0.0966336727142334\n",
      "Epoch 15, Loss: 0.034385472536087036, time: 0.09531807899475098\n",
      "Epoch 16, Loss: 0.03197014331817627, time: 0.09715723991394043\n",
      "Epoch 17, Loss: 0.02379852905869484, time: 0.09839773178100586\n",
      "Epoch 18, Loss: 0.05161699652671814, time: 0.10587358474731445\n",
      "Epoch 19, Loss: 0.025333162397146225, time: 0.10600948333740234\n",
      "Epoch 20, Loss: 0.018843304365873337, time: 0.09763526916503906\n",
      "Epoch 21, Loss: 0.029007423669099808, time: 0.10492348670959473\n",
      "Epoch 22, Loss: 0.028094839304685593, time: 0.10260462760925293\n",
      "Epoch 23, Loss: 0.04056887701153755, time: 0.10168123245239258\n",
      "Epoch 24, Loss: 0.03195895999670029, time: 0.1011049747467041\n",
      "Epoch 25, Loss: 0.022466469556093216, time: 0.10284018516540527\n",
      "Epoch 26, Loss: 0.028069984167814255, time: 0.09915447235107422\n",
      "Epoch 27, Loss: 0.023941844701766968, time: 0.1161038875579834\n",
      "Epoch 28, Loss: 0.01944904401898384, time: 0.09807348251342773\n",
      "Epoch 29, Loss: 0.03506067767739296, time: 0.10327959060668945\n",
      "Epoch 30, Loss: 0.03518560528755188, time: 0.10510396957397461\n",
      "Epoch 31, Loss: 0.025979477912187576, time: 0.09924554824829102\n",
      "Epoch 32, Loss: 0.029042139649391174, time: 0.09457182884216309\n",
      "Epoch 33, Loss: 0.028976697474718094, time: 0.09382319450378418\n",
      "Epoch 34, Loss: 0.022531840950250626, time: 0.09990692138671875\n",
      "Epoch 35, Loss: 0.03179597482085228, time: 0.09910249710083008\n",
      "Epoch 36, Loss: 0.029692601412534714, time: 0.10789132118225098\n",
      "Epoch 37, Loss: 0.025217566639184952, time: 0.10211682319641113\n",
      "Epoch 38, Loss: 0.028477270156145096, time: 0.0995638370513916\n",
      "Epoch 39, Loss: 0.014563488774001598, time: 0.09770631790161133\n",
      "Epoch 40, Loss: 0.03418857604265213, time: 0.09916877746582031\n",
      "Epoch 41, Loss: 0.019332993775606155, time: 0.09960007667541504\n",
      "Epoch 42, Loss: 0.05207578465342522, time: 0.1004791259765625\n",
      "Epoch 43, Loss: 0.024057231843471527, time: 0.10336685180664062\n",
      "Epoch 44, Loss: 0.02360602468252182, time: 0.10005545616149902\n",
      "Epoch 45, Loss: 0.02875896729528904, time: 0.0989532470703125\n",
      "Epoch 46, Loss: 0.028537487611174583, time: 0.09960746765136719\n",
      "Epoch 47, Loss: 0.04334726184606552, time: 0.10162162780761719\n",
      "Epoch 48, Loss: 0.02129921317100525, time: 0.09823775291442871\n",
      "Epoch 49, Loss: 0.026711124926805496, time: 0.09930062294006348\n",
      "Epoch 50, Loss: 0.025635698810219765, time: 0.10223078727722168\n",
      "Epoch 51, Loss: 0.025445014238357544, time: 0.09836816787719727\n",
      "Epoch 52, Loss: 0.014732703566551208, time: 0.10462665557861328\n",
      "Epoch 53, Loss: 0.020877894014120102, time: 0.0992124080657959\n",
      "Epoch 54, Loss: 0.028859052807092667, time: 0.09982562065124512\n",
      "Epoch 55, Loss: 0.029248347505927086, time: 0.09894347190856934\n",
      "Epoch 56, Loss: 0.027886008843779564, time: 0.0934913158416748\n",
      "Epoch 57, Loss: 0.02787867747247219, time: 0.09843635559082031\n",
      "Epoch 58, Loss: 0.023914650082588196, time: 0.09650731086730957\n",
      "Epoch 59, Loss: 0.018187353387475014, time: 0.10008621215820312\n",
      "Epoch 60, Loss: 0.02957175113260746, time: 0.09418940544128418\n",
      "Epoch 61, Loss: 0.026904607191681862, time: 0.09704804420471191\n",
      "Epoch 62, Loss: 0.029881933704018593, time: 0.09895944595336914\n",
      "Epoch 63, Loss: 0.02000412344932556, time: 0.09942746162414551\n",
      "Epoch 64, Loss: 0.032465409487485886, time: 0.09830951690673828\n",
      "Epoch 65, Loss: 0.02488192543387413, time: 0.10245203971862793\n",
      "Epoch 66, Loss: 0.010276093147695065, time: 0.09563565254211426\n",
      "Epoch 67, Loss: 0.03303844481706619, time: 0.10545110702514648\n",
      "Epoch 68, Loss: 0.01917877234518528, time: 0.10134458541870117\n",
      "Epoch 69, Loss: 0.03322339802980423, time: 0.10219645500183105\n",
      "Epoch 70, Loss: 0.021336087957024574, time: 0.09822893142700195\n",
      "Epoch 71, Loss: 0.015587076544761658, time: 0.1047980785369873\n",
      "Epoch 72, Loss: 0.027519529685378075, time: 0.09590768814086914\n",
      "Epoch 73, Loss: 0.02646426483988762, time: 0.0981605052947998\n",
      "Epoch 74, Loss: 0.028498012572526932, time: 0.10081005096435547\n",
      "Epoch 75, Loss: 0.023801583796739578, time: 0.10173320770263672\n",
      "Epoch 76, Loss: 0.016951104626059532, time: 0.09766840934753418\n",
      "Epoch 77, Loss: 0.027060234919190407, time: 0.11701297760009766\n",
      "Epoch 78, Loss: 0.03359611704945564, time: 0.09760141372680664\n",
      "Epoch 79, Loss: 0.031175632029771805, time: 0.10133576393127441\n",
      "Epoch 80, Loss: 0.02631586417555809, time: 0.09966611862182617\n",
      "Epoch 81, Loss: 0.027199026197195053, time: 0.09834408760070801\n",
      "Epoch 82, Loss: 0.021752890199422836, time: 0.10002708435058594\n",
      "Epoch 83, Loss: 0.022329095751047134, time: 0.09320521354675293\n",
      "Epoch 84, Loss: 0.014758618548512459, time: 0.10399699211120605\n",
      "Epoch 85, Loss: 0.02054746448993683, time: 0.09302973747253418\n",
      "Epoch 86, Loss: 0.025802064687013626, time: 0.1039586067199707\n",
      "Epoch 87, Loss: 0.019192922860383987, time: 0.10278797149658203\n",
      "Epoch 88, Loss: 0.009262836538255215, time: 0.10402202606201172\n",
      "Epoch 89, Loss: 0.033407993614673615, time: 0.10076332092285156\n",
      "Epoch 90, Loss: 0.020589929074048996, time: 0.11698627471923828\n",
      "Epoch 91, Loss: 0.014610262587666512, time: 0.10529685020446777\n",
      "Epoch 92, Loss: 0.024214183911681175, time: 0.10428619384765625\n",
      "Epoch 93, Loss: 0.03368968889117241, time: 0.09787464141845703\n",
      "Epoch 94, Loss: 0.02716561034321785, time: 0.09836888313293457\n",
      "Epoch 95, Loss: 0.010104677639901638, time: 0.09766054153442383\n",
      "Epoch 96, Loss: 0.0206857118755579, time: 0.09600424766540527\n",
      "Epoch 97, Loss: 0.019655654206871986, time: 0.09734869003295898\n",
      "Epoch 98, Loss: 0.0206796582788229, time: 0.10000467300415039\n",
      "Epoch 99, Loss: 0.025314131751656532, time: 0.09971952438354492\n",
      "Epoch 100, Loss: 0.01395559124648571, time: 0.10376954078674316\n",
      "Epoch 101, Loss: 0.026662880554795265, time: 0.09676027297973633\n",
      "Epoch 102, Loss: 0.021743644028902054, time: 0.09791922569274902\n",
      "Epoch 103, Loss: 0.0201730914413929, time: 0.09934544563293457\n",
      "Epoch 104, Loss: 0.024126004427671432, time: 0.09738421440124512\n",
      "Epoch 105, Loss: 0.008026963099837303, time: 0.1009669303894043\n",
      "Epoch 106, Loss: 0.0255625881254673, time: 0.10132241249084473\n",
      "Epoch 107, Loss: 0.01987319253385067, time: 0.09824419021606445\n",
      "Epoch 108, Loss: 0.018552888184785843, time: 0.10229754447937012\n",
      "Epoch 109, Loss: 0.029728835448622704, time: 0.09531617164611816\n",
      "Epoch 110, Loss: 0.015959372743964195, time: 0.0850675106048584\n",
      "Epoch 111, Loss: 0.023218097165226936, time: 0.0787973403930664\n",
      "Epoch 112, Loss: 0.018862104043364525, time: 0.07908940315246582\n",
      "Epoch 113, Loss: 0.016374722123146057, time: 0.09356927871704102\n",
      "Epoch 114, Loss: 0.014956529252231121, time: 0.10313820838928223\n",
      "Epoch 115, Loss: 0.02312452159821987, time: 0.10547065734863281\n",
      "Epoch 116, Loss: 0.020247381180524826, time: 0.09670639038085938\n",
      "Epoch 117, Loss: 0.007183633744716644, time: 0.09271574020385742\n",
      "Epoch 118, Loss: 0.021166060119867325, time: 0.09703445434570312\n",
      "Epoch 119, Loss: 0.014874649234116077, time: 0.09411168098449707\n",
      "Epoch 120, Loss: 0.019133197143673897, time: 0.09583425521850586\n",
      "Epoch 121, Loss: 0.015624756924808025, time: 0.10515069961547852\n",
      "Epoch 122, Loss: 0.0200203750282526, time: 0.10656857490539551\n",
      "Epoch 123, Loss: 0.014469591900706291, time: 0.10674548149108887\n",
      "Epoch 124, Loss: 0.02183879353106022, time: 0.10984373092651367\n",
      "Epoch 125, Loss: 0.01788713037967682, time: 0.09715700149536133\n",
      "Epoch 126, Loss: 0.019147608429193497, time: 0.09311795234680176\n",
      "Epoch 127, Loss: 0.025404399260878563, time: 0.09388136863708496\n",
      "Epoch 128, Loss: 0.01195240207016468, time: 0.08709359169006348\n",
      "Epoch 129, Loss: 0.01649956963956356, time: 0.09362030029296875\n",
      "Epoch 130, Loss: 0.011081414297223091, time: 0.10279989242553711\n",
      "Epoch 131, Loss: 0.014362869784235954, time: 0.1037139892578125\n",
      "Epoch 132, Loss: 0.009776261635124683, time: 0.10328006744384766\n",
      "Epoch 133, Loss: 0.016297677531838417, time: 0.10460686683654785\n",
      "Epoch 134, Loss: 0.02300523966550827, time: 0.10544323921203613\n",
      "Epoch 135, Loss: 0.02356863021850586, time: 0.10625839233398438\n",
      "Epoch 136, Loss: 0.04595932364463806, time: 0.09691095352172852\n",
      "Epoch 137, Loss: 0.012541452422738075, time: 0.0948801040649414\n",
      "Epoch 138, Loss: 0.02468750812113285, time: 0.10484123229980469\n",
      "Epoch 139, Loss: 0.01532964687794447, time: 0.09495282173156738\n",
      "Epoch 140, Loss: 0.014156393706798553, time: 0.09158802032470703\n",
      "Epoch 141, Loss: 0.01777629926800728, time: 0.09345650672912598\n",
      "Epoch 142, Loss: 0.014666534960269928, time: 0.09300780296325684\n",
      "Epoch 143, Loss: 0.021307487040758133, time: 0.09570050239562988\n",
      "Epoch 144, Loss: 0.018938252702355385, time: 0.10817694664001465\n",
      "Epoch 145, Loss: 0.015748770907521248, time: 0.09116506576538086\n",
      "Epoch 146, Loss: 0.006123824510723352, time: 0.10601305961608887\n",
      "Epoch 147, Loss: 0.01741849258542061, time: 0.10656142234802246\n",
      "Epoch 148, Loss: 0.016790932044386864, time: 0.10221433639526367\n",
      "Epoch 149, Loss: 0.026167109608650208, time: 0.10880661010742188\n",
      "Epoch 150, Loss: 0.011430730111896992, time: 0.10123443603515625\n",
      "Epoch 151, Loss: 0.018091833218932152, time: 0.11147761344909668\n",
      "Epoch 152, Loss: 0.014099007472395897, time: 0.11013102531433105\n",
      "Epoch 153, Loss: 0.019038444384932518, time: 0.10591411590576172\n",
      "Epoch 154, Loss: 0.03594931215047836, time: 0.10857963562011719\n",
      "Epoch 155, Loss: 0.01628240942955017, time: 0.10097932815551758\n",
      "Epoch 156, Loss: 0.012088491581380367, time: 0.10394430160522461\n",
      "Epoch 157, Loss: 0.011818586848676205, time: 0.11084675788879395\n",
      "Epoch 158, Loss: 0.010228906758129597, time: 0.10637927055358887\n",
      "Epoch 159, Loss: 0.018884945660829544, time: 0.10510635375976562\n",
      "Epoch 160, Loss: 0.016305649653077126, time: 0.10249567031860352\n",
      "Epoch 161, Loss: 0.02109072543680668, time: 0.10182976722717285\n",
      "Epoch 162, Loss: 0.024063577875494957, time: 0.10880708694458008\n",
      "Epoch 163, Loss: 0.012231436558067799, time: 0.10273385047912598\n",
      "Epoch 164, Loss: 0.0071864984929561615, time: 0.11049342155456543\n",
      "Epoch 165, Loss: 0.008972669951617718, time: 0.11242818832397461\n",
      "Epoch 166, Loss: 0.009492072276771069, time: 0.11217999458312988\n",
      "Epoch 167, Loss: 0.01440438162535429, time: 0.10294437408447266\n",
      "Epoch 168, Loss: 0.017035966739058495, time: 0.1054842472076416\n",
      "Epoch 169, Loss: 0.01845303177833557, time: 0.1094505786895752\n",
      "Epoch 170, Loss: 0.014284463599324226, time: 0.10805296897888184\n",
      "Epoch 171, Loss: 0.010738015174865723, time: 0.09392642974853516\n",
      "Epoch 172, Loss: 0.006879011634737253, time: 0.08875012397766113\n",
      "Epoch 173, Loss: 0.00838575791567564, time: 0.08943533897399902\n",
      "Epoch 174, Loss: 0.015471100807189941, time: 0.09322929382324219\n",
      "Epoch 175, Loss: 0.02048390731215477, time: 0.0908658504486084\n",
      "Epoch 176, Loss: 0.02175634540617466, time: 0.10627865791320801\n",
      "Epoch 177, Loss: 0.017101218923926353, time: 0.10968780517578125\n",
      "Epoch 178, Loss: 0.022022167220711708, time: 0.09356546401977539\n",
      "Epoch 179, Loss: 0.015852799639105797, time: 0.09833550453186035\n",
      "Epoch 180, Loss: 0.01752346381545067, time: 0.09384322166442871\n",
      "Epoch 181, Loss: 0.02884785830974579, time: 0.09178924560546875\n",
      "Epoch 182, Loss: 0.025083044543862343, time: 0.09053301811218262\n",
      "Epoch 183, Loss: 0.021092623472213745, time: 0.10379195213317871\n",
      "Epoch 184, Loss: 0.021955925971269608, time: 0.08964014053344727\n",
      "Epoch 185, Loss: 0.013685804791748524, time: 0.08870601654052734\n",
      "Epoch 186, Loss: 0.017399150878190994, time: 0.09089088439941406\n",
      "Epoch 187, Loss: 0.011113164015114307, time: 0.08562183380126953\n",
      "Epoch 188, Loss: 0.014953292906284332, time: 0.08655166625976562\n",
      "Epoch 189, Loss: 0.017155393958091736, time: 0.10433197021484375\n",
      "Epoch 190, Loss: 0.02123910002410412, time: 0.09947991371154785\n",
      "Epoch 191, Loss: 0.017234109342098236, time: 0.10259199142456055\n",
      "Epoch 192, Loss: 0.01499550323933363, time: 0.08956336975097656\n",
      "Epoch 193, Loss: 0.011807561852037907, time: 0.09509778022766113\n",
      "Epoch 194, Loss: 0.007674458436667919, time: 0.09160685539245605\n",
      "Epoch 195, Loss: 0.015325114130973816, time: 0.09148049354553223\n",
      "Epoch 196, Loss: 0.019685160368680954, time: 0.08620643615722656\n",
      "Epoch 197, Loss: 0.019762735813856125, time: 0.10278463363647461\n",
      "Epoch 198, Loss: 0.027115389704704285, time: 0.1034235954284668\n",
      "Epoch 199, Loss: 0.011820490472018719, time: 0.12572383880615234\n",
      "Epoch 200, Loss: 0.005668641068041325, time: 0.1205904483795166\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Training Loop\n",
    "num_epochs = 40 # Peut-être avec plus d'epoch on obtiendrait un meilleur résultat ? jsp\n",
    "# shoudl take about 12 hours :(\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    starttime = time.time()\n",
    "    for images, states in dataloader:\n",
    "        starttime = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, states)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    endtime = time.time()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}, time: {endtime-starttime}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'big_dataset_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0881, -0.2590,  0.0972,  0.6359],\n",
      "        [-0.0092, -0.0292, -0.0508, -0.0893],\n",
      "        [-0.0058, -0.1997,  0.0267,  0.3673],\n",
      "        [ 0.0515, -0.4126,  0.0539,  0.7681],\n",
      "        [ 0.0194,  0.6104, -0.0678, -1.0007],\n",
      "        [-0.1026, -0.6954,  0.1726,  1.4403],\n",
      "        [ 0.0058, -0.3947,  0.0392,  0.6920],\n",
      "        [ 0.0079, -0.0587,  0.0266,  0.0785],\n",
      "        [ 0.0152, -0.0362,  0.0072,  0.1172],\n",
      "        [-0.0662, -0.4603, -0.0057,  0.4803]]) tensor([[-0.0865, -0.3501,  0.0859,  0.7520],\n",
      "        [-0.0227,  0.0160, -0.0768, -0.0807],\n",
      "        [-0.0207, -0.4310,  0.0531,  0.8243],\n",
      "        [ 0.0554, -0.3971,  0.0421,  0.7354],\n",
      "        [ 0.0273,  0.6418, -0.0636, -1.0716],\n",
      "        [-0.0899, -0.6364,  0.2065,  1.4455],\n",
      "        [ 0.0148, -0.3859,  0.0327,  0.6212],\n",
      "        [-0.0036, -0.2315,  0.0305,  0.2541],\n",
      "        [ 0.0217, -0.1544, -0.0178,  0.3034],\n",
      "        [-0.0793, -0.4319,  0.0175,  0.5101]])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "for images, labels in dataloader:\n",
    "    with torch.no_grad():\n",
    "        print(model(images),labels)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voir encore mieux ce que ça donne\n",
    "\n",
    "# 1 : On collecte des images du cartpole (heuristique : random)\n",
    "\n",
    "env = gymnasium.make('CartPole-v1', render_mode = 'rgb_array')\n",
    "data_images_bis = []\n",
    "data_states_bis = []\n",
    "\n",
    "for episode in range(3):\n",
    "    observation_bis = env.reset()[0]\n",
    "    images_bis = []\n",
    "    for t in range(1000):\n",
    "        img = env.render()\n",
    "        tensor_image = transform(img).squeeze(0)  # Transform image immediately\n",
    "        images_bis.append(tensor_image)\n",
    "        \n",
    "        if len(images_bis) >= sequence_length:\n",
    "            # Stack the last sequence_length images to form a single sequence tensor\n",
    "            sequence_tensor = torch.stack(images_bis[-sequence_length:], dim=0)\n",
    "            data_images_bis.append(sequence_tensor)\n",
    "            data_states_bis.append(observation)\n",
    "        \n",
    "        action = env.action_space.sample()   # Use the heuristic policy\n",
    "        observation, reward, done, info, _ = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "data_states_bis = torch.tensor(data_states_bis, dtype=torch.float32)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "data_images_bis = torch.stack(data_images_bis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0085,  0.0607, -0.0027, -0.1543]]) tensor([-0.0224, -0.2072, -0.0182,  0.2929])\n",
      "tensor([[ 0.0134,  0.0207, -0.0058, -0.1028]]) tensor([-0.0266, -0.4020, -0.0123,  0.5798])\n",
      "tensor([[ 0.0163, -0.0432, -0.0074, -0.0155]]) tensor([-3.4596e-02, -5.9696e-01, -7.3849e-04,  8.6859e-01])\n",
      "tensor([[ 0.0086, -0.0660, -0.0017,  0.0230]]) tensor([-0.0465, -0.7921,  0.0166,  1.1610])\n",
      "tensor([[ 0.0053, -0.1836, -0.0024,  0.1661]]) tensor([-0.0624, -0.5972,  0.0399,  0.8736])\n",
      "tensor([[ 1.0203e-04, -1.2910e-01,  3.0604e-03,  8.9000e-02]]) tensor([-0.0743, -0.4026,  0.0573,  0.5937])\n",
      "tensor([[-0.0046, -0.0542,  0.0065, -0.0080]]) tensor([-0.0824, -0.5985,  0.0692,  0.9039])\n",
      "tensor([[-0.0097, -0.0971,  0.0050,  0.0420]]) tensor([-0.0943, -0.7945,  0.0873,  1.2175])\n",
      "tensor([[-0.0119, -0.1093,  0.0027,  0.0501]]) tensor([-0.1102, -0.6006,  0.1116,  0.9534])\n",
      "tensor([[-0.0146, -0.0981,  0.0026,  0.0321]]) tensor([-0.1222, -0.4071,  0.1307,  0.6978])\n",
      "tensor([[-0.0183,  0.0289,  0.0008, -0.1397]]) tensor([-0.1304, -0.2140,  0.1447,  0.4489])\n",
      "tensor([[-0.0244,  0.0794,  0.0037, -0.1921]]) tensor([-0.1347, -0.0212,  0.1536,  0.2051])\n",
      "tensor([[-0.0268,  0.1291,  0.0110, -0.2474]]) tensor([-0.1351, -0.2182,  0.1577,  0.5420])\n",
      "tensor([[-0.0287,  0.1212,  0.0044, -0.2529]]) tensor([-0.1395, -0.4151,  0.1686,  0.8800])\n",
      "tensor([[-0.0308,  0.0665,  0.0032, -0.1680]]) tensor([-0.1478, -0.2226,  0.1862,  0.6447])\n",
      "tensor([[-0.0322,  0.0361,  0.0078, -0.1161]]) tensor([-0.1522, -0.4198,  0.1991,  0.9897])\n",
      "tensor([[ 0.0064, -0.0603, -0.0005,  0.0156]]) tensor([-0.0445, -0.2333,  0.0134,  0.3152])\n",
      "tensor([[ 0.0080, -0.1057, -0.0027,  0.0740]]) tensor([-0.0491, -0.0383,  0.0197,  0.0268])\n",
      "tensor([[ 0.0093,  0.0041, -0.0023, -0.0764]]) tensor([-0.0499, -0.2337,  0.0203,  0.3257])\n",
      "tensor([[ 0.0122,  0.0150, -0.0020, -0.0901]]) tensor([-0.0546, -0.4292,  0.0268,  0.6247])\n",
      "tensor([[ 0.0089, -0.0990, -0.0034,  0.0510]]) tensor([-0.0631, -0.6246,  0.0393,  0.9257])\n",
      "tensor([[ 0.0015, -0.1141,  0.0025,  0.0666]]) tensor([-0.0756, -0.8203,  0.0578,  1.2304])\n",
      "tensor([[-0.0104, -0.1059,  0.0072,  0.0517]]) tensor([-0.0920, -0.6259,  0.0824,  0.9564])\n",
      "tensor([[-0.0152, -0.0660,  0.0044, -0.0176]]) tensor([-0.1046, -0.4320,  0.1015,  0.6907])\n",
      "tensor([[-0.0105, -0.0624, -0.0004, -0.0138]]) tensor([-0.1132, -0.2384,  0.1153,  0.4316])\n",
      "tensor([[-0.0168, -0.0141,  0.0058, -0.0588]]) tensor([-0.1180, -0.4350,  0.1240,  0.7583])\n",
      "tensor([[-0.0235, -0.0087, -0.0027, -0.0918]]) tensor([-0.1267, -0.2418,  0.1391,  0.5070])\n",
      "tensor([[-0.0324,  0.0255,  0.0005, -0.1326]]) tensor([-0.1315, -0.0489,  0.1493,  0.2612])\n",
      "tensor([[-0.0231,  0.0843,  0.0034, -0.1939]]) tensor([-0.1325, -0.2458,  0.1545,  0.5970])\n",
      "tensor([[-0.0302,  0.0929,  0.0036, -0.2124]]) tensor([-0.1374, -0.0531,  0.1664,  0.3567])\n",
      "tensor([[-0.0326,  0.1464,  0.0093, -0.2641]]) tensor([-0.1385,  0.1393,  0.1736,  0.1208])\n",
      "tensor([[-0.0431,  0.1982,  0.0309, -0.3045]]) tensor([-0.1357, -0.0578,  0.1760,  0.4628])\n",
      "tensor([[-0.0254,  0.1073,  0.0144, -0.2087]]) tensor([-0.1368, -0.2549,  0.1852,  0.8054])\n",
      "tensor([[-0.0282,  0.1148,  0.0198, -0.1996]]) tensor([-0.1419, -0.0628,  0.2013,  0.5762])\n",
      "tensor([[ 0.0218,  0.0971, -0.0123, -0.2062]]) tensor([ 0.0157, -0.1876,  0.0273,  0.3640])\n",
      "tensor([[ 0.0218,  0.0971, -0.0123, -0.2062]]) tensor([ 0.0120, -0.3831,  0.0346,  0.6652])\n",
      "tensor([[ 0.0179,  0.0155, -0.0079, -0.0948]]) tensor([ 0.0043, -0.1884,  0.0479,  0.3836])\n",
      "tensor([[ 0.0196,  0.1302, -0.0110, -0.2516]]) tensor([0.0005, 0.0060, 0.0555, 0.1064])\n",
      "tensor([[ 0.0202,  0.1504, -0.0096, -0.2735]]) tensor([ 0.0007,  0.2003,  0.0577, -0.1683])\n",
      "tensor([[ 0.0201,  0.1517, -0.0092, -0.2681]]) tensor([ 0.0047,  0.3945,  0.0543, -0.4422])\n",
      "tensor([[ 0.0197,  0.2390, -0.0098, -0.3816]]) tensor([ 0.0126,  0.1987,  0.0455, -0.1329])\n",
      "tensor([[ 0.0183,  0.1874, -0.0104, -0.3169]]) tensor([0.0165, 0.0029, 0.0428, 0.1737])\n",
      "tensor([[ 0.0245,  0.0443, -0.0116, -0.1307]]) tensor([ 0.0166,  0.1974,  0.0463, -0.1051])\n",
      "tensor([[ 0.0226,  0.0426, -0.0088, -0.1244]]) tensor([ 0.0205,  0.3918,  0.0442, -0.3829])\n",
      "tensor([[ 0.0204,  0.0919, -0.0055, -0.1795]]) tensor([ 0.0284,  0.1961,  0.0365, -0.0766])\n",
      "tensor([[ 0.0212,  0.0888, -0.0043, -0.1749]]) tensor([ 0.0323,  0.3907,  0.0350, -0.3575])\n",
      "tensor([[ 0.0265,  0.0826, -0.0051, -0.1625]]) tensor([ 0.0401,  0.1951,  0.0278, -0.0540])\n",
      "tensor([[ 0.0237, -0.0387, -0.0033,  0.0003]]) tensor([ 0.0440,  0.3898,  0.0268, -0.3378])\n",
      "tensor([[ 0.0226,  0.0417, -0.0035, -0.1061]]) tensor([ 0.0518,  0.5845,  0.0200, -0.6219])\n",
      "tensor([[ 0.0217,  0.0692, -0.0057, -0.1464]]) tensor([ 0.0635,  0.7794,  0.0076, -0.9082])\n",
      "tensor([[ 0.0199,  0.1772, -0.0068, -0.2921]]) tensor([ 0.0791,  0.5841, -0.0106, -0.6132])\n",
      "tensor([[ 0.0169,  0.1516, -0.0089, -0.2662]]) tensor([ 0.0908,  0.3892, -0.0229, -0.3239])\n",
      "tensor([[ 0.0158,  0.1128, -0.0166, -0.2291]]) tensor([ 0.0986,  0.1944, -0.0293, -0.0385])\n",
      "tensor([[ 0.0178,  0.0518, -0.0194, -0.1513]]) tensor([ 0.1024,  0.3899, -0.0301, -0.3403])\n",
      "tensor([[ 0.0202,  0.0718, -0.0190, -0.1730]]) tensor([ 0.1102,  0.1952, -0.0369, -0.0573])\n",
      "tensor([[ 0.0228,  0.0734, -0.0213, -0.1767]]) tensor([ 0.1141,  0.0007, -0.0381,  0.2236])\n",
      "tensor([[ 0.0210,  0.0102, -0.0177, -0.0871]]) tensor([ 0.1142,  0.1963, -0.0336, -0.0809])\n",
      "tensor([[ 0.0225, -0.0069, -0.0192, -0.0659]]) tensor([ 0.1181,  0.3919, -0.0352, -0.3840])\n",
      "tensor([[ 0.0261,  0.0798, -0.0207, -0.1794]]) tensor([ 0.1259,  0.1973, -0.0429, -0.1026])\n",
      "tensor([[ 0.0333,  0.1362, -0.0296, -0.2623]]) tensor([ 0.1299,  0.3930, -0.0449, -0.4085])\n",
      "tensor([[ 0.0379,  0.1643, -0.0340, -0.3076]]) tensor([ 0.1377,  0.1985, -0.0531, -0.1303])\n",
      "tensor([[ 0.0308,  0.0409, -0.0224, -0.1324]]) tensor([ 0.1417,  0.3944, -0.0557, -0.4393])\n",
      "tensor([[ 0.0401,  0.1136, -0.0334, -0.2372]]) tensor([ 0.1496,  0.5902, -0.0645, -0.7490])\n",
      "tensor([[ 0.0359,  0.1562, -0.0259, -0.2886]]) tensor([ 0.1614,  0.3961, -0.0795, -0.4773])\n",
      "tensor([[ 0.0309,  0.0522, -0.0230, -0.1474]]) tensor([ 0.1693,  0.2022, -0.0890, -0.2107])\n",
      "tensor([[ 0.0333, -0.0944, -0.0244,  0.0554]]) tensor([ 0.1734,  0.3984, -0.0932, -0.5301])\n",
      "tensor([[ 0.0238, -0.0581, -0.0152,  0.0044]]) tensor([ 0.1813,  0.2047, -0.1039, -0.2682])\n",
      "tensor([[ 0.0244, -0.0322, -0.0155, -0.0374]]) tensor([ 0.1854,  0.4012, -0.1092, -0.5917])\n",
      "tensor([[ 0.0227,  0.0340, -0.0196, -0.1397]]) tensor([ 0.1934,  0.5976, -0.1210, -0.9167])\n",
      "tensor([[ 0.0233,  0.0586, -0.0209, -0.1778]]) tensor([ 0.2054,  0.4043, -0.1394, -0.6644])\n",
      "tensor([[ 0.0339,  0.0647, -0.0295, -0.1835]]) tensor([ 0.2135,  0.6011, -0.1527, -0.9975])\n",
      "tensor([[ 0.0192,  0.1068, -0.0199, -0.2392]]) tensor([ 0.2255,  0.4083, -0.1726, -0.7564])\n",
      "tensor([[ 0.0461,  0.0685, -0.0363, -0.1775]]) tensor([ 0.2337,  0.2159, -0.1877, -0.5226])\n",
      "tensor([[ 0.0484, -0.0567, -0.0334,  0.0092]]) tensor([ 0.2380,  0.0239, -0.1982, -0.2945])\n",
      "tensor([[ 0.0465, -0.0916, -0.0344,  0.0555]]) tensor([ 0.2385,  0.2212, -0.2041, -0.6425])\n",
      "38.08352326601744\n"
     ]
    }
   ],
   "source": [
    "# 2 Afficher ce que prédit le modèle vs les vraies observations\n",
    "\n",
    "model.eval()\n",
    "total = 0 # Loss totale\n",
    "for images, states in zip(data_images_bis, data_states_bis):\n",
    "    with torch.no_grad():\n",
    "        print(model(images.unsqueeze(0)),states)\n",
    "        total += np.sum(np.array((model(images.unsqueeze(0))-states)**2))\n",
    "\n",
    "print(total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
