{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F  \n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import gymnasium \n",
    "import mon_env\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "sequence_length = 4  # Number of images in each sequence\n",
    "num_episodes = 3000   # Number of episodes for data collection\n",
    "\n",
    "\n",
    "# Environment Setup\n",
    "env = gymnasium.make('MonCartPole-v1', render_mode = 'rgb_array')\n",
    "data_images = []\n",
    "data_states = []\n",
    "\n",
    "# Transformer les images et les convertir en tenseurs\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((60, 135)),\n",
    "    transforms.Grayscale()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cart location for centering image crop\n",
    "def get_cart_location(screen_width):\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "# Cropping, downsampling (and Grayscaling) image\n",
    "def get_screen():\n",
    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "    screen = env.render().transpose((2, 0, 1))\n",
    "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width * 0.6)\n",
    "    cart_location = get_cart_location(screen_width)\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return transform(screen.transpose(1,2,0)).squeeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hatem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.x_threshold to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.x_threshold` for environment variables or `env.get_wrapper_attr('x_threshold')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\hatem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.state to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.state` for environment variables or `env.get_wrapper_attr('state')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\hatem\\AppData\\Local\\Temp\\ipykernel_12676\\1438530302.py:26: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  data_states = torch.tensor(data_states, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Collecter les données\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    observation = env.reset()[0] \n",
    "    images = [torch.zeros(60, 135) for _ in range(sequence_length)] # Initialisation avec images noires\n",
    "    for t in range(1000):\n",
    "        img = env.render()\n",
    "        tensor_image = get_screen() # shape : 60x135\n",
    "        # if t == 5:\n",
    "        #     fig, axes = plt.subplots(1, 2, figsize=(15, 5))  \n",
    "        #     axes[0].imshow(tensor_image)\n",
    "        \n",
    "        images.append(tensor_image)\n",
    "        sequence_tensor = torch.stack(images[-sequence_length:], dim=0) # shape : 4 x 60 x 135 (si sequence_length = 4)\n",
    "        data_images.append(sequence_tensor)\n",
    "        data_states.append(observation)\n",
    "\n",
    "        action = env.action_space.sample()  \n",
    "        observation, reward, done, info, _ = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Convert data_states to a tensor \n",
    "data_states = torch.tensor(data_states, dtype=torch.float32)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = TensorDataset(torch.stack(data_images), data_states)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Dans le dataloader : \n",
    "# images et labels associés. shape des images : 10 x 4 x 60 x 135\n",
    "#                            shape des labels : 10 x 4 (10 = batch_size, 4 car (x,x_dot,theta,theta_dot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CartPoleCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(sequence_length, 16, kernel_size=5, stride=1, padding=2),  # Input: 4 gray images, output: 16 channels, 60x135\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                 # Output size: 30x67\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                 # Output size: 15x33\n",
    "            # nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            # nn.ReLU(),\n",
    "            # nn.MaxPool2d(kernel_size=2, stride=2)                  # Output size: 7x16\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(15840, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,4)    # x, x_dot, theta, theta_dot\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output for the fully connected layers\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "# Instanciation du modèle\n",
    "model = CartPoleCNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 4]) torch.Size([10, 4, 60, 135])\n",
      "tensor([ 0.0986,  0.5630,  0.0026, -0.6533])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x3000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for images, labels in dataloader:\n",
    "    print(labels.shape,images.shape) # (batch_size,4), (batch_size,sequence_length,60,135)\n",
    "    fig,axes = plt.subplots(1,sequence_length,figsize = (20,30))\n",
    "    for i in range(sequence_length):\n",
    "        axes[i].imshow(images[0][i])\n",
    "    print(labels[0])\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.11199275404214859\n",
      "Epoch 2, Loss: 0.06364326179027557\n",
      "Epoch 3, Loss: 0.020071031525731087\n",
      "Epoch 4, Loss: 0.05183423310518265\n",
      "Epoch 5, Loss: 0.03407881408929825\n",
      "Epoch 6, Loss: 0.031121596693992615\n",
      "Epoch 7, Loss: 0.016564620658755302\n",
      "Epoch 8, Loss: 0.052262041717767715\n",
      "Epoch 9, Loss: 0.016239451244473457\n",
      "Epoch 10, Loss: 0.0683538094162941\n",
      "Epoch 11, Loss: 0.03447888046503067\n",
      "Epoch 12, Loss: 0.04580215364694595\n",
      "Epoch 13, Loss: 0.01716875471174717\n",
      "Epoch 14, Loss: 0.01932472176849842\n",
      "Epoch 15, Loss: 0.01234764326363802\n",
      "Epoch 16, Loss: 0.038950901478528976\n",
      "Epoch 17, Loss: 0.02717047929763794\n",
      "Epoch 18, Loss: 0.02766195312142372\n",
      "Epoch 19, Loss: 0.033405084162950516\n",
      "Epoch 20, Loss: 0.02635957859456539\n",
      "Epoch 21, Loss: 0.03088177926838398\n",
      "Epoch 22, Loss: 0.03457565978169441\n",
      "Epoch 23, Loss: 0.019625620916485786\n",
      "Epoch 24, Loss: 0.018856866285204887\n",
      "Epoch 25, Loss: 0.017579110339283943\n",
      "Epoch 26, Loss: 0.01820266619324684\n",
      "Epoch 27, Loss: 0.04282165691256523\n",
      "Epoch 28, Loss: 0.007989970967173576\n",
      "Epoch 29, Loss: 0.01860274188220501\n",
      "Epoch 30, Loss: 0.025050988420844078\n",
      "Epoch 31, Loss: 0.045386962592601776\n",
      "Epoch 32, Loss: 0.014032185077667236\n",
      "Epoch 33, Loss: 0.006699003279209137\n",
      "Epoch 34, Loss: 0.0106158796697855\n",
      "Epoch 35, Loss: 0.01843898370862007\n",
      "Epoch 36, Loss: 0.012420756742358208\n",
      "Epoch 37, Loss: 0.010194031521677971\n",
      "Epoch 38, Loss: 0.0024886038154363632\n",
      "Epoch 39, Loss: 0.006389289163053036\n",
      "Epoch 40, Loss: 0.00919934082776308\n",
      "Epoch 41, Loss: 0.007140373345464468\n",
      "Epoch 42, Loss: 0.016367310658097267\n",
      "Epoch 43, Loss: 0.007121379021555185\n",
      "Epoch 44, Loss: 0.010786822997033596\n",
      "Epoch 45, Loss: 0.013575938530266285\n",
      "Epoch 46, Loss: 0.0038690795190632343\n",
      "Epoch 47, Loss: 0.006766241509467363\n",
      "Epoch 48, Loss: 0.005405500065535307\n",
      "Epoch 49, Loss: 0.005213686265051365\n",
      "Epoch 50, Loss: 0.010371118783950806\n",
      "Epoch 51, Loss: 0.0023075744975358248\n",
      "Epoch 52, Loss: 0.004524501506239176\n",
      "Epoch 53, Loss: 0.01009492389857769\n",
      "Epoch 54, Loss: 0.01075915340334177\n",
      "Epoch 55, Loss: 0.01397847943007946\n",
      "Epoch 56, Loss: 0.0023042543325573206\n",
      "Epoch 57, Loss: 0.0010407960508018732\n",
      "Epoch 58, Loss: 0.004927936475723982\n",
      "Epoch 59, Loss: 0.002116433810442686\n",
      "Epoch 60, Loss: 0.00793380755931139\n",
      "Epoch 61, Loss: 0.0037351904902607203\n",
      "Epoch 62, Loss: 0.0011267156805843115\n",
      "Epoch 63, Loss: 0.004172349814325571\n",
      "Epoch 64, Loss: 0.004304158501327038\n",
      "Epoch 65, Loss: 0.0018144731875509024\n",
      "Epoch 66, Loss: 0.00500254612416029\n",
      "Epoch 67, Loss: 0.004338508006185293\n",
      "Epoch 68, Loss: 0.005174074787646532\n",
      "Epoch 69, Loss: 0.007137942127883434\n",
      "Epoch 70, Loss: 0.003859094111248851\n",
      "Epoch 71, Loss: 0.003210228169336915\n",
      "Epoch 72, Loss: 0.001078191096894443\n",
      "Epoch 73, Loss: 0.004274413455277681\n",
      "Epoch 74, Loss: 0.003966982942074537\n",
      "Epoch 75, Loss: 0.004194244276732206\n",
      "Epoch 76, Loss: 0.006483942270278931\n",
      "Epoch 77, Loss: 0.0013533374294638634\n",
      "Epoch 78, Loss: 0.005177523009479046\n",
      "Epoch 79, Loss: 0.0014197876444086432\n",
      "Epoch 80, Loss: 0.002099372912198305\n",
      "Epoch 81, Loss: 0.00028906259103678167\n",
      "Epoch 82, Loss: 0.0040017394348979\n",
      "Epoch 83, Loss: 0.0026478408835828304\n",
      "Epoch 84, Loss: 0.0024121880996972322\n",
      "Epoch 85, Loss: 0.007532772608101368\n",
      "Epoch 86, Loss: 0.002157636685296893\n",
      "Epoch 87, Loss: 0.0016879510367289186\n",
      "Epoch 88, Loss: 0.003041837364435196\n",
      "Epoch 89, Loss: 0.0030350876040756702\n",
      "Epoch 90, Loss: 0.009276138618588448\n",
      "Epoch 91, Loss: 0.0036750826984643936\n",
      "Epoch 92, Loss: 0.0033771765884011984\n",
      "Epoch 93, Loss: 0.0027677693869918585\n",
      "Epoch 94, Loss: 0.002065519569441676\n",
      "Epoch 95, Loss: 0.003268398344516754\n",
      "Epoch 96, Loss: 0.0029931317549198866\n",
      "Epoch 97, Loss: 0.0030143377371132374\n",
      "Epoch 98, Loss: 0.011383282020688057\n",
      "Epoch 99, Loss: 0.0033635038416832685\n",
      "Epoch 100, Loss: 0.0007041953504085541\n",
      "Epoch 101, Loss: 0.0012289144797250628\n",
      "Epoch 102, Loss: 0.0012108903611078858\n",
      "Epoch 103, Loss: 0.0019397701835259795\n",
      "Epoch 104, Loss: 0.0012857476249337196\n",
      "Epoch 105, Loss: 0.001180697581730783\n",
      "Epoch 106, Loss: 0.0010774474358186126\n",
      "Epoch 107, Loss: 0.0008229846716858447\n",
      "Epoch 108, Loss: 0.0023576172534376383\n",
      "Epoch 109, Loss: 0.0018412891076877713\n",
      "Epoch 110, Loss: 0.0018415860831737518\n",
      "Epoch 111, Loss: 0.0010302484733983874\n",
      "Epoch 112, Loss: 0.000708362611476332\n",
      "Epoch 113, Loss: 0.000617582758422941\n",
      "Epoch 114, Loss: 0.0006209379644133151\n",
      "Epoch 115, Loss: 0.000597254722379148\n",
      "Epoch 116, Loss: 0.0006274382467381656\n",
      "Epoch 117, Loss: 0.0007877264288254082\n",
      "Epoch 118, Loss: 0.0010224858997389674\n",
      "Epoch 119, Loss: 0.0018981372704729438\n",
      "Epoch 120, Loss: 0.0033005077857524157\n",
      "Epoch 121, Loss: 0.007929272018373013\n",
      "Epoch 122, Loss: 0.002845711773261428\n",
      "Epoch 123, Loss: 0.0043730102479457855\n",
      "Epoch 124, Loss: 0.003940812777727842\n",
      "Epoch 125, Loss: 0.001603121985681355\n",
      "Epoch 126, Loss: 0.0037149882409721613\n",
      "Epoch 127, Loss: 0.002169450744986534\n",
      "Epoch 128, Loss: 0.0022747355978935957\n",
      "Epoch 129, Loss: 0.0016460123006254435\n",
      "Epoch 130, Loss: 0.001869379193522036\n",
      "Epoch 131, Loss: 0.001812393544241786\n",
      "Epoch 132, Loss: 0.0016221291152760386\n",
      "Epoch 133, Loss: 0.002037512604147196\n",
      "Epoch 134, Loss: 0.0006337441736832261\n",
      "Epoch 135, Loss: 0.0017459919909015298\n",
      "Epoch 136, Loss: 0.0006463947356678545\n",
      "Epoch 137, Loss: 0.0008567545446567237\n",
      "Epoch 138, Loss: 0.00038483794196508825\n",
      "Epoch 139, Loss: 0.0008092732750810683\n",
      "Epoch 140, Loss: 0.0018768247682601213\n",
      "Epoch 141, Loss: 0.00044569626334123313\n",
      "Epoch 142, Loss: 0.0004949034191668034\n",
      "Epoch 143, Loss: 0.0006000325083732605\n",
      "Epoch 144, Loss: 0.0003066958743147552\n",
      "Epoch 145, Loss: 0.0010892763966694474\n",
      "Epoch 146, Loss: 0.0015101246535778046\n",
      "Epoch 147, Loss: 0.0007405153592117131\n",
      "Epoch 148, Loss: 0.00216695386916399\n",
      "Epoch 149, Loss: 0.0017605690518394113\n",
      "Epoch 150, Loss: 0.0019574821926653385\n",
      "Epoch 151, Loss: 0.0013476710300892591\n",
      "Epoch 152, Loss: 0.0039639039896428585\n",
      "Epoch 153, Loss: 0.002023933222517371\n",
      "Epoch 154, Loss: 0.002128939377143979\n",
      "Epoch 155, Loss: 0.005033215973526239\n",
      "Epoch 156, Loss: 0.0036322749219834805\n",
      "Epoch 157, Loss: 0.003975582774728537\n",
      "Epoch 158, Loss: 0.0008021519752219319\n",
      "Epoch 159, Loss: 0.0005533733638003469\n",
      "Epoch 160, Loss: 0.0008864469127729535\n",
      "Epoch 161, Loss: 0.0008775860769674182\n",
      "Epoch 162, Loss: 0.0006211733561940491\n",
      "Epoch 163, Loss: 0.0007665640441700816\n",
      "Epoch 164, Loss: 0.00040560445631854236\n",
      "Epoch 165, Loss: 0.00015124710625968874\n",
      "Epoch 166, Loss: 0.00024293137539643794\n",
      "Epoch 167, Loss: 0.0001748698705341667\n",
      "Epoch 168, Loss: 0.0004825117066502571\n",
      "Epoch 169, Loss: 0.00021381549595389515\n",
      "Epoch 170, Loss: 0.00028996836044825613\n",
      "Epoch 171, Loss: 0.00022850297682452947\n",
      "Epoch 172, Loss: 0.001328814192675054\n",
      "Epoch 173, Loss: 0.00039529468631371856\n",
      "Epoch 174, Loss: 0.0015876722754910588\n",
      "Epoch 175, Loss: 0.00029457485652528703\n",
      "Epoch 176, Loss: 0.0020643621683120728\n",
      "Epoch 177, Loss: 0.0032662765588611364\n",
      "Epoch 178, Loss: 0.00216929754242301\n",
      "Epoch 179, Loss: 0.004933380521833897\n",
      "Epoch 180, Loss: 0.0027035654056817293\n",
      "Epoch 181, Loss: 0.0068738823756575584\n",
      "Epoch 182, Loss: 0.0007257518009282649\n",
      "Epoch 183, Loss: 0.000980615266598761\n",
      "Epoch 184, Loss: 0.0006646596593782306\n",
      "Epoch 185, Loss: 0.0003724155540112406\n",
      "Epoch 186, Loss: 0.00030410176259465516\n",
      "Epoch 187, Loss: 0.0002093332150252536\n",
      "Epoch 188, Loss: 0.0002468666934873909\n",
      "Epoch 189, Loss: 0.00028176119667477906\n",
      "Epoch 190, Loss: 0.0003175023302901536\n",
      "Epoch 191, Loss: 0.00012388787581585348\n",
      "Epoch 192, Loss: 0.00017501460388302803\n",
      "Epoch 193, Loss: 6.538503657793626e-05\n",
      "Epoch 194, Loss: 0.00010451918933540583\n",
      "Epoch 195, Loss: 0.000282422115560621\n",
      "Epoch 196, Loss: 7.428605749737471e-05\n",
      "Epoch 197, Loss: 0.0005011765169911087\n",
      "Epoch 198, Loss: 0.0002800341753754765\n",
      "Epoch 199, Loss: 0.00042649151873774827\n",
      "Epoch 200, Loss: 0.000617948651779443\n",
      "Epoch 201, Loss: 0.0006599024636670947\n",
      "Epoch 202, Loss: 0.002397175645455718\n",
      "Epoch 203, Loss: 0.00241386448033154\n",
      "Epoch 204, Loss: 0.002517885295674205\n",
      "Epoch 205, Loss: 0.001160198007710278\n",
      "Epoch 206, Loss: 0.0003447989874985069\n",
      "Epoch 207, Loss: 0.0022169803269207478\n",
      "Epoch 208, Loss: 0.0031979449559003115\n",
      "Epoch 209, Loss: 0.0009991469560191035\n",
      "Epoch 210, Loss: 0.001243166858330369\n",
      "Epoch 211, Loss: 0.0009815095691010356\n",
      "Epoch 212, Loss: 0.0007348462240770459\n",
      "Epoch 213, Loss: 0.0002501396811567247\n",
      "Epoch 214, Loss: 0.0003748015151359141\n",
      "Epoch 215, Loss: 0.000791371741797775\n",
      "Epoch 216, Loss: 0.0006939739687368274\n",
      "Epoch 217, Loss: 0.00013291178038343787\n",
      "Epoch 218, Loss: 0.0002925127337221056\n",
      "Epoch 219, Loss: 0.0007652028580196202\n",
      "Epoch 220, Loss: 0.0006993948481976986\n",
      "Epoch 221, Loss: 0.0003219787613488734\n",
      "Epoch 222, Loss: 0.0003912980610039085\n",
      "Epoch 223, Loss: 0.0010291788494214416\n",
      "Epoch 224, Loss: 0.0018590245163068175\n",
      "Epoch 225, Loss: 0.002004116540774703\n",
      "Epoch 226, Loss: 0.0005970039637759328\n",
      "Epoch 227, Loss: 0.0008648111252114177\n",
      "Epoch 228, Loss: 0.002625169465318322\n",
      "Epoch 229, Loss: 0.0010537522612139583\n",
      "Epoch 230, Loss: 0.0007049384876154363\n",
      "Epoch 231, Loss: 0.0012345740105956793\n",
      "Epoch 232, Loss: 0.0007048701518215239\n",
      "Epoch 233, Loss: 0.0007430611876770854\n",
      "Epoch 234, Loss: 0.0009744183043949306\n",
      "Epoch 235, Loss: 0.00046861395821906626\n",
      "Epoch 236, Loss: 0.0034295967780053616\n",
      "Epoch 237, Loss: 0.0010666208108887076\n",
      "Epoch 238, Loss: 0.003254498587921262\n",
      "Epoch 239, Loss: 0.002036702586337924\n",
      "Epoch 240, Loss: 0.000724618905223906\n",
      "Epoch 241, Loss: 0.0009456156985834241\n",
      "Epoch 242, Loss: 0.0006477006827481091\n",
      "Epoch 243, Loss: 0.0002443780249450356\n",
      "Epoch 244, Loss: 0.0006679080543108284\n",
      "Epoch 245, Loss: 0.0005509479087777436\n",
      "Epoch 246, Loss: 0.0013691546628251672\n",
      "Epoch 247, Loss: 0.0011166471522301435\n",
      "Epoch 248, Loss: 0.0007870425470173359\n",
      "Epoch 249, Loss: 0.0005791072617284954\n",
      "Epoch 250, Loss: 0.0009502157336100936\n",
      "Epoch 251, Loss: 0.0029783297795802355\n",
      "Epoch 252, Loss: 0.0013024366926401854\n",
      "Epoch 253, Loss: 0.0005301283672451973\n",
      "Epoch 254, Loss: 0.0004399285826366395\n",
      "Epoch 255, Loss: 0.0014188200002536178\n",
      "Epoch 256, Loss: 0.00048715932643972337\n",
      "Epoch 257, Loss: 0.0006648977287113667\n",
      "Epoch 258, Loss: 0.0004942011437378824\n",
      "Epoch 259, Loss: 0.0006683254614472389\n",
      "Epoch 260, Loss: 0.00038625902379862964\n",
      "Epoch 261, Loss: 0.0005388433346524835\n",
      "Epoch 262, Loss: 0.0010369477095082402\n",
      "Epoch 263, Loss: 0.0010282762814313173\n",
      "Epoch 264, Loss: 0.0002931556082330644\n",
      "Epoch 265, Loss: 0.0007600082317367196\n",
      "Epoch 266, Loss: 0.0003403386508580297\n",
      "Epoch 267, Loss: 0.0009728192235343158\n",
      "Epoch 268, Loss: 0.0011759981280192733\n",
      "Epoch 269, Loss: 0.0033237349707633257\n",
      "Epoch 270, Loss: 0.004154428374022245\n",
      "Epoch 271, Loss: 0.0013908551773056388\n",
      "Epoch 272, Loss: 0.0007062536315061152\n",
      "Epoch 273, Loss: 0.000615516270045191\n",
      "Epoch 274, Loss: 0.00029881150112487376\n",
      "Epoch 275, Loss: 0.0009740815730765462\n",
      "Epoch 276, Loss: 0.0003285372513346374\n",
      "Epoch 277, Loss: 0.0002794180472847074\n",
      "Epoch 278, Loss: 0.00012834600056521595\n",
      "Epoch 279, Loss: 0.00012457779666874558\n",
      "Epoch 280, Loss: 0.0004302304587326944\n",
      "Epoch 281, Loss: 0.0004913117154501379\n",
      "Epoch 282, Loss: 0.0006903440807946026\n",
      "Epoch 283, Loss: 0.0004973915056325495\n",
      "Epoch 284, Loss: 0.00047400270705111325\n",
      "Epoch 285, Loss: 0.0001360630994895473\n",
      "Epoch 286, Loss: 0.00048163076280616224\n",
      "Epoch 287, Loss: 0.0003051395178772509\n",
      "Epoch 288, Loss: 0.00023284177586901933\n",
      "Epoch 289, Loss: 0.0015558855375275016\n",
      "Epoch 290, Loss: 0.0007745964103378356\n",
      "Epoch 291, Loss: 0.0003287192084826529\n",
      "Epoch 292, Loss: 0.001075470820069313\n",
      "Epoch 293, Loss: 0.0006110022659413517\n",
      "Epoch 294, Loss: 0.0013783491449430585\n",
      "Epoch 295, Loss: 0.0010148423025384545\n",
      "Epoch 296, Loss: 0.0011246552458032966\n",
      "Epoch 297, Loss: 0.0012150595430284739\n",
      "Epoch 298, Loss: 0.0009664999088272452\n",
      "Epoch 299, Loss: 0.0007604541024193168\n",
      "Epoch 300, Loss: 0.0009039815631695092\n",
      "Epoch 301, Loss: 0.002069206442683935\n",
      "Epoch 302, Loss: 0.0013172191102057695\n",
      "Epoch 303, Loss: 0.0002931957133114338\n",
      "Epoch 304, Loss: 0.0006271345191635191\n",
      "Epoch 305, Loss: 0.00040668132714927197\n",
      "Epoch 306, Loss: 0.00023897447681520134\n",
      "Epoch 307, Loss: 5.702091584680602e-05\n",
      "Epoch 308, Loss: 0.00026868301210924983\n",
      "Epoch 309, Loss: 0.00025175936752930284\n",
      "Epoch 310, Loss: 0.0001693946251180023\n",
      "Epoch 311, Loss: 0.00025258021196350455\n",
      "Epoch 312, Loss: 0.0002432570036035031\n",
      "Epoch 313, Loss: 0.0001359916786896065\n",
      "Epoch 314, Loss: 0.0003139651962555945\n",
      "Epoch 315, Loss: 0.00015095005801413208\n",
      "Epoch 316, Loss: 0.0005673185805790126\n",
      "Epoch 317, Loss: 0.0008733421564102173\n",
      "Epoch 318, Loss: 0.0006445712060667574\n",
      "Epoch 319, Loss: 0.0004979808581992984\n",
      "Epoch 320, Loss: 0.003141476074233651\n",
      "Epoch 321, Loss: 0.0005402461974881589\n",
      "Epoch 322, Loss: 0.0004520683141890913\n",
      "Epoch 323, Loss: 0.0006701996899209917\n",
      "Epoch 324, Loss: 0.00041678588604554534\n",
      "Epoch 325, Loss: 0.0003856815746985376\n",
      "Epoch 326, Loss: 0.00040903620538301766\n",
      "Epoch 327, Loss: 0.00047095539048314095\n",
      "Epoch 328, Loss: 0.0001442541542928666\n",
      "Epoch 329, Loss: 0.0004729603242594749\n",
      "Epoch 330, Loss: 0.00029437741613946855\n",
      "Epoch 331, Loss: 0.0002675990399438888\n",
      "Epoch 332, Loss: 0.00034154238528572023\n",
      "Epoch 333, Loss: 0.0008393203024752438\n",
      "Epoch 334, Loss: 0.0001842622586991638\n",
      "Epoch 335, Loss: 0.00048776090261526406\n",
      "Epoch 336, Loss: 0.0008390689617954195\n",
      "Epoch 337, Loss: 0.00020347429381217808\n",
      "Epoch 338, Loss: 0.0002600302395876497\n",
      "Epoch 339, Loss: 0.0012105881469324231\n",
      "Epoch 340, Loss: 0.0013870518887415528\n",
      "Epoch 341, Loss: 0.0012028543278574944\n",
      "Epoch 342, Loss: 0.0015812983037903905\n",
      "Epoch 343, Loss: 0.00032745805219747126\n",
      "Epoch 344, Loss: 0.0015376564115285873\n",
      "Epoch 345, Loss: 0.00417456915602088\n",
      "Epoch 346, Loss: 0.0008996358956210315\n",
      "Epoch 347, Loss: 0.001059990143403411\n",
      "Epoch 348, Loss: 0.0001681275898590684\n",
      "Epoch 349, Loss: 0.00044004913070239127\n",
      "Epoch 350, Loss: 0.0007192062330432236\n",
      "Epoch 351, Loss: 0.0003063636540900916\n",
      "Epoch 352, Loss: 0.0012041128939017653\n",
      "Epoch 353, Loss: 0.00016344500181730837\n",
      "Epoch 354, Loss: 0.00036238363827578723\n",
      "Epoch 355, Loss: 0.0003225205291528255\n",
      "Epoch 356, Loss: 5.5193544540088624e-05\n",
      "Epoch 357, Loss: 0.00019817323482129723\n",
      "Epoch 358, Loss: 0.00021110291709192097\n",
      "Epoch 359, Loss: 0.0004300821165088564\n",
      "Epoch 360, Loss: 0.00011199191067134961\n",
      "Epoch 361, Loss: 0.0006641126237809658\n",
      "Epoch 362, Loss: 0.0002647761721163988\n",
      "Epoch 363, Loss: 0.000299134902888909\n",
      "Epoch 364, Loss: 0.00031414235127158463\n",
      "Epoch 365, Loss: 0.0003717238432727754\n",
      "Epoch 366, Loss: 0.0003861530276481062\n",
      "Epoch 367, Loss: 0.00043031395762227476\n",
      "Epoch 368, Loss: 0.0004983631079085171\n",
      "Epoch 369, Loss: 0.0005404386320151389\n",
      "Epoch 370, Loss: 0.000746090488974005\n",
      "Epoch 371, Loss: 0.0002134507813025266\n",
      "Epoch 372, Loss: 0.0005051278276368976\n",
      "Epoch 373, Loss: 0.0006653472664766014\n",
      "Epoch 374, Loss: 0.0006749312160536647\n",
      "Epoch 375, Loss: 0.0005026271683163941\n",
      "Epoch 376, Loss: 0.00040386751061305404\n",
      "Epoch 377, Loss: 0.0014556710375472903\n",
      "Epoch 378, Loss: 0.0006645534886047244\n",
      "Epoch 379, Loss: 0.0009318264201283455\n",
      "Epoch 380, Loss: 0.00038059987127780914\n",
      "Epoch 381, Loss: 0.0003585670201573521\n",
      "Epoch 382, Loss: 0.00031778239645063877\n",
      "Epoch 383, Loss: 0.00014423602260649204\n",
      "Epoch 384, Loss: 0.0002821931557264179\n",
      "Epoch 385, Loss: 0.0002721307973843068\n",
      "Epoch 386, Loss: 0.00021163247583899647\n",
      "Epoch 387, Loss: 0.00048585073091089725\n",
      "Epoch 388, Loss: 0.000306034431559965\n",
      "Epoch 389, Loss: 0.00030150552629493177\n",
      "Epoch 390, Loss: 0.00020050174498464912\n",
      "Epoch 391, Loss: 0.0009362348937429488\n",
      "Epoch 392, Loss: 0.0004346791538409889\n",
      "Epoch 393, Loss: 0.0020325244404375553\n",
      "Epoch 394, Loss: 0.0009810889605432749\n",
      "Epoch 395, Loss: 0.0014722931664437056\n",
      "Epoch 396, Loss: 0.00023149288608692586\n",
      "Epoch 397, Loss: 0.0007882917998358607\n",
      "Epoch 398, Loss: 0.0003127816889900714\n",
      "Epoch 399, Loss: 0.0002774753957055509\n",
      "Epoch 400, Loss: 0.00035530581953935325\n",
      "Epoch 401, Loss: 0.000613488897215575\n",
      "Epoch 402, Loss: 0.0009482670575380325\n",
      "Epoch 403, Loss: 0.0006343393470160663\n",
      "Epoch 404, Loss: 0.0002798524801619351\n",
      "Epoch 405, Loss: 0.0006047380738891661\n",
      "Epoch 406, Loss: 0.0016893470892682672\n",
      "Epoch 407, Loss: 0.0004965504049323499\n",
      "Epoch 408, Loss: 0.0022228017915040255\n",
      "Epoch 409, Loss: 0.0008837450295686722\n",
      "Epoch 410, Loss: 0.001188770867884159\n",
      "Epoch 411, Loss: 0.0005644577322527766\n",
      "Epoch 412, Loss: 0.0004911199794150889\n",
      "Epoch 413, Loss: 0.0007978507201187313\n",
      "Epoch 414, Loss: 0.0003257286734879017\n",
      "Epoch 415, Loss: 0.0002980136196129024\n",
      "Epoch 416, Loss: 0.0004134294867981225\n",
      "Epoch 417, Loss: 0.0002435727510601282\n",
      "Epoch 418, Loss: 0.0002238322194898501\n",
      "Epoch 419, Loss: 0.00020685502386186272\n",
      "Epoch 420, Loss: 0.0003211584407836199\n",
      "Epoch 421, Loss: 0.0003521982580423355\n",
      "Epoch 422, Loss: 0.00033816142240539193\n",
      "Epoch 423, Loss: 0.00035260015283711255\n",
      "Epoch 424, Loss: 0.000503601913806051\n",
      "Epoch 425, Loss: 0.00038191251223906875\n",
      "Epoch 426, Loss: 0.0004555047198664397\n",
      "Epoch 427, Loss: 0.00041514929034747183\n",
      "Epoch 428, Loss: 0.0004123531107325107\n",
      "Epoch 429, Loss: 0.00028269822360016406\n",
      "Epoch 430, Loss: 0.00022738755797035992\n",
      "Epoch 431, Loss: 0.0007371779647655785\n",
      "Epoch 432, Loss: 0.0006153163267299533\n",
      "Epoch 433, Loss: 0.0008572369115427136\n",
      "Epoch 434, Loss: 0.0011334059527143836\n",
      "Epoch 435, Loss: 0.000745643861591816\n",
      "Epoch 436, Loss: 0.000566297210752964\n",
      "Epoch 437, Loss: 0.00022751119104214013\n",
      "Epoch 438, Loss: 0.0006727338768541813\n",
      "Epoch 439, Loss: 0.0017081095138564706\n",
      "Epoch 440, Loss: 0.0005097514949738979\n",
      "Epoch 441, Loss: 0.0004494510649237782\n",
      "Epoch 442, Loss: 0.0004358447331469506\n",
      "Epoch 443, Loss: 0.00028774028760381043\n",
      "Epoch 444, Loss: 0.00012782697740476578\n",
      "Epoch 445, Loss: 0.0001791526301531121\n",
      "Epoch 446, Loss: 6.201610813150182e-05\n",
      "Epoch 447, Loss: 0.00037758491816930473\n",
      "Epoch 448, Loss: 0.00010456593008711934\n",
      "Epoch 449, Loss: 0.00012234431051183492\n",
      "Epoch 450, Loss: 0.00011161179281771183\n",
      "Epoch 451, Loss: 9.68684398685582e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, states)\n\u001b[0;32m      9\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 10\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m             )\n\u001b[1;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    158\u001b[0m         group,\n\u001b[0;32m    159\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    164\u001b[0m         state_steps)\n\u001b[1;32m--> 166\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 316\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\adam.py:382\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight_decay \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    380\u001b[0m     grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39madd(param, alpha\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m--> 382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    383\u001b[0m     grad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(grad)\n\u001b[0;32m    384\u001b[0m     exp_avg \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(exp_avg)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "num_epochs = 25 # Peut-être avec plus d'epoch on obtiendrait un meilleur résultat ? jsp\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for images, states in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, states)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'big_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0092, -0.9540,  0.0486,  1.4971],\n",
      "        [-0.0357, -0.0229,  0.1048,  0.3178],\n",
      "        [ 0.0239, -0.2082, -0.0146,  0.3355],\n",
      "        [-0.0185, -0.4558, -0.0187,  0.5941],\n",
      "        [-0.0435, -0.0369, -0.0542, -0.1362],\n",
      "        [-0.0636,  0.1759,  0.0462, -0.1915],\n",
      "        [ 0.2299,  0.5334,  0.0527,  0.0604],\n",
      "        [ 0.0601, -0.0400, -0.0620,  0.1555],\n",
      "        [-0.0750,  0.0388,  0.1127,  0.2067],\n",
      "        [ 0.0088, -0.1738, -0.0764, -0.1388]]) tensor([[-0.0143, -0.9412,  0.0528,  1.4701],\n",
      "        [-0.0348, -0.0212,  0.1052,  0.3179],\n",
      "        [ 0.0211, -0.2158, -0.0181,  0.3296],\n",
      "        [-0.0234, -0.4372, -0.0156,  0.5839],\n",
      "        [-0.0416, -0.0287, -0.0545, -0.1491],\n",
      "        [-0.0674,  0.1740,  0.0512, -0.1989],\n",
      "        [ 0.2192,  0.5393,  0.0624,  0.0577],\n",
      "        [ 0.0585, -0.0374, -0.0562,  0.1533],\n",
      "        [-0.0784,  0.0382,  0.1062,  0.1937],\n",
      "        [ 0.0088, -0.1547, -0.0760, -0.1607]])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "for images, labels in dataloader:\n",
    "    with torch.no_grad():\n",
    "        print(model(images),labels)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voir encore mieux ce que ça donne\n",
    "\n",
    "# 1 : On collecte des images du cartpole (heuristique : random)\n",
    "\n",
    "env = gymnasium.make('CartPole-v1', render_mode = 'rgb_array')\n",
    "data_images_bis = []\n",
    "data_states_bis = []\n",
    "\n",
    "for episode in range(3):\n",
    "    observation_bis = env.reset()[0]\n",
    "    images_bis = []\n",
    "    for t in range(1000):\n",
    "        img = env.render()\n",
    "        tensor_image = transform(img).squeeze(0)  # Transform image immediately\n",
    "        images_bis.append(tensor_image)\n",
    "        \n",
    "        if len(images_bis) >= sequence_length:\n",
    "            # Stack the last sequence_length images to form a single sequence tensor\n",
    "            sequence_tensor = torch.stack(images_bis[-sequence_length:], dim=0)\n",
    "            data_images_bis.append(sequence_tensor)\n",
    "            data_states_bis.append(observation)\n",
    "        \n",
    "        action = env.action_space.sample()   # Use the heuristic policy\n",
    "        observation, reward, done, info, _ = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "data_states_bis = torch.tensor(data_states_bis, dtype=torch.float32)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "data_images_bis = torch.stack(data_images_bis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0761, 0.0623, 0.0388, 0.2811]]) tensor([ 0.0437, -0.2364,  0.0307,  0.2813])\n",
      "tensor([[0.0666, 0.0124, 0.0406, 0.3425]]) tensor([ 0.0389, -0.0417,  0.0363, -0.0016])\n",
      "tensor([[0.0733, 0.0225, 0.0401, 0.3391]]) tensor([ 0.0381,  0.1529,  0.0363, -0.2826])\n",
      "tensor([[0.0847, 0.0777, 0.0390, 0.2695]]) tensor([ 0.0412,  0.3475,  0.0306, -0.5636])\n",
      "tensor([[0.0915, 0.1408, 0.0348, 0.1756]]) tensor([ 0.0481,  0.1519,  0.0193, -0.2614])\n",
      "tensor([[0.0890, 0.1273, 0.0311, 0.1857]]) tensor([ 0.0512, -0.0435,  0.0141,  0.0373])\n",
      "tensor([[0.0790, 0.0817, 0.0326, 0.2456]]) tensor([ 0.0503, -0.2388,  0.0149,  0.3344])\n",
      "tensor([[0.0630, 0.0012, 0.0365, 0.3529]]) tensor([ 0.0455, -0.4341,  0.0215,  0.6317])\n",
      "tensor([[ 0.0589, -0.0352,  0.0398,  0.4050]]) tensor([ 0.0368, -0.2393,  0.0342,  0.3459])\n",
      "tensor([[ 0.0669, -0.0215,  0.0404,  0.3898]]) tensor([ 0.0320, -0.4349,  0.0411,  0.6491])\n",
      "tensor([[ 0.0518, -0.0644,  0.0402,  0.4256]]) tensor([ 0.0233, -0.6306,  0.0541,  0.9545])\n",
      "tensor([[ 0.0579, -0.0586,  0.0412,  0.4359]]) tensor([ 0.0107, -0.4362,  0.0732,  0.6793])\n",
      "tensor([[ 0.0603, -0.0513,  0.0403,  0.4360]]) tensor([ 0.0020, -0.2422,  0.0868,  0.4105])\n",
      "tensor([[0.0655, 0.0018, 0.0419, 0.3805]]) tensor([-0.0028, -0.4384,  0.0950,  0.7292])\n",
      "tensor([[0.0676, 0.0238, 0.0435, 0.3529]]) tensor([-0.0116, -0.6347,  0.1095,  1.0502])\n",
      "tensor([[ 0.0552, -0.0261,  0.0480,  0.4109]]) tensor([-0.0243, -0.4412,  0.1306,  0.7938])\n",
      "tensor([[ 0.0569, -0.0262,  0.0545,  0.4332]]) tensor([-0.0331, -0.2481,  0.1464,  0.5449])\n",
      "tensor([[ 0.0629, -0.0023,  0.0565,  0.4100]]) tensor([-0.0381, -0.0553,  0.1573,  0.3017])\n",
      "tensor([[0.0690, 0.0458, 0.0583, 0.3669]]) tensor([-0.0392, -0.2523,  0.1634,  0.6396])\n",
      "tensor([[0.0695, 0.0111, 0.0558, 0.4095]]) tensor([-0.0442, -0.4493,  0.1761,  0.9789])\n",
      "tensor([[ 0.0710, -0.0071,  0.0547,  0.4324]]) tensor([-0.0532, -0.2569,  0.1957,  0.7463])\n",
      "tensor([[ 0.0663, -0.0139,  0.0329,  0.3597]]) tensor([ 0.0047, -0.5579,  0.0332,  0.8661])\n",
      "tensor([[ 0.0612, -0.0264,  0.0374,  0.3872]]) tensor([-0.0065, -0.3632,  0.0506,  0.5840])\n",
      "tensor([[0.0640, 0.0032, 0.0396, 0.3556]]) tensor([-0.0137, -0.1688,  0.0622,  0.3077])\n",
      "tensor([[0.0623, 0.0110, 0.0441, 0.3526]]) tensor([-0.0171,  0.0254,  0.0684,  0.0352])\n",
      "tensor([[0.0648, 0.0288, 0.0448, 0.3277]]) tensor([-0.0166,  0.2194,  0.0691, -0.2351])\n",
      "tensor([[0.0660, 0.0279, 0.0442, 0.3152]]) tensor([-0.0122,  0.0234,  0.0644,  0.0786])\n",
      "tensor([[0.0652, 0.0270, 0.0448, 0.3166]]) tensor([-0.0117, -0.1726,  0.0660,  0.3908])\n",
      "tensor([[0.0626, 0.0118, 0.0450, 0.3379]]) tensor([-0.0152, -0.3686,  0.0738,  0.7036])\n",
      "tensor([[ 0.0553, -0.0036,  0.0476,  0.3645]]) tensor([-0.0226, -0.1746,  0.0879,  0.4350])\n",
      "tensor([[ 0.0560, -0.0105,  0.0511,  0.3997]]) tensor([-0.0261,  0.0192,  0.0966,  0.1713])\n",
      "tensor([[0.0619, 0.0187, 0.0531, 0.3731]]) tensor([-0.0257,  0.2128,  0.1000, -0.0895])\n",
      "tensor([[0.0735, 0.0779, 0.0513, 0.2958]]) tensor([-0.0214,  0.0164,  0.0982,  0.2330])\n",
      "tensor([[0.0694, 0.0647, 0.0516, 0.2999]]) tensor([-0.0211, -0.1799,  0.1029,  0.5550])\n",
      "tensor([[0.0645, 0.0368, 0.0511, 0.3372]]) tensor([-0.0247, -0.3764,  0.1140,  0.8782])\n",
      "tensor([[0.0599, 0.0021, 0.0535, 0.3916]]) tensor([-0.0322, -0.5728,  0.1315,  1.2045])\n",
      "tensor([[ 0.0641, -0.0188,  0.0545,  0.4283]]) tensor([-0.0437, -0.7694,  0.1556,  1.5353])\n",
      "tensor([[ 0.0571, -0.0622,  0.0602,  0.4825]]) tensor([-0.0591, -0.9660,  0.1863,  1.8722])\n",
      "tensor([[0.0871, 0.0966, 0.0308, 0.2272]]) tensor([ 0.0370, -0.1693,  0.0194,  0.3054])\n",
      "tensor([[0.0825, 0.0778, 0.0339, 0.2519]]) tensor([0.0336, 0.0255, 0.0255, 0.0189])\n",
      "tensor([[0.0815, 0.0754, 0.0361, 0.2590]]) tensor([ 0.0341, -0.1699,  0.0259,  0.3195])\n",
      "tensor([[0.0703, 0.0062, 0.0369, 0.3373]]) tensor([ 0.0307, -0.3654,  0.0323,  0.6202])\n",
      "tensor([[ 0.0655, -0.0188,  0.0374,  0.3700]]) tensor([ 0.0234, -0.5610,  0.0447,  0.9229])\n",
      "tensor([[ 0.0602, -0.0389,  0.0401,  0.4064]]) tensor([ 0.0122, -0.3665,  0.0631,  0.6446])\n",
      "tensor([[ 0.0636, -0.0319,  0.0393,  0.4093]]) tensor([ 0.0049, -0.1723,  0.0760,  0.3724])\n",
      "tensor([[0.0728, 0.0299, 0.0401, 0.3432]]) tensor([0.0014, 0.0217, 0.0835, 0.1046])\n",
      "tensor([[0.0760, 0.0726, 0.0422, 0.2849]]) tensor([ 0.0019,  0.2155,  0.0856, -0.1606])\n",
      "tensor([[0.0778, 0.0978, 0.0438, 0.2599]]) tensor([0.0062, 0.0193, 0.0824, 0.1578])\n",
      "tensor([[0.0782, 0.1002, 0.0440, 0.2625]]) tensor([ 0.0066,  0.2131,  0.0855, -0.1078])\n",
      "tensor([[0.0835, 0.1199, 0.0435, 0.2389]]) tensor([0.0108, 0.0169, 0.0834, 0.2106])\n",
      "tensor([[0.0792, 0.0963, 0.0433, 0.2642]]) tensor([ 0.0112, -0.1793,  0.0876,  0.5284])\n",
      "tensor([[0.0725, 0.0501, 0.0426, 0.3261]]) tensor([ 0.0076, -0.3756,  0.0981,  0.8473])\n",
      "tensor([[0.0716, 0.0404, 0.0441, 0.3424]]) tensor([ 6.8397e-05, -5.7189e-01,  1.1509e-01,  1.1692e+00])\n",
      "tensor([[ 0.0560, -0.0414,  0.0440,  0.4386]]) tensor([-0.0114, -0.7683,  0.1385,  1.4956])\n",
      "tensor([[ 0.0550, -0.0644,  0.0453,  0.4599]]) tensor([-0.0267, -0.9648,  0.1684,  1.8281])\n",
      "tensor([[ 0.0508, -0.0967,  0.0544,  0.5190]]) tensor([-0.0460, -0.7719,  0.2049,  1.5922])\n",
      "23.390504783019423\n"
     ]
    }
   ],
   "source": [
    "# 2 Afficher ce que prédit le modèle vs les vraies observations\n",
    "\n",
    "model.eval()\n",
    "total = 0 # Loss totale\n",
    "for images, states in zip(data_images_bis, data_states_bis):\n",
    "    with torch.no_grad():\n",
    "        print(model(images.unsqueeze(0)),states)\n",
    "        total += np.sum(np.array((model(images.unsqueeze(0))-states)**2))\n",
    "\n",
    "print(total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
