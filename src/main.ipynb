{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[easypip] Installing bbrl_gymnasium>=0.2.0\n",
      "[easypip] Installing bbrl_gymnasium[box2d]\n",
      "[easypip] Installing bbrl_gymnasium[classic_control]\n"
     ]
    }
   ],
   "source": [
    "from easypip import easyimport, easyinstall, is_notebook\n",
    "easyinstall(\"bbrl>=0.2.2\")\n",
    "easyinstall(\"swig\")\n",
    "easyinstall(\"bbrl_gymnasium>=0.2.0\")\n",
    "easyinstall(\"bbrl_gymnasium[box2d]\")\n",
    "easyinstall(\"bbrl_gymnasium[classic_control]\")\n",
    "easyinstall(\"tensorboard\")\n",
    "easyinstall(\"moviepy\")\n",
    "easyinstall(\"box2d-kengz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from moviepy.editor import ipython_display as video_display\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Tuple, Optional\n",
    "from functools import partial\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "import bbrl_gymnasium\n",
    "\n",
    "import copy\n",
    "from abc import abstractmethod, ABC\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from time import strftime\n",
    "\n",
    "from gymnasium import Env, Space, Wrapper, make\n",
    "\n",
    "from bbrl.agents.agent import Agent\n",
    "from bbrl import get_arguments, get_class, instantiate_class\n",
    "from bbrl.workspace import Workspace\n",
    "from bbrl.agents import Agents, TemporalAgent\n",
    "from bbrl.agents.gymnasium import GymAgent, ParallelGymAgent, make_env, record_video\n",
    "from bbrl.utils.replay_buffer import ReplayBuffer\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from typing import Tuple\n",
    "from bbrl.agents.gymnasium import make_env, GymAgent, ParallelGymAgent\n",
    "from functools import partial\n",
    "\n",
    "from bbrl import instantiate_class\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "\n",
    "import random as python_random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for launching tensorboard\n",
    "# For Colab - otherwise, it is easier and better to launch tensorboard from\n",
    "# the terminal\n",
    "def setup_tensorboard(path):\n",
    "    path = Path(path)\n",
    "    answer = \"\"\n",
    "    if is_notebook():\n",
    "        if get_ipython().__class__.__module__ == \"google.colab._shell\":\n",
    "            answer = \"y\"\n",
    "        while answer not in [\"y\", \"n\"]:\n",
    "                answer = input(f\"Do you want to launch tensorboard in this notebook [y/n] \").lower()\n",
    "\n",
    "    if answer == \"y\":\n",
    "        get_ipython().run_line_magic(\"load_ext\", \"tensorboard\")\n",
    "        get_ipython().run_line_magic(\"tensorboard\", f\"--logdir {path.absolute()}\")\n",
    "    else:\n",
    "        import sys\n",
    "        import os\n",
    "        import os.path as osp\n",
    "        print(f\"Launch tensorboard from the shell:\\n{osp.dirname(sys.executable)}/tensorboard --logdir={path.absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "# Memes parametres que dans les notebooks, pour le moment on y touche pas mais a changer apres peut etre\n",
    "params={\n",
    "  \"save_best\": False,\n",
    "  \"logger\":{\n",
    "    \"classname\": \"bbrl.utils.logger.TFLogger\",\n",
    "    \"log_dir\": \"./tblogs/dqn-buffer-\" + str(time.time()),\n",
    "    \"cache_size\": 10000,\n",
    "    \"every_n_seconds\": 10,\n",
    "    \"verbose\": False,    \n",
    "    },\n",
    "\n",
    "  \"algorithm\":{\n",
    "    \"seed\": SEED,\n",
    "    \"max_grad_norm\": 0.5,\n",
    "    \"epsilon\": 0.02,\n",
    "    \"n_envs\": 8,\n",
    "    \"n_steps\": 32,\n",
    "    \"n_updates\": 32,\n",
    "    \"eval_interval\": 2000,\n",
    "    \"learning_starts\": 2000,\n",
    "    \"nb_evals\": 10,\n",
    "    \"buffer_size\": 1e6,\n",
    "    \"batch_size\": 256,\n",
    "    \"target_critic_update\": 5000,\n",
    "    \"max_epochs\": 100, #MAX ITER \n",
    "    \"discount_factor\": 0.99,\n",
    "    \"architecture\":{\"hidden_size\": [64, 64]},\n",
    "  },\n",
    "  \"gym_env\":{\n",
    "    \"env_name\": \"CartPole-v1\",\n",
    "  },\n",
    "  \"optimizer\":\n",
    "  {\n",
    "    \"classname\": \"torch.optim.Adam\",\n",
    "    \"lr\": 1e-3,\n",
    "  }\n",
    "}\n",
    "\n",
    "config=OmegaConf.create(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST AVEC UN RANDOM AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meme randomAgent que dans les notebooks\n",
    "class RandomAgent(Agent):\n",
    "    def __init__(self, action_dim):\n",
    "        super().__init__()\n",
    "        self.action_dim = action_dim\n",
    "        self.all_observations = []\n",
    "\n",
    "    def forward(self, t: int, choose_action=True, **kwargs):\n",
    "        \"\"\"An Agent can use self.workspace\"\"\"\n",
    "        obs = self.get((\"env/env_obs\", t))\n",
    "        action = torch.randint(0, self.action_dim, (len(obs), ))\n",
    "        self.set((\"action\", t), action)\n",
    "        self.all_observations.append(obs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on extend le parallelGymAgent pour pouvoir obtenir l'environment des agents\n",
    "# finalement y pas eu besoin apres donc on peut changer ca mais on fera apres \n",
    "# A CHANGER A PRIORI C'EST PLUS UTILE ICI\n",
    "class CustomParallelGymAgent(ParallelGymAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def get_my_attribute(self):\n",
    "        return self.envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#un des agents les plus importants:\n",
    "#on utilise le CustomParallelGymAgent pour obtenir l'environment d'un agent puis on extrait les informations\n",
    "#notamment l'image du cartpole a chaque temps t\n",
    "#avec cette image, on la preprocess et on la passe dans un cnn (defini plus bas)\n",
    "class AttributeAccessAgent(Agent):\n",
    "    def __init__(self, env_agent, pre_processing_agent, cnn_agent):\n",
    "        super().__init__()\n",
    "        #les agents\n",
    "        self.env_agent = env_agent  # CustomParallelGymAgent ou ParallelGymAgent\n",
    "        self.pre_processing_agent = pre_processing_agent\n",
    "        self.cnn_agent = cnn_agent\n",
    "        #init de la liste de listes pour store les images\n",
    "        self.list_images = [[] for _ in range(self.env_agent.num_envs)]\n",
    "        self.list_features = [[] for _ in range(self.env_agent.num_envs)]\n",
    "    \n",
    "    def forward(self, t: int, **kwargs):\n",
    "        #ici on fait le forward pour chaque agent (donc en fonction du nombre d'environments)\n",
    "        for env_index in range(self.env_agent.num_envs):\n",
    "            image = self.env_agent.envs[env_index].render() #on recupere l'image\n",
    "            display = False #pour afficher en temps reel mais pas utile pour +1 env\n",
    "            if display: #print a chaque temps l'image originale\n",
    "                clear_output(wait=True)   \n",
    "                plt.imshow(image)  \n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "                print(\"Displayed image at time\", t, \" of agent \", env_index)\n",
    "            #print(image.shape)\n",
    "            \n",
    "            image_pre_processed = self.pre_processing_agent.preprocess(image) #image renvoyee pas le preprocesseur\n",
    "\n",
    "            self.list_images[env_index].append(image_pre_processed) #on ajoute au bon endroit dans la liste\n",
    "\n",
    "            # attention a bien mettre l'image preprocessed ici sinon y a des pb avec le cnn\n",
    "            features = self.cnn_agent.process_image(image_pre_processed) \n",
    "            #print(features.shape)\n",
    "            self.list_features[env_index].append(features) #on ajoute pareil pour les features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent pour gerer le preprocessing, rien de trop particulier\n",
    "# on pourrait faire du preprocessing plus complexe si jamais on obtient des mauvais resultats\n",
    "class PreProcessingAgent(Agent):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def preprocess(self, image):\n",
    "        processed_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)  #on mets en noir et blanc\n",
    "        processed_image = cv2.resize(processed_image, (224, 224))  #on rescale \n",
    "        return processed_image #image de taille (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modele CNN: c'est une implementation plutot basqiue de cnn, truc classique qu'on trouve sur internet\n",
    "#on fait avec 5 layers pcq a priori ca devrait suffire\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            #layer 1\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 16 x 112 x 112\n",
    "            \n",
    "            #layer 2\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 32 x 56 x 56\n",
    "            \n",
    "            #layer 3\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 64 x 28 x 28\n",
    "            \n",
    "            #layer 4\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 128 x 14 x 14\n",
    "            \n",
    "            #layer 5\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 256 x 7 x 7\n",
    "        )\n",
    "        \n",
    "        self.fc_layers = nn.Sequential( #on cherche a avoir un vecteur de taille 128 (une ligne)\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 7 * 7, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),  \n",
    "        )\n",
    "        \n",
    "    def forward(self, x): #ici c'est l'image qui a ete converti en tensor\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l'agent cnn qui implemente le modele cnn defini en haut\n",
    "class CNNAgent(Agent):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = CNN() #le cnn\n",
    "\n",
    "    def process_image(self, image):\n",
    "        #on converti l'image (preprocessed) en un tensor qu'on peut mettre dans le cnn\n",
    "        #taille [1, 1, 224, 224], batch_size, channels, height, width\n",
    "        image_tensor = torch.tensor(image, dtype=torch.float).unsqueeze(0).unsqueeze(0)  \n",
    "        image_tensor = image_tensor / 255.0   #petite normalisation\n",
    "\n",
    "        with torch.no_grad(): #on calcule pas le gradient (a voir si on le fait apres)\n",
    "            features = self.cnn(image_tensor)\n",
    "        features = features.cpu().numpy()\n",
    "        return features #on renvoie le vecteur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#petite fonction pour afficher toutes les etapes d'une execution d'un agent dans un environement\n",
    "def displayImagesPerAgent(images_per_agent):\n",
    "    n_cols = 4  # nb de colonnes avec des images a afficher\n",
    "    \n",
    "    for env_index, images in enumerate(images_per_agent):\n",
    "        print(f\"Environment {env_index + 1}:\")\n",
    "        n_images = len(images)\n",
    "        n_rows = (n_images + n_cols - 1) // n_cols  #nb de lignes\n",
    "\n",
    "        #trucs a changer pour l'affichage mais pas beosin d'y toucher je pense\n",
    "        figsize_width = 10  \n",
    "        figsize_height = n_rows * (figsize_width / n_cols) * 0.5 \n",
    "        plt.figure(figsize=(figsize_width, figsize_height))\n",
    "\n",
    "        for i, image in enumerate(images):\n",
    "            plt.subplot(n_rows, n_cols, i+1)\n",
    "            plt.imshow(image)\n",
    "            plt.axis('off')  \n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donzo got all images\n",
      "Environment 1:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x630 with 26 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "workspace = Workspace()\n",
    "nb_env = 1 #changer le nombre d'environements qu'on veut ici\n",
    "action_dim = 2 #vaudrait mieux prednre depuis le parallelgymagent mais ca va jamais changer ici\n",
    "autoreset = False #pour le moment pouor le simplifier la vie\n",
    "pre_processing_agent = PreProcessingAgent()\n",
    "cnn_agent = CNNAgent()\n",
    "\n",
    "im_env_agent = CustomParallelGymAgent(partial(make_env, config.gym_env.env_name, render_mode=\"rgb_array\", autoreset=autoreset), nb_env).seed(SEED)\n",
    "attribute_access_agent = AttributeAccessAgent(im_env_agent, pre_processing_agent, cnn_agent)\n",
    "agents = Agents(im_env_agent, RandomAgent(action_dim), attribute_access_agent)\n",
    "t_agents = TemporalAgent(agents)\n",
    "\n",
    "\n",
    "t_agents(workspace, t=0, stop_variable=\"env/done\", stochastic=True)\n",
    "images = attribute_access_agent.list_images \n",
    "print('Donzo got all images')\n",
    "displayImagesPerAgent(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK ON VA ESSAYER DE FAIRE UN AGENT DQN MAINTENANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "test = len(attribute_access_agent.list_features[0][0][0])#lol\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "  \"save_best\": False,\n",
    "  \"logger\":{\n",
    "    \"classname\": \"bbrl.utils.logger.TFLogger\",\n",
    "    \"log_dir\": \"./tblogs/dqn-simple-\" + str(time.time()),\n",
    "    \"cache_size\": 10000,\n",
    "    \"every_n_seconds\": 10,\n",
    "    \"verbose\": False,    \n",
    "    },\n",
    "\n",
    "  \"algorithm\":{\n",
    "    \"seed\": 3,\n",
    "    \"max_grad_norm\": 0.5,\n",
    "    \"epsilon\": 0.02,\n",
    "    \"n_envs\": 2,\n",
    "    \"n_steps\": 32,\n",
    "    \"eval_interval\": 2000,\n",
    "    \"nb_measures\": 200,\n",
    "    \"nb_evals\": 10,\n",
    "    \"discount_factor\": 0.99,\n",
    "    \"architecture\":{\"hidden_size\": [128, 128]},\n",
    "  },\n",
    "  \"gym_env\":{\n",
    "    \"env_name\": \"CartPole-v1\",\n",
    "  },\n",
    "  \"optimizer\":\n",
    "  {\n",
    "    \"classname\": \"torch.optim.Adam\",\n",
    "    \"lr\": 2e-3,\n",
    "  }\n",
    "}\n",
    "\n",
    "cfg=OmegaConf.create(params)\n",
    "pre_processing_agent = PreProcessingAgent()\n",
    "cnn_agent = CNNAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(sizes, activation, output_activation=nn.Identity()):\n",
    "    \"\"\"Helper function to build a multi-layer perceptron (function from $\\mathbb R^n$ to $\\mathbb R^p$)\n",
    "    \n",
    "    Args:\n",
    "        sizes (List[int]): the number of neurons at each layer\n",
    "        activation (nn.Module): a PyTorch activation function (after each layer but the last)\n",
    "        output_activation (nn.Module): a PyTorch activation function (last layer)\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    for j in range(len(sizes) - 1):\n",
    "        act = activation if j < len(sizes) - 2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j + 1]), act]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "#QUAND UN BUILD LE MLP FAUT QUE sizes SOIT DU TYPE [128, 64, 2]\n",
    "#ex: mlp = build_mlp(sizes=[128] + [64, 64] + [2], activation=nn.ReLU(), output_activation=nn.Identity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env_agents(cfg) -> Tuple[GymAgent, GymAgent]:\n",
    "    # Returns a pair of environments (train / evaluation) based on a configuration `cfg`\n",
    "    \n",
    "    # Train environment\n",
    "    train_env_agent = ParallelGymAgent(\n",
    "        partial(make_env,  cfg.gym_env.env_name, render_mode=\"rgb_array\", autoreset=False),\n",
    "        cfg.algorithm.n_envs\n",
    "    ).seed(cfg.algorithm.seed)\n",
    "\n",
    "    # Test environment\n",
    "    eval_env_agent = ParallelGymAgent(\n",
    "        partial(make_env, cfg.gym_env.env_name, render_mode=\"rgb_array\"), \n",
    "        cfg.algorithm.nb_evals\n",
    "    ).seed(cfg.algorithm.seed)\n",
    "\n",
    "    return train_env_agent, eval_env_agent\n",
    "\n",
    "train_env_agent, eval_env_agent = get_env_agents(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Petit soucis a priori: apres avoir regarde dans le code source de bbrl, je crois qu'on peut pas rajouter de variables dans le workspace. Donc il faut contourner ce probleme et utiliser d'autres methodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], []]\n"
     ]
    }
   ],
   "source": [
    "class DiscreteQAgent(Agent):\n",
    "    \"\"\"BBRL agent (discrete actions) based on a MLP\"\"\"\n",
    "    def __init__(self, state_dim, hidden_layers, action_dim, attribute_access_train, attribute_access_eval):\n",
    "        super().__init__()\n",
    "        self.model = build_mlp(\n",
    "            [state_dim] + list(hidden_layers) + [action_dim], activation=nn.ReLU()\n",
    "        )\n",
    "        self.attribute_access_train = attribute_access_train\n",
    "        self.attribute_access_eval = attribute_access_eval\n",
    "\n",
    "\n",
    "    def forward(self, t: int, choose_action=True, **kwargs):\n",
    "        \"\"\"An Agent can use self.workspace\"\"\"\n",
    "\n",
    "        # Retrieves the observation from the environment at time t\n",
    "        # obs = self.get((\"env/env_obs\", t))\n",
    "\n",
    "        # HONESTLY DONT KNOW WHICH AGENT TO USE HERE BUT WE'LL SEE\n",
    "        obs = self.attribute_access_train.list_features[0][t]\n",
    "\n",
    "        # Computes the critic (Q) values for the observation\n",
    "        q_values = self.model(obs)\n",
    "\n",
    "        # ... and sets the q-values (one for each possible action)\n",
    "        self.set((\"q_values\", t), q_values)\n",
    "\n",
    "        # Flag to toggle the fact that the action is chosen\n",
    "        # by this agent; otherwise, we use a specific agent\n",
    "        # (ex. epsilon-greedy) that implements the current policy,\n",
    "        # see below (Exploration method)\n",
    "        if choose_action:\n",
    "            action = q_values.argmax(1)\n",
    "            self.set((\"action\", t), action)\n",
    "\n",
    "attribute_access_train = AttributeAccessAgent(train_env_agent, pre_processing_agent, cnn_agent)\n",
    "attribute_access_eval = AttributeAccessAgent(eval_env_agent, pre_processing_agent, cnn_agent)\n",
    "\n",
    "\n",
    "critic = DiscreteQAgent(128, cfg.algorithm.architecture.hidden_size, 2, attribute_access_train, attribute_access_eval)\n",
    "\n",
    "print(critic.attribute_access_train.list_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGreedyActionSelector(Agent):\n",
    "    def __init__(self, epsilon):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, t: int, **kwargs):\n",
    "        # Retrieves the q values \n",
    "        # (matrix nb. of episodes x nb. of actions)\n",
    "        q_values = self.get((\"q_values\", t))\n",
    "        size, nb_actions = q_values.size()\n",
    "\n",
    "        # Flag \n",
    "        is_random = torch.rand(size).lt(self.epsilon).float()\n",
    "        random_action = torch.randint(low=0, high=nb_actions, size=(size,))\n",
    "        max_action = q_values.max(1)[1]\n",
    "\n",
    "        # Choose the action based on the is_random flag\n",
    "        action = is_random * random_action + (1 - is_random) * max_action\n",
    "\n",
    "        # Sets the action at time t\n",
    "        self.set((\"action\", t), action.long())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est ici (en bas) qu'il va falloir modifier le code pour acceder aux observations correctement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dqn_agent(cfg, train_env_agent, eval_env_agent, attribute_access_train, attribute_access_eval) -> Tuple[TemporalAgent, TemporalAgent]:\n",
    "    # Get the observation / action state space dimensions\n",
    "    obs_size = 128\n",
    "    act_size = 2\n",
    "\n",
    "    # Our discrete Q-Agent    \n",
    "    critic = DiscreteQAgent(obs_size, cfg.algorithm.architecture.hidden_size, act_size, attribute_access_train, attribute_access_eval)\n",
    "\n",
    "    # The agent used for training\n",
    "    explorer = EGreedyActionSelector(cfg.algorithm.epsilon)\n",
    "    q_agent = TemporalAgent(critic)\n",
    "    tr_agent = Agents(train_env_agent, critic, explorer)\n",
    "    train_agent = TemporalAgent(tr_agent)\n",
    "\n",
    "    # The agent used for evaluation\n",
    "    ev_agent = Agents(eval_env_agent, critic)\n",
    "    eval_agent = TemporalAgent(ev_agent)\n",
    "    \n",
    "    return train_agent, eval_agent, q_agent\n",
    "\n",
    "dqn_agent = create_dqn_agent(cfg, train_env_agent, eval_env_agent, attribute_access_train, attribute_access_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bbrl import instantiate_class\n",
    "\n",
    "class Logger():\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        self.logger = instantiate_class(cfg.logger)\n",
    "\n",
    "    def add_log(self, log_string, loss, steps):\n",
    "        self.logger.add_scalar(log_string, loss.item(), steps)\n",
    "\n",
    "    # A specific function for RL algorithms having a critic, an actor and an entropy losses\n",
    "    def log_losses(self, critic_loss, entropy_loss, actor_loss, steps):\n",
    "        self.add_log(\"critic_loss\", critic_loss, steps)\n",
    "        self.add_log(\"entropy_loss\", entropy_loss, steps)\n",
    "        self.add_log(\"actor_loss\", actor_loss, steps)\n",
    "\n",
    "    def log_reward_losses(self, rewards, nb_steps):\n",
    "        self.add_log(\"reward/mean\", rewards.mean(), nb_steps)\n",
    "        self.add_log(\"reward/max\", rewards.max(), nb_steps)\n",
    "        self.add_log(\"reward/min\", rewards.min(), nb_steps)\n",
    "        self.add_log(\"reward/median\", rewards.median(), nb_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the tensor dimensions\n",
    "# T = maximum number of time steps\n",
    "# B = number of episodes run in parallel \n",
    "# A = state space dimension\n",
    "\n",
    "def compute_critic_loss(cfg, reward: torch.Tensor, must_bootstrap: torch.Tensor, q_values: torch.Tensor, action: torch.LongTensor):\n",
    "\n",
    "    q_values_for_actions = q_values.gather(2, action.unsqueeze(-1)).squeeze(-1)\n",
    "    \n",
    "    # Compute the max Q-value for the next state, but not for the last timestep\n",
    "    next_q_values = q_values[1:].max(dim=2)[0]\n",
    "    # Compute the expected Q-values (target) for the current state and action\n",
    "    target_q_values = reward[:-1] + cfg[\"algorithm\"][\"discount_factor\"] * next_q_values * must_bootstrap[:-1]\n",
    "    \n",
    "    # Compute the loss as the mean squared error between the current and target Q-values\n",
    "    loss = F.mse_loss(q_values_for_actions[:-1], target_q_values)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the optimizer over the q agent\n",
    "def setup_optimizer(cfg, q_agent):\n",
    "    optimizer_args = get_arguments(cfg.optimizer)\n",
    "    parameters = q_agent.parameters()\n",
    "    optimizer = get_class(cfg.optimizer)(parameters, **optimizer_args)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dqn(cfg):\n",
    "    # 1)  Build the  logger\n",
    "    logger = Logger(cfg)\n",
    "    best_reward = float('-inf')\n",
    "\n",
    "    # 2) Create the environment agent\n",
    "    train_env_agent, eval_env_agent = get_env_agents(cfg)\n",
    "\n",
    "    attribute_access_train = AttributeAccessAgent(train_env_agent, pre_processing_agent, cnn_agent)\n",
    "    attribute_access_eval = AttributeAccessAgent(eval_env_agent, pre_processing_agent, cnn_agent)\n",
    "\n",
    "\n",
    "    critic = DiscreteQAgent(128, cfg.algorithm.architecture.hidden_size, 2, attribute_access_train, attribute_access_eval)\n",
    "    \n",
    "    # 3) Create the DQN Agent\n",
    "    train_agent, eval_agent, q_agent = create_dqn_agent(\n",
    "        cfg, train_env_agent, eval_env_agent, attribute_access_train, attribute_access_eval\n",
    "    )\n",
    "\n",
    "    # Note that no parameter is needed to create the workspace.\n",
    "    # In the training loop, calling the train_agent\n",
    "    # will take the workspace as parameter\n",
    "\n",
    "    # 6) Configure the optimizer\n",
    "    optimizer = setup_optimizer(cfg, q_agent)\n",
    "    nb_steps = 0\n",
    "    tmp_steps = 0\n",
    "    nb_measures = 0\n",
    "\n",
    "    while nb_measures < cfg.algorithm.nb_measures:\n",
    "        train_workspace = Workspace()\n",
    "\n",
    "        # Run \n",
    "        train_agent(train_workspace, t=0, stop_variable=\"env/done\", stochastic=True)\n",
    "\n",
    "        q_values, done, truncated, reward, action = train_workspace[\n",
    "            \"q_values\", \"env/done\", \"env/truncated\", \"env/reward\", \"action\"\n",
    "        ]\n",
    "\n",
    "        nb_steps += len(action.flatten())\n",
    "        \n",
    "        # Determines whether values of the critic should be propagated\n",
    "        # True if the episode reached a time limit or if the task was not done\n",
    "        # See https://colab.research.google.com/drive/1erLbRKvdkdDy0Zn1X_JhC01s1QAt4BBj\n",
    "        must_bootstrap = torch.logical_or(~done, truncated)\n",
    "        \n",
    "        # Compute critic loss\n",
    "        critic_loss = compute_critic_loss(cfg, reward, must_bootstrap, q_values, action)\n",
    "\n",
    "        # Store the loss for tensorboard display\n",
    "        logger.add_log(\"critic_loss\", critic_loss, nb_steps)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            q_agent.parameters(), cfg.algorithm.max_grad_norm\n",
    "        )\n",
    "        optimizer.step()\n",
    "\n",
    "        if nb_steps - tmp_steps > cfg.algorithm.eval_interval:\n",
    "            nb_measures += 1\n",
    "            tmp_steps = nb_steps\n",
    "            eval_workspace = Workspace()  # Used for evaluation\n",
    "            eval_agent(\n",
    "                eval_workspace, t=0, stop_variable=\"env/done\", choose_action=True\n",
    "            )\n",
    "            rewards = eval_workspace[\"env/cumulated_reward\"][-1]\n",
    "            mean = rewards.mean()\n",
    "            logger.add_log(\"reward\", mean, nb_steps)\n",
    "            print(f\"nb_steps: {nb_steps}, reward: {mean}\")\n",
    "            if cfg.save_best and mean > best_reward:\n",
    "                best_reward = mean\n",
    "                directory = \"./dqn_critic/\"\n",
    "                if not os.path.exists(directory):\n",
    "                    os.makedirs(directory)\n",
    "                filename = directory + \"dqn_\" + str(mean.item()) + \".agt\"\n",
    "                eval_agent.save_model(filename)\n",
    "                \n",
    "    return train_agent, eval_agent, q_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launch tensorboard from the shell:\n",
      "c:\\Users\\titouan\\anaconda3/tensorboard --logdir=c:\\Users\\titouan\\OneDrive\\Bureau\\SORBONNE S2\\PLDAC_BBRL\\src/tblogs\n",
      "0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [186]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m cfg\u001b[38;5;241m=\u001b[39mOmegaConf\u001b[38;5;241m.\u001b[39mcreate(params)\n\u001b[0;32m     14\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(config\u001b[38;5;241m.\u001b[39malgorithm\u001b[38;5;241m.\u001b[39mseed)\n\u001b[1;32m---> 15\u001b[0m train_agent, eval_agent, q_agent \u001b[38;5;241m=\u001b[39m \u001b[43mrun_dqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [183]\u001b[0m, in \u001b[0;36mrun_dqn\u001b[1;34m(cfg)\u001b[0m\n\u001b[0;32m     31\u001b[0m train_workspace \u001b[38;5;241m=\u001b[39m Workspace()\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Run \u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_workspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_variable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menv/done\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstochastic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m q_values, done, truncated, reward, action \u001b[38;5;241m=\u001b[39m train_workspace[\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq_values\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv/done\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv/truncated\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv/reward\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     38\u001b[0m ]\n\u001b[0;32m     40\u001b[0m nb_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(action\u001b[38;5;241m.\u001b[39mflatten())\n",
      "File \u001b[1;32mc:\\Users\\titouan\\anaconda3\\lib\\site-packages\\bbrl\\agents\\utils.py:79\u001b[0m, in \u001b[0;36mTemporalAgent.__call__\u001b[1;34m(self, workspace, t, n_steps, stop_variable, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m _t \u001b[38;5;241m=\u001b[39m t\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent(workspace, t\u001b[38;5;241m=\u001b[39m_t, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_variable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     81\u001b[0m         s \u001b[38;5;241m=\u001b[39m workspace\u001b[38;5;241m.\u001b[39mget(stop_variable, _t)\n",
      "File \u001b[1;32mc:\\Users\\titouan\\anaconda3\\lib\\site-packages\\bbrl\\agents\\utils.py:31\u001b[0m, in \u001b[0;36mAgents.__call__\u001b[1;34m(self, workspace, **kwargs)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, workspace, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents:\n\u001b[1;32m---> 31\u001b[0m         a(workspace, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\titouan\\anaconda3\\lib\\site-packages\\bbrl\\agents\\agent.py:75\u001b[0m, in \u001b[0;36mAgent.__call__\u001b[1;34m(self, workspace, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m workspace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Agent.__call__] workspace must not be None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkspace \u001b[38;5;241m=\u001b[39m workspace\n\u001b[1;32m---> 75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkspace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Input \u001b[1;32mIn [185]\u001b[0m, in \u001b[0;36mDiscreteQAgent.forward\u001b[1;34m(self, t, choose_action, **kwargs)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Retrieves the observation from the environment at time t\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# obs = self.get((\"env/env_obs\", t))\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# HONESTLY DONT KNOW WHICH AGENT TO USE HERE BUT WE'LL SEE\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(t)\n\u001b[1;32m---> 20\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattribute_access_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_features\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(obs)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Computes the critic (Q) values for the observation\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# For Colab - otherwise, it is easier and better to launch tensorboard from the terminal\n",
    "# tensorboard --logdir ./tblogs\n",
    "if is_notebook() and get_ipython().__class__.__module__ == \"google.colab._shell\":\n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir ./tblogs\n",
    "    pass\n",
    "else:\n",
    "    import sys\n",
    "    import os\n",
    "    import os.path as osp\n",
    "    print(f\"Launch tensorboard from the shell:\\n{osp.dirname(sys.executable)}/tensorboard --logdir={os.getcwd()}/tblogs\")\n",
    "\n",
    "cfg=OmegaConf.create(params)\n",
    "torch.manual_seed(config.algorithm.seed)\n",
    "train_agent, eval_agent, q_agent = run_dqn(cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
