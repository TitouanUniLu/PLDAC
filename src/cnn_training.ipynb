{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F  # Ensure this import is added\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import gymnasium \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 1 : RGB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "sequence_length = 4  # Number of images in each sequence, IL FAUT TESTER AVEC 2 P-E CA CHANGE TOUT \n",
    "num_episodes = 300   ### JOUER AVEC CE PARAMETRE POUR AMELIORER LE MODELE : TESTER AVEC 1000 SERAIT COOL\n",
    "\n",
    "# Environment Setup\n",
    "env = gymnasium.make('CartPole-v1',render_mode=\"rgb_array\")\n",
    "data_images = []\n",
    "data_states = []\n",
    "\n",
    "# Transformation for images\n",
    "transform = transforms.Compose([transforms.ToPILImage(), \n",
    "                    transforms.Resize(60, interpolation=Image.LANCZOS),\n",
    "                    transforms.ToTensor()])\n",
    "\n",
    "\n",
    "# Cart location for centering image crop\n",
    "def get_cart_location(screen_width):\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "# Cropping, downsampling (and Grayscaling) image\n",
    "def get_screen():\n",
    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "    screen = env.render().transpose((2, 0, 1))\n",
    "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width * 0.6)\n",
    "    cart_location = get_cart_location(screen_width)\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    screen = torch.from_numpy(screen)\n",
    "    return transform(screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data Collection using Heuristic Policy\n",
    "for episode in range(num_episodes):\n",
    "    observation = env.reset()[0]\n",
    "    images = [torch.zeros(3, 60, 135) for _ in range(4)]\n",
    "           \n",
    "    for t in range(1000):\n",
    "        tensor_image = get_screen()  # Transform image immediately\n",
    "        # if t==4:\n",
    "        #     print(tensor_image.shape)\n",
    "        #     plt.imshow(np.array(tensor_image.permute(1,2,0)))\n",
    "        images.append(tensor_image)\n",
    "        \n",
    "        if len(images) >= sequence_length:\n",
    "            # Stack the last sequence_length images to form a single sequence tensor\n",
    "            sequence_tensor = torch.stack(images[-sequence_length:], dim=0).permute(1, 0, 2, 3)\n",
    "            data_images.append(sequence_tensor)\n",
    "            data_states.append(observation)\n",
    "        \n",
    "        action = env.action_space.sample()  \n",
    "        observation, reward, done, info, _ = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Convert data_states to a tensor\n",
    "data_states = torch.tensor(data_states, dtype=torch.float32)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = TensorDataset(torch.stack(data_images), data_states)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv3d(3, 16, kernel_size=(3, 3, 3), stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2)),\n",
    "            nn.Conv3d(16, 32, kernel_size=(3, 3, 3), stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2)),\n",
    "        )\n",
    "        # Correctly calculate the input size for the linear layer based on the output from conv_layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(63360, 128),  # Adjusted based on actual output size\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 4)  # Predicting 4 state variables\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor for the fully connected layer\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.3072144091129303\n"
     ]
    }
   ],
   "source": [
    "# Model instantiation and training setup\n",
    "model = CNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 1 # Peut-être avec plus d'epoch on obtiendrait un meilleur résultat ? jsp\n",
    "for epoch in range(num_epochs):\n",
    "    for images, states in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, states)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'cartpole_cnn_rgb.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 4, 60, 135]) torch.Size([10, 4])\n",
      "tensor([[-0.0183,  0.0237, -0.0026,  0.0365]], grad_fn=<AddmmBackward0>) tensor([ 0.0107,  0.0295, -0.0081, -0.0399])\n"
     ]
    }
   ],
   "source": [
    "# Voir un peu ce que ça donne\n",
    "\n",
    "model.eval()\n",
    "for images, states in dataloader:\n",
    "    print(images.shape,states.shape)\n",
    "    print(model(images[0].unsqueeze(0)),states[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 4]) torch.Size([10, 4, 60, 135, 3])\n",
      "tensor([-0.0125,  0.2285,  0.0047, -0.2600])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x3000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for images, labels in dataloader:\n",
    "    images = images.permute((0,2,3,4,1))\n",
    "    print(labels.shape,images.shape)\n",
    "    fig,axes = plt.subplots(1,4,figsize = (20,30))\n",
    "    for i in range(4):\n",
    "        axes[i].imshow(images[0][i])\n",
    "    print(labels[0])\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\serialization.py:541\u001b[0m, in \u001b[0;36m_check_seekable\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 541\u001b[0m     \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseek\u001b[49m(f\u001b[38;5;241m.\u001b[39mtell())\n\u001b[0;32m    542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'seek'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m CNN()\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcartpole_cnn_rgb.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\serialization.py:450\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _open_buffer_writer(name_or_buffer)\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m--> 450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_buffer_reader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    452\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in mode but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\serialization.py:435\u001b[0m, in \u001b[0;36m_open_buffer_reader.__init__\u001b[1;34m(self, buffer)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, buffer):\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(buffer)\n\u001b[1;32m--> 435\u001b[0m     \u001b[43m_check_seekable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\serialization.py:544\u001b[0m, in \u001b[0;36m_check_seekable\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m    542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (io\u001b[38;5;241m.\u001b[39mUnsupportedOperation, \u001b[38;5;167;01mAttributeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 544\u001b[0m     \u001b[43mraise_err_msg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseek\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\serialization.py:537\u001b[0m, in \u001b[0;36m_check_seekable.<locals>.raise_err_msg\u001b[1;34m(patterns, e)\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n\u001b[0;32m    534\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. You can only torch.load from a file that is seekable.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    535\u001b[0m                         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please pre-load the data into a buffer like io.BytesIO and\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    536\u001b[0m                         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m try to load from it instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 537\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(msg)\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead."
     ]
    }
   ],
   "source": [
    "model = CNN()\n",
    "model = torch.load(model.state_dict(), 'cartpole_cnn_rgb.pth')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voir encore mieux ce que ça donne\n",
    "\n",
    "# 1 : On collecte des images du cartpole (heuristique : random)\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "data_images_bis = []\n",
    "data_states_bis = []\n",
    "\n",
    "for episode in range(3):\n",
    "    observation_bis = env.reset()\n",
    "    images_bis = []\n",
    "    for t in range(1000):\n",
    "        img = env.render(mode='rgb_array')\n",
    "        img_pil = Image.fromarray(img)\n",
    "        tensor_image = transform(img_pil)  # Transform image immediately\n",
    "        images_bis.append(tensor_image)\n",
    "        \n",
    "        if len(images_bis) >= sequence_length:\n",
    "            # Stack the last sequence_length images to form a single sequence tensor\n",
    "            sequence_tensor = torch.stack(images_bis[-sequence_length:], dim=0).permute(1, 0, 2, 3)\n",
    "            data_images_bis.append(sequence_tensor)\n",
    "            data_states_bis.append(observation)\n",
    "        \n",
    "        action = env.action_space.sample()   # Use the heuristic policy\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "data_states_bis = torch.tensor(data_states_bis, dtype=torch.float32)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "data_images_bis = torch.stack(data_images_bis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0299, 0.0496, 0.0353, 0.0723]]) tensor([ 0.0389, -0.2293,  0.0359,  0.3186])\n",
      "tensor([[0.0299, 0.0496, 0.0353, 0.0723]]) tensor([ 0.0343, -0.4249,  0.0423,  0.6224])\n",
      "tensor([[ 0.0246, -0.2409,  0.0376,  0.4888]]) tensor([ 0.0258, -0.6206,  0.0547,  0.9281])\n",
      "tensor([[-0.0030, -0.4574,  0.0725,  0.8211]]) tensor([ 0.0134, -0.4262,  0.0733,  0.6531])\n",
      "tensor([[-0.0075, -0.3330,  0.0781,  0.6277]]) tensor([ 0.0048, -0.6223,  0.0864,  0.9680])\n",
      "tensor([[-0.0099, -0.3732,  0.0813,  0.6977]]) tensor([-0.0076, -0.8185,  0.1057,  1.2865])\n",
      "tensor([[-0.0377, -0.6290,  0.1080,  1.1038]]) tensor([-0.0240, -0.6248,  0.1315,  1.0287])\n",
      "tensor([[-0.0493, -0.5892,  0.1321,  1.0962]]) tensor([-0.0365, -0.4317,  0.1520,  0.7800])\n",
      "tensor([[-0.0617, -0.3433,  0.1516,  0.8059]]) tensor([-0.0451, -0.6285,  0.1676,  1.1164])\n",
      "tensor([[-0.0767, -0.5178,  0.1705,  1.1121]]) tensor([-0.0577, -0.8254,  0.1900,  1.4566])\n",
      "tensor([[ 0.0286, -0.0267,  0.0286,  0.1492]]) tensor([ 0.0384,  0.2369,  0.0298, -0.3111])\n",
      "tensor([[ 0.0335,  0.2746,  0.0182, -0.3211]]) tensor([ 0.0432,  0.0414,  0.0236, -0.0092])\n",
      "tensor([[ 0.0411,  0.1214,  0.0125, -0.0573]]) tensor([ 0.0440,  0.2362,  0.0234, -0.2943])\n",
      "tensor([[ 0.0461,  0.3158,  0.0122, -0.3282]]) tensor([ 0.0487,  0.4310,  0.0175, -0.5795])\n",
      "tensor([[ 0.0547,  0.4291, -0.0127, -0.5427]]) tensor([ 0.0573,  0.2356,  0.0059, -0.2814])\n",
      "tensor([[ 0.0598,  0.1114, -0.0113, -0.0501]]) tensor([0.0621, 0.0404, 0.0003, 0.0132])\n",
      "tensor([[ 0.0571,  0.0031, -0.0109,  0.0623]]) tensor([ 0.0629,  0.2355,  0.0006, -0.2794])\n",
      "tensor([[ 0.0615,  0.2904, -0.0179, -0.3809]]) tensor([ 0.0676,  0.0404, -0.0050,  0.0134])\n",
      "tensor([[ 0.0652,  0.0928, -0.0167, -0.0558]]) tensor([ 0.0684, -0.1547, -0.0048,  0.3045])\n",
      "tensor([[ 0.0638,  0.0025, -0.0046,  0.0911]]) tensor([0.0653, 0.0405, 0.0013, 0.0103])\n",
      "tensor([[ 0.0628, -0.0110, -0.0072,  0.0868]]) tensor([ 0.0661, -0.1546,  0.0015,  0.3034])\n",
      "tensor([[ 0.0486, -0.2793,  0.0042,  0.4780]]) tensor([ 0.0630, -0.3498,  0.0076,  0.5966])\n",
      "tensor([[ 0.0543, -0.1305,  0.0100,  0.2650]]) tensor([ 0.0560, -0.5450,  0.0195,  0.8917])\n",
      "tensor([[ 0.0225, -0.4515,  0.0437,  0.7831]]) tensor([ 0.0451, -0.7404,  0.0374,  1.1904])\n",
      "tensor([[ 2.3914e-04, -5.6258e-01,  6.6716e-02,  9.5413e-01]]) tensor([ 0.0303, -0.5458,  0.0612,  0.9097])\n",
      "tensor([[ 0.0087, -0.2927,  0.0735,  0.5735]]) tensor([ 0.0194, -0.3515,  0.0794,  0.6368])\n",
      "tensor([[-4.9227e-04, -2.5010e-01,  9.0898e-02,  5.7275e-01]]) tensor([ 0.0124, -0.5476,  0.0921,  0.9534])\n",
      "tensor([[-0.0025, -0.3075,  0.0970,  0.6918]]) tensor([ 0.0014, -0.3539,  0.1112,  0.6910])\n",
      "tensor([[-0.0112, -0.3014,  0.1126,  0.7137]]) tensor([-0.0057, -0.1605,  0.1250,  0.4353])\n",
      "tensor([[-0.0171, -0.2574,  0.1186,  0.6645]]) tensor([-0.0089,  0.0327,  0.1337,  0.1845])\n",
      "tensor([[-0.0231, -0.0309,  0.1334,  0.3965]]) tensor([-0.0082, -0.1641,  0.1374,  0.5162])\n",
      "tensor([[-0.0181, -0.0137,  0.1319,  0.4203]]) tensor([-0.0115,  0.0289,  0.1477,  0.2698])\n",
      "tensor([[-0.0155,  0.0630,  0.1283,  0.3198]]) tensor([-0.0109, -0.1680,  0.1531,  0.6051])\n",
      "tensor([[-0.0126,  0.0773,  0.1335,  0.3302]]) tensor([-0.0143,  0.0247,  0.1652,  0.3643])\n",
      "tensor([[-0.0094,  0.0569,  0.1426,  0.4050]]) tensor([-0.0138,  0.2171,  0.1725,  0.1280])\n",
      "tensor([[-0.0062,  0.0681,  0.1491,  0.4247]]) tensor([-0.0095,  0.0200,  0.1751,  0.4697])\n",
      "tensor([[-0.0066,  0.0396,  0.1509,  0.4782]]) tensor([-0.0091, -0.1771,  0.1844,  0.8121])\n",
      "tensor([[-0.0076,  0.0078,  0.1585,  0.5485]]) tensor([-0.0126, -0.3742,  0.2007,  1.1566])\n",
      "tensor([[-0.0001,  0.0180,  0.0056, -0.0399]]) tensor([ 0.0075, -0.2131, -0.0173,  0.2286])\n",
      "tensor([[ 0.0009, -0.0027,  0.0025, -0.0128]]) tensor([ 0.0033, -0.0177, -0.0128, -0.0695])\n",
      "tensor([[ 6.2454e-05,  5.3259e-02,  6.9692e-03, -7.1872e-02]]) tensor([ 0.0029, -0.2127, -0.0141,  0.2191])\n",
      "tensor([[ 0.0023, -0.2143,  0.0062,  0.3158]]) tensor([-0.0013, -0.4076, -0.0098,  0.5073])\n",
      "tensor([[-0.0072, -0.3265,  0.0188,  0.4748]]) tensor([-9.4857e-03, -6.0259e-01,  3.8146e-04,  7.9692e-01])\n",
      "tensor([[-0.0215, -0.3539,  0.0338,  0.5216]]) tensor([-0.0215, -0.7977,  0.0163,  1.0897])\n",
      "tensor([[-0.0329, -0.5643,  0.0432,  0.8464]]) tensor([-0.0375, -0.9930,  0.0381,  1.3875])\n",
      "tensor([[-0.0760, -0.8226,  0.0877,  1.2678]]) tensor([-0.0574, -0.7984,  0.0659,  1.1070])\n",
      "tensor([[-0.0954, -0.6659,  0.1068,  1.0534]]) tensor([-0.0733, -0.9943,  0.0880,  1.4196])\n",
      "tensor([[-0.1005, -0.6812,  0.1075,  1.0663]]) tensor([-0.0932, -1.1904,  0.1164,  1.7384])\n",
      "tensor([[-0.1132, -0.9165,  0.1248,  1.4036]]) tensor([-0.1170, -0.9968,  0.1512,  1.4841])\n",
      "tensor([[-0.1447, -0.8250,  0.1657,  1.3229]]) tensor([-0.1370, -0.8038,  0.1808,  1.2422])\n",
      "tensor([[-0.1583, -0.7316,  0.1763,  1.2033]]) tensor([-0.1530, -0.6114,  0.2057,  1.0111])\n",
      "8.720455790637061\n"
     ]
    }
   ],
   "source": [
    "# 2 Afficher ce que prédit le modèle vs les vraies observations\n",
    "\n",
    "model.eval()\n",
    "total = 0 # Loss totale\n",
    "for images, states in zip(data_images_bis, data_states_bis):\n",
    "    with torch.no_grad():\n",
    "        print(model(images.unsqueeze(0)),states)\n",
    "        total += np.sum(np.array((model(images.unsqueeze(0))-states)**2))\n",
    "\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'gotta_test_that_one.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 2 : GREY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "sequence_length = 4  # Number of images in each sequence\n",
    "num_episodes = 1000   # Number of episodes for data collection\n",
    "                     # ENCORE UNE FOIS best_grey_model A ETE FAIT A 300, VOIR AVEC 1000 ? \n",
    "\n",
    "# Environment Setup\n",
    "env = gym.make('CartPole-v1')\n",
    "data_images = []\n",
    "data_states = []\n",
    "\n",
    "# Transformer les images et les convertir en tenseurs\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((60, 135)),\n",
    "    transforms.Grayscale()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cart location for centering image crop\n",
    "def get_cart_location(screen_width):\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "# Cropping, downsampling (and Grayscaling) image\n",
    "def get_screen():\n",
    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width * 0.6)\n",
    "    cart_location = get_cart_location(screen_width)\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return transform(screen.transpose(1,2,0)).squeeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hatem\\AppData\\Local\\Temp\\ipykernel_16112\\1955873250.py:28: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  data_states = torch.tensor(data_states, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Collecter les données\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    observation = env.reset()\n",
    "    images = [torch.zeros(60, 135) for _ in range(4)]\n",
    "    for t in range(1000):\n",
    "        img = env.render(mode='rgb_array')\n",
    "        tensor_image = get_screen()\n",
    "        # if t == 5:\n",
    "        #     fig, axes = plt.subplots(1, 2, figsize=(15, 5))  # Crée une figure et des axes avec 1 ligne et 'n_images' colonnes\n",
    "        #     axes[0].imshow(tensor_image)\n",
    "        #     axes[1].imshow(tensor_image_bis)\n",
    "\n",
    "        images.append(tensor_image)\n",
    "        \n",
    "        sequence_tensor = torch.stack(images[-sequence_length:], dim=0)\n",
    "        data_images.append(sequence_tensor)\n",
    "        data_states.append(observation)\n",
    "\n",
    "        action = env.action_space.sample()  \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "# env.close()\n",
    "\n",
    "# Convert data_states to a tensor \n",
    "data_states = torch.tensor(data_states, dtype=torch.float32)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = TensorDataset(torch.stack(data_images), data_states)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CartPoleCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(4, 16, kernel_size=5, stride=1, padding=2),  # Input: 4 gray images, output: 16 channels, 60x135\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                 # Output size: ? 30x67\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                 # Output size: ? 15x33\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)                  # Output size: ? 7x16\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 16, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,4)    # x, x_dot, theta, theta_dot\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output for the fully connected layers\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "# Instanciation du modèle\n",
    "model = CartPoleCNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 4]) torch.Size([10, 4, 60, 135])\n",
      "tensor([-0.0964,  0.2193,  0.1114, -0.0642])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x3000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for images, labels in dataloader:\n",
    "    print(labels.shape,images.shape)\n",
    "    fig,axes = plt.subplots(1,4,figsize = (20,30))\n",
    "    for i in range(4):\n",
    "        axes[i].imshow(images[0][i])\n",
    "    print(labels[0])\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.040232785046100616\n",
      "Epoch 2, Loss: 0.04797070100903511\n",
      "Epoch 3, Loss: 0.025605758652091026\n",
      "Epoch 4, Loss: 0.07242388278245926\n",
      "Epoch 5, Loss: 0.06184414401650429\n",
      "Epoch 6, Loss: 0.060747385025024414\n",
      "Epoch 7, Loss: 0.05459335073828697\n",
      "Epoch 8, Loss: 0.05036822333931923\n",
      "Epoch 9, Loss: 0.04405743256211281\n",
      "Epoch 10, Loss: 0.037607256323099136\n",
      "Epoch 11, Loss: 0.03315708413720131\n",
      "Epoch 12, Loss: 0.06277186423540115\n",
      "Epoch 13, Loss: 0.018332751467823982\n",
      "Epoch 14, Loss: 0.04693877696990967\n",
      "Epoch 15, Loss: 0.034181419759988785\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "num_epochs = 15 # Peut-être avec plus d'epoch on obtiendrait un meilleur résultat ? jsp\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for images, states in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, states)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voir encore mieux ce que ça donne\n",
    "\n",
    "# 1 : On collecte des images du cartpole (heuristique : random)\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "data_images_bis = []\n",
    "data_states_bis = []\n",
    "\n",
    "for episode in range(3):\n",
    "    observation_bis = env.reset()\n",
    "    images_bis = []\n",
    "    for t in range(1000):\n",
    "        img = env.render(mode='rgb_array')\n",
    "        tensor_image = transform(img).squeeze(0)  # Transform image immediately\n",
    "        images_bis.append(tensor_image)\n",
    "        \n",
    "        if len(images_bis) >= sequence_length:\n",
    "            # Stack the last sequence_length images to form a single sequence tensor\n",
    "            sequence_tensor = torch.stack(images_bis[-sequence_length:], dim=0)\n",
    "            data_images_bis.append(sequence_tensor)\n",
    "            data_states_bis.append(observation)\n",
    "        \n",
    "        action = env.action_space.sample()   # Use the heuristic policy\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "data_states_bis = torch.tensor(data_states_bis, dtype=torch.float32)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "data_images_bis = torch.stack(data_images_bis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0094,  0.1349,  0.0035, -0.1393]]) tensor([ 0.0172,  0.1797,  0.0396, -0.2858])\n",
      "tensor([[ 0.0061,  0.1454,  0.0386, -0.0396]]) tensor([ 0.0208,  0.3742,  0.0339, -0.5657])\n",
      "tensor([[0.0024, 0.1497, 0.0562, 0.0115]]) tensor([ 0.0283,  0.1787,  0.0226, -0.2625])\n",
      "tensor([[ 0.0072,  0.1407,  0.0330, -0.0509]]) tensor([ 0.0318,  0.3735,  0.0173, -0.5480])\n",
      "tensor([[ 0.0082,  0.1489,  0.0164, -0.1109]]) tensor([ 0.0393,  0.5683,  0.0064, -0.8352])\n",
      "tensor([[ 0.0052,  0.1780,  0.0446, -0.0458]]) tensor([ 0.0507,  0.7634, -0.0103, -1.1258])\n",
      "tensor([[-0.0044,  0.1806,  0.0924,  0.0875]]) tensor([ 0.0659,  0.5684, -0.0328, -0.8364])\n",
      "tensor([[-0.0038,  0.1718,  0.0887,  0.0902]]) tensor([ 0.0773,  0.7639, -0.0496, -1.1392])\n",
      "tensor([[0.0029, 0.1395, 0.0548, 0.0220]]) tensor([ 0.0926,  0.9597, -0.0724, -1.4470])\n",
      "tensor([[0.0017, 0.1368, 0.0543, 0.0210]]) tensor([ 0.1118,  1.1556, -0.1013, -1.7614])\n",
      "tensor([[ 0.0098,  0.0888,  0.0067, -0.0695]]) tensor([ 0.1349,  0.9618, -0.1365, -1.5019])\n",
      "tensor([[ 0.0093,  0.0806,  0.0113, -0.0436]]) tensor([ 0.1541,  1.1583, -0.1666, -1.8339])\n",
      "tensor([[ 0.0111,  0.1176, -0.0089, -0.1567]]) tensor([ 0.1773,  1.3548, -0.2032, -2.1734])\n",
      "tensor([[-0.0064,  0.1930,  0.0925,  0.0518]]) tensor([ 0.0238, -0.5625,  0.0627,  0.9394])\n",
      "tensor([[-0.0036,  0.2090,  0.0847,  0.0137]]) tensor([ 0.0126, -0.3683,  0.0815,  0.6671])\n",
      "tensor([[ 0.0072,  0.2119,  0.0405, -0.1129]]) tensor([ 0.0052, -0.5645,  0.0948,  0.9843])\n",
      "tensor([[ 0.0136,  0.2822,  0.0290, -0.2389]]) tensor([-0.0061, -0.3707,  0.1145,  0.7228])\n",
      "tensor([[ 0.0096,  0.3679,  0.0628, -0.2349]]) tensor([-0.0135, -0.1774,  0.1290,  0.4683])\n",
      "tensor([[ 0.0090,  0.3613,  0.0709, -0.2013]]) tensor([-0.0170, -0.3741,  0.1384,  0.7987])\n",
      "tensor([[ 0.0104,  0.3340,  0.0541, -0.2088]]) tensor([-0.0245, -0.5708,  0.1543,  1.1315])\n",
      "tensor([[ 0.0142,  0.2162,  0.0142, -0.2015]]) tensor([-0.0359, -0.3780,  0.1770,  0.8909])\n",
      "tensor([[ 0.0130,  0.3069,  0.0360, -0.2535]]) tensor([-0.0435, -0.1856,  0.1948,  0.6587])\n",
      "tensor([[ 0.0120,  0.3778,  0.0601, -0.2512]]) tensor([-0.0472,  0.0063,  0.2080,  0.4331])\n",
      "tensor([[ 0.0099,  0.3350,  0.0581, -0.1987]]) tensor([-0.0026,  0.1983, -0.0179, -0.3136])\n",
      "tensor([[ 0.0077,  0.3048,  0.0599, -0.1533]]) tensor([ 0.0014,  0.3937, -0.0242, -0.6118])\n",
      "tensor([[ 0.0060,  0.2793,  0.0602, -0.1188]]) tensor([ 0.0092,  0.5891, -0.0364, -0.9120])\n",
      "tensor([[ 0.0010,  0.2440,  0.0746, -0.0370]]) tensor([ 0.0210,  0.3945, -0.0546, -0.6310])\n",
      "tensor([[-0.0015,  0.2103,  0.0810,  0.0175]]) tensor([ 0.0289,  0.5904, -0.0673, -0.9404])\n",
      "tensor([[-0.0061,  0.1665,  0.0997,  0.1285]]) tensor([ 0.0407,  0.7863, -0.0861, -1.2534])\n",
      "tensor([[ 0.0080,  0.1355,  0.0249, -0.0692]]) tensor([ 0.0564,  0.9824, -0.1111, -1.5718])\n",
      "tensor([[-0.0097,  0.1722,  0.1341,  0.2068]]) tensor([ 0.0761,  0.7888, -0.1426, -1.3157])\n",
      "tensor([[-0.0011,  0.1352,  0.0778,  0.1058]]) tensor([ 0.0919,  0.5957, -0.1689, -1.0709])\n",
      "tensor([[0.0042, 0.1134, 0.0420, 0.0025]]) tensor([ 0.1038,  0.7927, -0.1903, -1.4114])\n",
      "54.2442552279681\n"
     ]
    }
   ],
   "source": [
    "# 2 Afficher ce que prédit le modèle vs les vraies observations\n",
    "\n",
    "model.eval()\n",
    "total = 0 # Loss totale\n",
    "for images, states in zip(data_images_bis, data_states_bis):\n",
    "    with torch.no_grad():\n",
    "        print(model(images.unsqueeze(0)),states)\n",
    "        total += np.sum(np.array((model(images.unsqueeze(0))-states)**2))\n",
    "\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'cnn_grey_1000.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST 3 : LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleCNN_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CartPoleCNN_LSTM, self).__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1), # Traite chaque image individuellement\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Flatten() # Flatten les sorties pour préparation à l'LSTM\n",
    "        )\n",
    "        self.lstm = nn.LSTM(input_size=16*30*67, hidden_size=128, batch_first=True) \n",
    "        self.fc_layer = nn.Linear(128, 4) # Output x, x_dot, theta, theta_dot\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, H, W = x.shape\n",
    "        c_in = x.view(batch_size * seq_len, 1, H, W) # Remodeler pour CNN\n",
    "        c_out = self.conv_layer(c_in)\n",
    "        r_in = c_out.view(batch_size, seq_len, -1) # Remodeler pour LSTM\n",
    "        lstm_out, (hn, cn) = self.lstm(r_in)\n",
    "        out = self.fc_layer(hn[-1]) # Utiliser le dernier hidden state\n",
    "        return out\n",
    "\n",
    "    \n",
    "# Instanciation du modèle\n",
    "model = CartPoleCNN_LSTM()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cart location for centering image crop\n",
    "def get_cart_location(screen_width):\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "# Cropping, downsampling (and Grayscaling) image\n",
    "def get_screen():\n",
    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width * 0.6)\n",
    "    cart_location = get_cart_location(screen_width)\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return transform(screen.transpose(1,2,0)).squeeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "sequence_length = 4  # Number of images in each sequence\n",
    "num_episodes = 300   # Number of episodes for data collection\n",
    "                     # ENCORE UNE FOIS best_grey_model A ETE FAIT A 300, VOIR AVEC 1000 ? \n",
    "\n",
    "# Environment Setup\n",
    "env = gym.make('CartPole-v1')\n",
    "data_images = []\n",
    "data_states = []\n",
    "\n",
    "# Transformer les images et les convertir en tenseurs\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((60, 135)),\n",
    "    transforms.Grayscale()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hatem\\AppData\\Local\\Temp\\ipykernel_14524\\1740750942.py:26: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  data_states = torch.tensor(data_states, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "for episode in range(num_episodes):\n",
    "    observation = env.reset()\n",
    "    images = [torch.zeros(60, 135) for _ in range(4)]\n",
    "    for t in range(1000):\n",
    "        img = env.render(mode='rgb_array')\n",
    "        tensor_image = get_screen()\n",
    "        # if t == 5:\n",
    "        #     fig, axes = plt.subplots(1, 2, figsize=(15, 5))  # Crée une figure et des axes avec 1 ligne et 'n_images' colonnes\n",
    "        #     axes[0].imshow(tensor_image)\n",
    "        #     axes[1].imshow(tensor_image_bis)\n",
    "\n",
    "        images.append(tensor_image)\n",
    "        \n",
    "        sequence_tensor = torch.stack(images[-sequence_length:], dim=0)\n",
    "        data_images.append(sequence_tensor)\n",
    "        data_states.append(observation)\n",
    "\n",
    "        action = env.action_space.sample()  \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Convert data_states to a tensor \n",
    "data_states = torch.tensor(data_states, dtype=torch.float32)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = TensorDataset(torch.stack(data_images), data_states)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1642354428768158\n",
      "Epoch 2, Loss: 0.20948323607444763\n",
      "Epoch 3, Loss: 0.18387961387634277\n",
      "Epoch 4, Loss: 0.17517533898353577\n",
      "Epoch 5, Loss: 0.34073683619499207\n",
      "Epoch 6, Loss: 0.40426793694496155\n",
      "Epoch 7, Loss: 0.13599316775798798\n",
      "Epoch 8, Loss: 0.329766184091568\n",
      "Epoch 9, Loss: 0.277312695980072\n",
      "Epoch 10, Loss: 0.3386182188987732\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "num_epochs = 10 \n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for images, states in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, states)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "torch.save(model.state_dict(), 'cnn_lstm_grey.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
