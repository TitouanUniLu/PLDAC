{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F  # Ensure this import is added\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import gym\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 1 : RGB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "sequence_length = 4  # Number of images in each sequence\n",
    "num_episodes = 300   ### JOUER AVEC CE PARAMETRE POUR AMELIORER LE MODELE : TESTER AVEC 1000 SERAIT COOL\n",
    "\n",
    "# Environment Setup\n",
    "env = gym.make('CartPole-v1')\n",
    "data_images = []\n",
    "data_states = []\n",
    "\n",
    "# Transformation for images\n",
    "transform = transforms.Compose([transforms.ToPILImage(), \n",
    "                    transforms.Resize(60, interpolation=Image.LANCZOS),\n",
    "                    transforms.ToTensor()])\n",
    "\n",
    "\n",
    "# Cart location for centering image crop\n",
    "def get_cart_location(screen_width):\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "# Cropping, downsampling (and Grayscaling) image\n",
    "def get_screen():\n",
    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width * 0.6)\n",
    "    cart_location = get_cart_location(screen_width)\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    screen = torch.from_numpy(screen)\n",
    "    return transform(screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data Collection using Heuristic Policy\n",
    "for episode in range(num_episodes):\n",
    "    observation = env.reset()\n",
    "    images = [torch.zeros(3, 60, 135) for _ in range(4)]\n",
    "           \n",
    "    for t in range(1000):\n",
    "        tensor_image = get_screen()  # Transform image immediately\n",
    "        # if t==4:\n",
    "        #     print(tensor_image.shape)\n",
    "        #     plt.imshow(np.array(tensor_image.permute(1,2,0)))\n",
    "        images.append(tensor_image)\n",
    "        \n",
    "        if len(images) >= sequence_length:\n",
    "            # Stack the last sequence_length images to form a single sequence tensor\n",
    "            sequence_tensor = torch.stack(images[-sequence_length:], dim=0).permute(1, 0, 2, 3)\n",
    "            data_images.append(sequence_tensor)\n",
    "            data_states.append(observation)\n",
    "        \n",
    "        action = env.action_space.sample()  \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Convert data_states to a tensor\n",
    "data_states = torch.tensor(data_states, dtype=torch.float32)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = TensorDataset(torch.stack(data_images), data_states)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv3d(3, 16, kernel_size=(3, 3, 3), stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2)),\n",
    "            nn.Conv3d(16, 32, kernel_size=(3, 3, 3), stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2)),\n",
    "        )\n",
    "        # Correctly calculate the input size for the linear layer based on the output from conv_layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(63360, 128),  # Adjusted based on actual output size\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 4)  # Predicting 4 state variables\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor for the fully connected layer\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.07291144877672195\n",
      "Epoch 2, Loss: 0.0323127880692482\n",
      "Epoch 3, Loss: 0.0680825263261795\n",
      "Epoch 4, Loss: 0.014706824906170368\n",
      "Epoch 5, Loss: 0.011884740553796291\n",
      "Epoch 6, Loss: 0.06468658894300461\n",
      "Epoch 7, Loss: 0.033202096819877625\n",
      "Epoch 8, Loss: 0.03171757236123085\n",
      "Epoch 9, Loss: 0.04470457136631012\n",
      "Epoch 10, Loss: 0.07456962764263153\n"
     ]
    }
   ],
   "source": [
    "# Model instantiation and training setup\n",
    "model = CNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 10 # Peut-être avec plus d'epoch on obtiendrait un meilleur résultat ? jsp\n",
    "for epoch in range(num_epochs):\n",
    "    for images, states in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, states)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'cartpole_cnn_rgb.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 4, 60, 135]) torch.Size([10, 4])\n",
      "tensor([[ 0.0022,  0.0096, -0.0036,  0.0202]], grad_fn=<AddmmBackward0>) tensor([ 0.0405,  0.0406,  0.0216, -0.0420])\n"
     ]
    }
   ],
   "source": [
    "# Voir un peu ce que ça donne\n",
    "\n",
    "model.eval()\n",
    "for images, states in dataloader:\n",
    "    print(images.shape,states.shape)\n",
    "    print(model(images[0].unsqueeze(0)),states[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voir encore mieux ce que ça donne\n",
    "\n",
    "# 1 : On collecte des images du cartpole (heuristique : random)\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "data_images_bis = []\n",
    "data_states_bis = []\n",
    "\n",
    "for episode in range(3):\n",
    "    observation_bis = env.reset()\n",
    "    images_bis = []\n",
    "    for t in range(1000):\n",
    "        img = env.render(mode='rgb_array')\n",
    "        img_pil = Image.fromarray(img)\n",
    "        tensor_image = transform(img_pil)  # Transform image immediately\n",
    "        images_bis.append(tensor_image)\n",
    "        \n",
    "        if len(images_bis) >= sequence_length:\n",
    "            # Stack the last sequence_length images to form a single sequence tensor\n",
    "            sequence_tensor = torch.stack(images_bis[-sequence_length:], dim=0).permute(1, 0, 2, 3)\n",
    "            data_images_bis.append(sequence_tensor)\n",
    "            data_states_bis.append(observation)\n",
    "        \n",
    "        action = env.action_space.sample()   # Use the heuristic policy\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "data_states_bis = torch.tensor(data_states_bis, dtype=torch.float32)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "data_images_bis = torch.stack(data_images_bis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0299, 0.0496, 0.0353, 0.0723]]) tensor([ 0.0389, -0.2293,  0.0359,  0.3186])\n",
      "tensor([[0.0299, 0.0496, 0.0353, 0.0723]]) tensor([ 0.0343, -0.4249,  0.0423,  0.6224])\n",
      "tensor([[ 0.0246, -0.2409,  0.0376,  0.4888]]) tensor([ 0.0258, -0.6206,  0.0547,  0.9281])\n",
      "tensor([[-0.0030, -0.4574,  0.0725,  0.8211]]) tensor([ 0.0134, -0.4262,  0.0733,  0.6531])\n",
      "tensor([[-0.0075, -0.3330,  0.0781,  0.6277]]) tensor([ 0.0048, -0.6223,  0.0864,  0.9680])\n",
      "tensor([[-0.0099, -0.3732,  0.0813,  0.6977]]) tensor([-0.0076, -0.8185,  0.1057,  1.2865])\n",
      "tensor([[-0.0377, -0.6290,  0.1080,  1.1038]]) tensor([-0.0240, -0.6248,  0.1315,  1.0287])\n",
      "tensor([[-0.0493, -0.5892,  0.1321,  1.0962]]) tensor([-0.0365, -0.4317,  0.1520,  0.7800])\n",
      "tensor([[-0.0617, -0.3433,  0.1516,  0.8059]]) tensor([-0.0451, -0.6285,  0.1676,  1.1164])\n",
      "tensor([[-0.0767, -0.5178,  0.1705,  1.1121]]) tensor([-0.0577, -0.8254,  0.1900,  1.4566])\n",
      "tensor([[ 0.0286, -0.0267,  0.0286,  0.1492]]) tensor([ 0.0384,  0.2369,  0.0298, -0.3111])\n",
      "tensor([[ 0.0335,  0.2746,  0.0182, -0.3211]]) tensor([ 0.0432,  0.0414,  0.0236, -0.0092])\n",
      "tensor([[ 0.0411,  0.1214,  0.0125, -0.0573]]) tensor([ 0.0440,  0.2362,  0.0234, -0.2943])\n",
      "tensor([[ 0.0461,  0.3158,  0.0122, -0.3282]]) tensor([ 0.0487,  0.4310,  0.0175, -0.5795])\n",
      "tensor([[ 0.0547,  0.4291, -0.0127, -0.5427]]) tensor([ 0.0573,  0.2356,  0.0059, -0.2814])\n",
      "tensor([[ 0.0598,  0.1114, -0.0113, -0.0501]]) tensor([0.0621, 0.0404, 0.0003, 0.0132])\n",
      "tensor([[ 0.0571,  0.0031, -0.0109,  0.0623]]) tensor([ 0.0629,  0.2355,  0.0006, -0.2794])\n",
      "tensor([[ 0.0615,  0.2904, -0.0179, -0.3809]]) tensor([ 0.0676,  0.0404, -0.0050,  0.0134])\n",
      "tensor([[ 0.0652,  0.0928, -0.0167, -0.0558]]) tensor([ 0.0684, -0.1547, -0.0048,  0.3045])\n",
      "tensor([[ 0.0638,  0.0025, -0.0046,  0.0911]]) tensor([0.0653, 0.0405, 0.0013, 0.0103])\n",
      "tensor([[ 0.0628, -0.0110, -0.0072,  0.0868]]) tensor([ 0.0661, -0.1546,  0.0015,  0.3034])\n",
      "tensor([[ 0.0486, -0.2793,  0.0042,  0.4780]]) tensor([ 0.0630, -0.3498,  0.0076,  0.5966])\n",
      "tensor([[ 0.0543, -0.1305,  0.0100,  0.2650]]) tensor([ 0.0560, -0.5450,  0.0195,  0.8917])\n",
      "tensor([[ 0.0225, -0.4515,  0.0437,  0.7831]]) tensor([ 0.0451, -0.7404,  0.0374,  1.1904])\n",
      "tensor([[ 2.3914e-04, -5.6258e-01,  6.6716e-02,  9.5413e-01]]) tensor([ 0.0303, -0.5458,  0.0612,  0.9097])\n",
      "tensor([[ 0.0087, -0.2927,  0.0735,  0.5735]]) tensor([ 0.0194, -0.3515,  0.0794,  0.6368])\n",
      "tensor([[-4.9227e-04, -2.5010e-01,  9.0898e-02,  5.7275e-01]]) tensor([ 0.0124, -0.5476,  0.0921,  0.9534])\n",
      "tensor([[-0.0025, -0.3075,  0.0970,  0.6918]]) tensor([ 0.0014, -0.3539,  0.1112,  0.6910])\n",
      "tensor([[-0.0112, -0.3014,  0.1126,  0.7137]]) tensor([-0.0057, -0.1605,  0.1250,  0.4353])\n",
      "tensor([[-0.0171, -0.2574,  0.1186,  0.6645]]) tensor([-0.0089,  0.0327,  0.1337,  0.1845])\n",
      "tensor([[-0.0231, -0.0309,  0.1334,  0.3965]]) tensor([-0.0082, -0.1641,  0.1374,  0.5162])\n",
      "tensor([[-0.0181, -0.0137,  0.1319,  0.4203]]) tensor([-0.0115,  0.0289,  0.1477,  0.2698])\n",
      "tensor([[-0.0155,  0.0630,  0.1283,  0.3198]]) tensor([-0.0109, -0.1680,  0.1531,  0.6051])\n",
      "tensor([[-0.0126,  0.0773,  0.1335,  0.3302]]) tensor([-0.0143,  0.0247,  0.1652,  0.3643])\n",
      "tensor([[-0.0094,  0.0569,  0.1426,  0.4050]]) tensor([-0.0138,  0.2171,  0.1725,  0.1280])\n",
      "tensor([[-0.0062,  0.0681,  0.1491,  0.4247]]) tensor([-0.0095,  0.0200,  0.1751,  0.4697])\n",
      "tensor([[-0.0066,  0.0396,  0.1509,  0.4782]]) tensor([-0.0091, -0.1771,  0.1844,  0.8121])\n",
      "tensor([[-0.0076,  0.0078,  0.1585,  0.5485]]) tensor([-0.0126, -0.3742,  0.2007,  1.1566])\n",
      "tensor([[-0.0001,  0.0180,  0.0056, -0.0399]]) tensor([ 0.0075, -0.2131, -0.0173,  0.2286])\n",
      "tensor([[ 0.0009, -0.0027,  0.0025, -0.0128]]) tensor([ 0.0033, -0.0177, -0.0128, -0.0695])\n",
      "tensor([[ 6.2454e-05,  5.3259e-02,  6.9692e-03, -7.1872e-02]]) tensor([ 0.0029, -0.2127, -0.0141,  0.2191])\n",
      "tensor([[ 0.0023, -0.2143,  0.0062,  0.3158]]) tensor([-0.0013, -0.4076, -0.0098,  0.5073])\n",
      "tensor([[-0.0072, -0.3265,  0.0188,  0.4748]]) tensor([-9.4857e-03, -6.0259e-01,  3.8146e-04,  7.9692e-01])\n",
      "tensor([[-0.0215, -0.3539,  0.0338,  0.5216]]) tensor([-0.0215, -0.7977,  0.0163,  1.0897])\n",
      "tensor([[-0.0329, -0.5643,  0.0432,  0.8464]]) tensor([-0.0375, -0.9930,  0.0381,  1.3875])\n",
      "tensor([[-0.0760, -0.8226,  0.0877,  1.2678]]) tensor([-0.0574, -0.7984,  0.0659,  1.1070])\n",
      "tensor([[-0.0954, -0.6659,  0.1068,  1.0534]]) tensor([-0.0733, -0.9943,  0.0880,  1.4196])\n",
      "tensor([[-0.1005, -0.6812,  0.1075,  1.0663]]) tensor([-0.0932, -1.1904,  0.1164,  1.7384])\n",
      "tensor([[-0.1132, -0.9165,  0.1248,  1.4036]]) tensor([-0.1170, -0.9968,  0.1512,  1.4841])\n",
      "tensor([[-0.1447, -0.8250,  0.1657,  1.3229]]) tensor([-0.1370, -0.8038,  0.1808,  1.2422])\n",
      "tensor([[-0.1583, -0.7316,  0.1763,  1.2033]]) tensor([-0.1530, -0.6114,  0.2057,  1.0111])\n",
      "8.720455790637061\n"
     ]
    }
   ],
   "source": [
    "# 2 Afficher ce que prédit le modèle vs les vraies observations\n",
    "\n",
    "model.eval()\n",
    "total = 0 # Loss totale\n",
    "for images, states in zip(data_images_bis, data_states_bis):\n",
    "    with torch.no_grad():\n",
    "        print(model(images.unsqueeze(0)),states)\n",
    "        total += np.sum(np.array((model(images.unsqueeze(0))-states)**2))\n",
    "\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'gotta_test_that_one.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 2 : GREY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "sequence_length = 4  # Number of images in each sequence\n",
    "num_episodes = 300   # Number of episodes for data collection\n",
    "                     # ENCORE UNE FOIS best_grey_model A ETE FAIT A 300, VOIR AVEC 1000 ? \n",
    "\n",
    "# Environment Setup\n",
    "env = gym.make('CartPole-v1')\n",
    "data_images = []\n",
    "data_states = []\n",
    "\n",
    "# Transformer les images et les convertir en tenseurs\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((60, 135)),\n",
    "    transforms.Grayscale()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cart location for centering image crop\n",
    "def get_cart_location(screen_width):\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "# Cropping, downsampling (and Grayscaling) image\n",
    "def get_screen():\n",
    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width * 0.6)\n",
    "    cart_location = get_cart_location(screen_width)\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return transform(screen.transpose(1,2,0)).squeeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Collecter les données\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    observation = env.reset()\n",
    "    images = [torch.zeros(60, 135) for _ in range(4)]\n",
    "    for t in range(1000):\n",
    "        img = env.render(mode='rgb_array')\n",
    "        tensor_image = get_screen()\n",
    "        # if t == 5:\n",
    "        #     fig, axes = plt.subplots(1, 2, figsize=(15, 5))  # Crée une figure et des axes avec 1 ligne et 'n_images' colonnes\n",
    "        #     axes[0].imshow(tensor_image)\n",
    "        #     axes[1].imshow(tensor_image_bis)\n",
    "\n",
    "        images.append(tensor_image)\n",
    "        \n",
    "        sequence_tensor = torch.stack(images[-sequence_length:], dim=0)\n",
    "        data_images.append(sequence_tensor)\n",
    "        data_states.append(observation)\n",
    "\n",
    "        action = env.action_space.sample()  \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "# env.close()\n",
    "\n",
    "# Convert data_states to a tensor\n",
    "data_states = torch.tensor(data_states, dtype=torch.float32)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = TensorDataset(torch.stack(data_images), data_states)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CartPoleCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(4, 16, kernel_size=5, stride=1, padding=2),  # Input: 4 gray images, output: 16 channels, 60x135\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                 # Output size: ? 30x67\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                 # Output size: ? 15x33\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)                  # Output size: ? 7x16\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 16, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,4)    # x, x_dot, theta, theta_dot\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output for the fully connected layers\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "# Instanciation du modèle\n",
    "model = CartPoleCNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 4]) torch.Size([10, 4, 60, 135])\n",
      "tensor([-0.1114, -1.3923,  0.1666,  2.2089])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x3000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for images, labels in dataloader:\n",
    "    print(labels.shape,images.shape)\n",
    "    fig,axes = plt.subplots(1,4,figsize = (20,30))\n",
    "    for i in range(4):\n",
    "        axes[i].imshow(images[0][i])\n",
    "    print(labels[0])\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4276227056980133\n",
      "Epoch 2, Loss: 0.0886773020029068\n",
      "Epoch 3, Loss: 0.04507780820131302\n",
      "Epoch 4, Loss: 0.04576120525598526\n",
      "Epoch 5, Loss: 0.05920986086130142\n",
      "Epoch 6, Loss: 0.04326784983277321\n",
      "Epoch 7, Loss: 0.08375322818756104\n",
      "Epoch 8, Loss: 0.05944535881280899\n",
      "Epoch 9, Loss: 0.017115509137511253\n",
      "Epoch 10, Loss: 0.05878958851099014\n",
      "Epoch 11, Loss: 0.04696138575673103\n",
      "Epoch 12, Loss: 0.039336491376161575\n",
      "Epoch 13, Loss: 0.04241881147027016\n",
      "Epoch 14, Loss: 0.02552243508398533\n",
      "Epoch 15, Loss: 0.07572563737630844\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "num_epochs = 15 # Peut-être avec plus d'epoch on obtiendrait un meilleur résultat ? jsp\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for images, states in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, states)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voir encore mieux ce que ça donne\n",
    "\n",
    "# 1 : On collecte des images du cartpole (heuristique : random)\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "data_images_bis = []\n",
    "data_states_bis = []\n",
    "\n",
    "for episode in range(3):\n",
    "    observation_bis = env.reset()\n",
    "    images_bis = []\n",
    "    for t in range(1000):\n",
    "        img = env.render(mode='rgb_array')\n",
    "        tensor_image = transform(img).squeeze(0)  # Transform image immediately\n",
    "        images_bis.append(tensor_image)\n",
    "        \n",
    "        if len(images_bis) >= sequence_length:\n",
    "            # Stack the last sequence_length images to form a single sequence tensor\n",
    "            sequence_tensor = torch.stack(images_bis[-sequence_length:], dim=0)\n",
    "            data_images_bis.append(sequence_tensor)\n",
    "            data_states_bis.append(observation)\n",
    "        \n",
    "        action = env.action_space.sample()   # Use the heuristic policy\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "data_states_bis = torch.tensor(data_states_bis, dtype=torch.float32)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "data_images_bis = torch.stack(data_images_bis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0013,  0.0514,  0.0029, -0.0560]]) tensor([ 0.0065, -0.6055,  0.0538,  0.9156])\n",
      "tensor([[-0.0009,  0.0634,  0.0017, -0.0747]]) tensor([-0.0056, -0.4112,  0.0722,  0.6403])\n",
      "tensor([[-0.0012,  0.0543,  0.0026, -0.0605]]) tensor([-0.0138, -0.2171,  0.0850,  0.3712])\n",
      "tensor([[-0.0005,  0.0730,  0.0008, -0.0895]]) tensor([-0.0181, -0.0233,  0.0924,  0.1064])\n",
      "tensor([[-2.5229e-04,  8.0552e-02,  6.5079e-05, -1.0122e-01]]) tensor([-0.0186, -0.2196,  0.0945,  0.4268])\n",
      "tensor([[-0.0004,  0.0769,  0.0004, -0.0955]]) tensor([-0.0230, -0.4159,  0.1031,  0.7477])\n",
      "tensor([[-0.0009,  0.0621,  0.0018, -0.0726]]) tensor([-0.0313, -0.6123,  0.1180,  1.0709])\n",
      "tensor([[-0.0014,  0.0486,  0.0031, -0.0517]]) tensor([-0.0436, -0.4189,  0.1394,  0.8175])\n",
      "tensor([[-0.0013,  0.0516,  0.0028, -0.0563]]) tensor([-0.0519, -0.6157,  0.1558,  1.1506])\n",
      "tensor([[-0.0015,  0.0450,  0.0035, -0.0461]]) tensor([-0.0643, -0.4229,  0.1788,  0.9105])\n",
      "tensor([[-0.0013,  0.0511,  0.0029, -0.0556]]) tensor([-0.0727, -0.6199,  0.1970,  1.2536])\n",
      "tensor([[ 0.0003,  0.0955, -0.0014, -0.1245]]) tensor([ 0.0477,  0.5820, -0.0593, -0.9026])\n",
      "tensor([[ 0.0007,  0.1089, -0.0026, -0.1451]]) tensor([ 0.0594,  0.7779, -0.0774, -1.2133])\n",
      "tensor([[ 0.0015,  0.1320, -0.0049, -0.1810]]) tensor([ 0.0749,  0.9739, -0.1016, -1.5292])\n",
      "tensor([[ 0.0026,  0.1612, -0.0077, -0.2262]]) tensor([ 0.0944,  1.1701, -0.1322, -1.8518])\n",
      "tensor([[ 0.0039,  0.2009, -0.0115, -0.2877]]) tensor([ 0.1178,  1.3664, -0.1693, -2.1825])\n",
      "tensor([[ 0.0002,  0.0942, -0.0012, -0.1224]]) tensor([ 0.0533,  0.6147, -0.0212, -0.8981])\n",
      "tensor([[ 0.0015,  0.1301, -0.0047, -0.1780]]) tensor([ 0.0656,  0.8101, -0.0391, -1.1973])\n",
      "tensor([[ 0.0021,  0.1475, -0.0064, -0.2050]]) tensor([ 0.0818,  1.0058, -0.0631, -1.5020])\n",
      "tensor([[ 0.0027,  0.1642, -0.0080, -0.2309]]) tensor([ 0.1019,  0.8115, -0.0931, -1.2297])\n",
      "tensor([[ 0.0030,  0.1737, -0.0089, -0.2455]]) tensor([ 0.1181,  1.0076, -0.1177, -1.5500])\n",
      "tensor([[ 0.0040,  0.2028, -0.0117, -0.2907]]) tensor([ 0.1383,  1.2040, -0.1487, -1.8770])\n",
      "tensor([[ 0.0046,  0.2190, -0.0132, -0.3158]]) tensor([ 0.1623,  1.0107, -0.1863, -1.6339])\n",
      "40.16013644635677\n"
     ]
    }
   ],
   "source": [
    "# 2 Afficher ce que prédit le modèle vs les vraies observations\n",
    "\n",
    "model.eval()\n",
    "total = 0 # Loss totale\n",
    "for images, states in zip(data_images_bis, data_states_bis):\n",
    "    with torch.no_grad():\n",
    "        print(model(images.unsqueeze(0)),states)\n",
    "        total += np.sum(np.array((model(images.unsqueeze(0))-states)**2))\n",
    "\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'cnn_batchnorm.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
