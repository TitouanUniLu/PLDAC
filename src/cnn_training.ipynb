{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F  # Ensure this import is added\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import gymnasium \n",
    "import mon_env\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 1 : RGB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "sequence_length = 2  # Number of images in each sequence, IL FAUT TESTER AVEC 2 P-E CA CHANGE TOUT \n",
    "num_episodes = 500   ### JOUER AVEC CE PARAMETRE POUR AMELIORER LE MODELE : TESTER AVEC 1000 SERAIT COOL\n",
    "\n",
    "# Environment Setup\n",
    "env = gymnasium.make('MonCartPole-v1',render_mode=\"rgb_array\")\n",
    "data_images = []\n",
    "data_states = []\n",
    "\n",
    "# Transformation for images\n",
    "transform = transforms.Compose([transforms.ToPILImage(), \n",
    "                    transforms.Resize(60, interpolation=Image.LANCZOS),\n",
    "                    transforms.ToTensor()])\n",
    "\n",
    "\n",
    "# Cart location for centering image crop\n",
    "def get_cart_location(screen_width):\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "# Cropping, downsampling (and Grayscaling) image\n",
    "def get_screen():\n",
    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "    screen = env.render().transpose((2, 0, 1))\n",
    "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width * 0.6)\n",
    "    cart_location = get_cart_location(screen_width)\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    screen = torch.from_numpy(screen)\n",
    "    return transform(screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data Collection using Heuristic Policy\n",
    "for episode in range(num_episodes):\n",
    "    observation = env.reset()[0]\n",
    "    images = [torch.zeros(3, 60, 135) for _ in range(sequence_length)]\n",
    "           \n",
    "    for t in range(1000):\n",
    "        tensor_image = get_screen()  # Transform image immediately\n",
    "        # if t==4:\n",
    "        #     print(tensor_image.shape)\n",
    "        #     plt.imshow(np.array(tensor_image.permute(1,2,0)))\n",
    "        images.append(tensor_image)\n",
    "        \n",
    "        if len(images) >= sequence_length:\n",
    "            # Stack the last sequence_length images to form a single sequence tensor\n",
    "            sequence_tensor = torch.stack(images[-sequence_length:], dim=0).permute(1, 0, 2, 3)\n",
    "            data_images.append(sequence_tensor)\n",
    "            data_states.append(observation)\n",
    "        \n",
    "        action = env.action_space.sample()  \n",
    "        observation, reward, done, info, _ = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Convert data_states to a tensor\n",
    "data_states = torch.tensor(data_states, dtype=torch.float32)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = TensorDataset(torch.stack(data_images), data_states)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv3d(3, 16, kernel_size=(3, 3, 3), stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2)),\n",
    "            nn.Conv3d(16, 32, kernel_size=(3, 3, 3), stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2)),\n",
    "        )\n",
    "        # Correctly calculate the input size for the linear layer based on the output from conv_layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(int(63360/2), 128),  # Adjusted based on actual output size #63360/2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 4)  # Predicting 4 state variables\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor for the fully connected layer\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.0643385499715805\n",
      "Epoch 2, Loss: 0.013731541112065315\n",
      "Epoch 3, Loss: 0.028273921459913254\n",
      "Epoch 4, Loss: 0.050622984766960144\n",
      "Epoch 5, Loss: 0.014159468933939934\n",
      "Epoch 6, Loss: 0.028432276099920273\n",
      "Epoch 7, Loss: 0.03141402080655098\n",
      "Epoch 8, Loss: 0.09023912996053696\n",
      "Epoch 9, Loss: 0.002768975216895342\n",
      "Epoch 10, Loss: 0.043720047920942307\n"
     ]
    }
   ],
   "source": [
    "# Model instantiation and training setup\n",
    "model = CNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 10 # Peut-être avec plus d'epoch on obtiendrait un meilleur résultat ? jsp\n",
    "for epoch in range(num_epochs):\n",
    "    for images, states in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, states)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'cartpole_cnn_rgb_enhanced_2_img_2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 2, 60, 135]) torch.Size([10, 4])\n",
      "tensor([[0.0071, 0.0142, 0.0027, 0.0002]], grad_fn=<AddmmBackward0>) tensor([-0.0318,  0.0131, -0.0086, -0.0153])\n"
     ]
    }
   ],
   "source": [
    "# Voir un peu ce que ça donne\n",
    "\n",
    "model.eval()\n",
    "for images, states in dataloader:\n",
    "    print(images.shape,states.shape)\n",
    "    print(model(images[0].unsqueeze(0)),states[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 4]) torch.Size([10, 2, 60, 135, 3])\n",
      "torch.Size([60, 135, 3])\n",
      "torch.Size([60, 135, 3])\n",
      "tensor([ 0.0461,  0.1650,  0.0089, -0.3010])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x3000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for images, labels in dataloader:\n",
    "    images = images.permute((0,2,3,4,1))\n",
    "    print(labels.shape,images.shape)\n",
    "    fig,axes = plt.subplots(1,sequence_length,figsize = (20,30))\n",
    "    for i in range(sequence_length):\n",
    "        axes[i].imshow(images[0][i])\n",
    "        print(images[0][i].shape)\n",
    "    print(labels[0])\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\serialization.py:541\u001b[0m, in \u001b[0;36m_check_seekable\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 541\u001b[0m     \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseek\u001b[49m(f\u001b[38;5;241m.\u001b[39mtell())\n\u001b[0;32m    542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'seek'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m CNN()\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcartpole_cnn_rgb.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\serialization.py:450\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _open_buffer_writer(name_or_buffer)\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m--> 450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_buffer_reader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    452\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in mode but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\serialization.py:435\u001b[0m, in \u001b[0;36m_open_buffer_reader.__init__\u001b[1;34m(self, buffer)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, buffer):\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(buffer)\n\u001b[1;32m--> 435\u001b[0m     \u001b[43m_check_seekable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\serialization.py:544\u001b[0m, in \u001b[0;36m_check_seekable\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m    542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (io\u001b[38;5;241m.\u001b[39mUnsupportedOperation, \u001b[38;5;167;01mAttributeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 544\u001b[0m     \u001b[43mraise_err_msg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseek\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\serialization.py:537\u001b[0m, in \u001b[0;36m_check_seekable.<locals>.raise_err_msg\u001b[1;34m(patterns, e)\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n\u001b[0;32m    534\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. You can only torch.load from a file that is seekable.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    535\u001b[0m                         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please pre-load the data into a buffer like io.BytesIO and\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    536\u001b[0m                         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m try to load from it instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 537\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(msg)\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead."
     ]
    }
   ],
   "source": [
    "model = CNN()\n",
    "model = torch.load(model.state_dict(), 'cartpole_cnn_rgb.pth')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voir encore mieux ce que ça donne\n",
    "\n",
    "# 1 : On collecte des images du cartpole (heuristique : random)\n",
    "\n",
    "env = gymnasium.make('MonCartPole-v1',render_mode=\"rgb_array\")\n",
    "data_images_bis = []\n",
    "data_states_bis = []\n",
    "\n",
    "for episode in range(3):\n",
    "    observation_bis = env.reset()[0]\n",
    "    images_bis = []\n",
    "    for t in range(1000):\n",
    "        img = get_screen()\n",
    "        # img_pil = Image.fromarray(img)\n",
    "        tensor_image = transform(img)  # Transform image immediately\n",
    "        images_bis.append(tensor_image)\n",
    "        \n",
    "        if len(images_bis) >= sequence_length:\n",
    "            # Stack the last sequence_length images to form a single sequence tensor\n",
    "            sequence_tensor = torch.stack(images_bis[-sequence_length:], dim=0).permute(1, 0, 2, 3)\n",
    "            data_images_bis.append(sequence_tensor)\n",
    "            data_states_bis.append(observation)\n",
    "        \n",
    "        action = env.action_space.sample()   # Use the heuristic policy\n",
    "        observation, reward, done, info, _ = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "data_states_bis = torch.tensor(data_states_bis, dtype=torch.float32)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "data_images_bis = torch.stack(data_images_bis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.7380e-02, -1.8578e-02,  1.2642e-03,  2.5662e-05]]) tensor([-0.0419,  0.1784,  0.0120, -0.3325])\n",
      "tensor([[-0.0307,  0.1369,  0.0111, -0.2131]]) tensor([-0.0383, -0.0169,  0.0054, -0.0361])\n",
      "tensor([[-0.0392,  0.0151,  0.0114, -0.0105]]) tensor([-0.0387,  0.1781,  0.0047, -0.3270])\n",
      "tensor([[-0.0359,  0.2437, -0.0032, -0.3744]]) tensor([-0.0351,  0.3732, -0.0019, -0.6182])\n",
      "tensor([[-0.0261,  0.3148, -0.0050, -0.4424]]) tensor([-0.0276,  0.1781, -0.0142, -0.3262])\n",
      "tensor([[-0.0119,  0.1968, -0.0091, -0.3199]]) tensor([-0.0241,  0.3734, -0.0208, -0.6233])\n",
      "tensor([[-0.0049,  0.3752, -0.0271, -0.6239]]) tensor([-0.0166,  0.1786, -0.0332, -0.3372])\n",
      "tensor([[-0.0007,  0.1463, -0.0460, -0.3060]]) tensor([-0.0130,  0.3742, -0.0400, -0.6402])\n",
      "tensor([[-0.0014,  0.3388, -0.0438, -0.6239]]) tensor([-0.0056,  0.1796, -0.0528, -0.3604])\n",
      "tensor([[ 0.0045,  0.2130, -0.0523, -0.4138]]) tensor([-0.0020,  0.3755, -0.0600, -0.6692])\n",
      "tensor([[ 0.0108,  0.3091, -0.0598, -0.6023]]) tensor([ 0.0055,  0.1812, -0.0734, -0.3960])\n",
      "tensor([[ 0.0103,  0.1631, -0.0747, -0.4693]]) tensor([ 0.0092,  0.3773, -0.0813, -0.7109])\n",
      "tensor([[ 0.0304,  0.4257, -0.0785, -0.7727]]) tensor([ 0.0167,  0.1834, -0.0955, -0.4448])\n",
      "tensor([[ 0.0195,  0.2616, -0.0881, -0.5797]]) tensor([ 0.0204, -0.0102, -0.1044, -0.1837])\n",
      "tensor([[ 0.0275,  0.0109, -0.1094, -0.2519]]) tensor([ 0.0202,  0.1862, -0.1081, -0.5074])\n",
      "tensor([[ 0.0281,  0.2155, -0.1048, -0.5480]]) tensor([ 0.0239, -0.0072, -0.1182, -0.2507])\n",
      "tensor([[ 0.0471,  0.0438, -0.1168, -0.3124]]) tensor([ 0.0238, -0.2005, -0.1232,  0.0025])\n",
      "tensor([[ 0.0053, -0.2391, -0.1278, -0.1401]]) tensor([ 0.0197, -0.3937, -0.1232,  0.2539])\n",
      "tensor([[-0.0059, -0.4452, -0.1055,  0.2326]]) tensor([ 0.0119, -0.5868, -0.1181,  0.5053])\n",
      "tensor([[-0.0218, -0.6251, -0.0944,  0.4819]]) tensor([ 1.3898e-04, -3.9025e-01, -1.0800e-01,  1.7790e-01])\n",
      "tensor([[-0.0285, -0.4481, -0.1121,  0.1814]]) tensor([-0.0077, -0.1938, -0.1044, -0.1468])\n",
      "tensor([[-0.0013, -0.0932, -0.1184, -0.2307]]) tensor([-0.0115, -0.3872, -0.1074,  0.1112])\n",
      "tensor([[-0.0196, -0.3386, -0.1053,  0.0729]]) tensor([-0.0193, -0.5807, -0.1052,  0.3682])\n",
      "tensor([[-0.0402, -0.5124, -0.0996,  0.1505]]) tensor([-0.0309, -0.7742, -0.0978,  0.6259])\n",
      "tensor([[-0.0790, -0.6835, -0.0469,  0.5050]]) tensor([-0.0464, -0.5778, -0.0853,  0.3041])\n",
      "tensor([[-0.0440, -0.3555, -0.0891,  0.0186]]) tensor([-0.0579, -0.7716, -0.0792,  0.5687])\n",
      "tensor([[-0.0950, -0.8494, -0.0404,  0.6950]]) tensor([-0.0734, -0.9656, -0.0678,  0.8354])\n",
      "tensor([[-0.1044, -0.8635, -0.0016,  0.7331]]) tensor([-0.0927, -0.7696, -0.0511,  0.5222])\n",
      "tensor([[-0.1157, -0.6012, -0.0077,  0.3545]]) tensor([-0.1081, -0.9639, -0.0407,  0.7984])\n",
      "tensor([[-0.1299, -0.7903, -0.0417,  0.4057]]) tensor([-0.1274, -1.1585, -0.0247,  1.0780])\n",
      "tensor([[-0.1756, -0.9231,  0.0282,  0.7573]]) tensor([-0.1505, -0.9630, -0.0031,  0.7777])\n",
      "tensor([[-0.1694, -0.7324,  0.0142,  0.6122]]) tensor([-0.1698, -0.7679,  0.0124,  0.4840])\n",
      "tensor([[-0.1829, -0.6657,  0.0226,  0.4004]]) tensor([-0.1851, -0.5729,  0.0221,  0.1953])\n",
      "tensor([[-0.1948, -0.6232,  0.0371,  0.4799]]) tensor([-0.1966, -0.3781,  0.0260, -0.0904])\n",
      "tensor([[-0.1652, -0.4070, -0.0036,  0.0568]]) tensor([-0.2042, -0.1834,  0.0242, -0.3747])\n",
      "tensor([[-0.1918, -0.0183,  0.0027, -0.4713]]) tensor([-0.2078, -0.3789,  0.0167, -0.0745])\n",
      "tensor([[-0.2015, -0.2555,  0.0342, -0.0479]]) tensor([-0.2154, -0.1840,  0.0152, -0.3619])\n",
      "tensor([[-0.1621,  0.0906, -0.0161, -0.4377]]) tensor([-0.2191,  0.0109,  0.0080, -0.6498])\n",
      "tensor([[-0.1537,  0.2783, -0.0154, -0.6621]]) tensor([-0.2189,  0.2059, -0.0050, -0.9399])\n",
      "tensor([[-0.1342,  0.2847, -0.0830, -0.8112]]) tensor([-0.2148,  0.0109, -0.0238, -0.6488])\n",
      "tensor([[-0.1276,  0.1726, -0.1075, -0.8012]]) tensor([-0.2145,  0.2063, -0.0368, -0.9489])\n",
      "tensor([[-0.1074,  0.3097, -0.1437, -0.9880]]) tensor([-0.2104,  0.0117, -0.0558, -0.6680])\n",
      "tensor([[-0.0736,  0.1901, -0.0772, -0.6915]]) tensor([-0.2102,  0.2076, -0.0691, -0.9777])\n",
      "tensor([[-0.1120, -0.0229, -0.1163, -0.5568]]) tensor([-0.2060,  0.4035, -0.0887, -1.2913])\n",
      "tensor([[-0.1745, -0.0697, -0.1570, -0.6219]]) tensor([-0.1980,  0.5997, -0.1145, -1.6104])\n",
      "tensor([[-0.2350,  0.0293, -0.1556, -0.9269]]) tensor([-0.1860,  0.4061, -0.1467, -1.3555])\n",
      "tensor([[-0.1884,  0.2816, -0.1571, -1.2220]]) tensor([-0.1778,  0.2131, -0.1738, -1.1121])\n",
      "tensor([[-0.1833, -0.0867, -0.1613, -0.9198]]) tensor([-0.1736,  0.0206, -0.1961, -0.8786])\n",
      "tensor([[-0.0029,  0.1216, -0.0078, -0.1594]]) tensor([-0.0119, -0.1528, -0.0036,  0.2605])\n",
      "tensor([[-0.0175, -0.2616,  0.0012,  0.3567]]) tensor([-0.0149,  0.0424,  0.0016, -0.0333])\n",
      "tensor([[ 0.0017,  0.0170, -0.0164, -0.0918]]) tensor([-0.0141, -0.1527,  0.0009,  0.2599])\n",
      "tensor([[-0.0053, -0.0301, -0.0055,  0.0276]]) tensor([-0.0171,  0.0424,  0.0061, -0.0325])\n",
      "tensor([[-0.0001, -0.0015, -0.0163, -0.0632]]) tensor([-0.0163,  0.2374,  0.0055, -0.3233])\n",
      "tensor([[-0.0007,  0.2655, -0.0173, -0.4665]]) tensor([-0.0115,  0.4324, -0.0010, -0.6142])\n",
      "tensor([[ 0.0008,  0.3653, -0.0109, -0.5666]]) tensor([-0.0029,  0.2373, -0.0133, -0.3219])\n",
      "tensor([[ 0.0029,  0.2059, -0.0201, -0.3606]]) tensor([ 0.0018,  0.0424, -0.0197, -0.0334])\n",
      "tensor([[ 0.0092,  0.0796, -0.0112, -0.0425]]) tensor([ 0.0027, -0.1524, -0.0204,  0.2530])\n",
      "tensor([[-0.0073, -0.1153, -0.0022,  0.1534]]) tensor([-3.5381e-04, -3.4725e-01, -1.5333e-02,  5.3918e-01])\n",
      "tensor([[-0.0039, -0.3776,  0.0132,  0.4920]]) tensor([-0.0073, -0.5422, -0.0045,  0.8270])\n",
      "tensor([[-0.0188, -0.4997,  0.0073,  0.7405]]) tensor([-0.0181, -0.3470,  0.0120,  0.5329])\n",
      "tensor([[-0.0123, -0.2256, -0.0021,  0.3379]]) tensor([-0.0251, -0.1520,  0.0226,  0.2440])\n",
      "tensor([[-0.0225, -0.2250,  0.0151,  0.3175]]) tensor([-0.0281, -0.3475,  0.0275,  0.5437])\n",
      "tensor([[-0.0330, -0.3063,  0.0302,  0.5110]]) tensor([-0.0351, -0.1527,  0.0384,  0.2599])\n",
      "tensor([[-0.0349, -0.1686,  0.0350,  0.2380]]) tensor([-0.0381,  0.0418,  0.0436, -0.0205])\n",
      "tensor([[-0.0280,  0.1236,  0.0319, -0.1375]]) tensor([-0.0373, -0.1539,  0.0432,  0.2856])\n",
      "tensor([[-0.0162, -0.0808,  0.0585,  0.2912]]) tensor([-0.0404, -0.3496,  0.0489,  0.5916])\n",
      "tensor([[-0.0429, -0.3375,  0.0550,  0.5059]]) tensor([-0.0474, -0.1552,  0.0607,  0.3147])\n",
      "tensor([[-0.0370, -0.0834,  0.0726,  0.2597]]) tensor([-0.0505, -0.3511,  0.0670,  0.6259])\n",
      "tensor([[-0.0536, -0.2366,  0.0875,  0.5617]]) tensor([-0.0575, -0.5471,  0.0795,  0.9390])\n",
      "tensor([[-0.0765, -0.5752,  0.1039,  0.9406]]) tensor([-0.0684, -0.3532,  0.0983,  0.6723])\n",
      "tensor([[-0.0869, -0.3105,  0.1142,  0.6672]]) tensor([-0.0755, -0.1595,  0.1118,  0.4121])\n",
      "tensor([[-0.0417, -0.1755,  0.0848,  0.4158]]) tensor([-0.0787, -0.3560,  0.1200,  0.7378])\n",
      "tensor([[-0.0788, -0.2712,  0.1214,  0.6425]]) tensor([-0.0858, -0.5526,  0.1348,  1.0658])\n",
      "tensor([[-0.1045, -0.5156,  0.1469,  1.0578]]) tensor([-0.0969, -0.3595,  0.1561,  0.8182])\n",
      "tensor([[-0.0799, -0.2679,  0.1431,  0.6706]]) tensor([-0.1040, -0.5564,  0.1725,  1.1557])\n",
      "tensor([[-0.1075, -0.4950,  0.1641,  1.0558]]) tensor([-0.1152, -0.7533,  0.1956,  1.4971])\n",
      "tensor([[ 0.0030, -0.0515, -0.0467, -0.0526]]) tensor([ 0.0016,  0.1729, -0.0480, -0.3530])\n",
      "tensor([[ 0.0104,  0.2301, -0.0459, -0.4212]]) tensor([ 0.0050,  0.3687, -0.0551, -0.6604])\n",
      "tensor([[ 0.0321,  0.4209, -0.0654, -0.7324]]) tensor([ 0.0124,  0.5645, -0.0683, -0.9699])\n",
      "tensor([[ 0.0406,  0.5746, -0.0758, -0.9453]]) tensor([ 0.0237,  0.3704, -0.0877, -0.6994])\n",
      "tensor([[ 0.0408,  0.3881, -0.0884, -0.7076]]) tensor([ 0.0311,  0.1766, -0.1017, -0.4356])\n",
      "tensor([[ 0.0367,  0.1680, -0.1025, -0.4966]]) tensor([ 0.0346,  0.3730, -0.1104, -0.7585])\n",
      "tensor([[ 0.0615,  0.4255, -0.1342, -0.8910]]) tensor([ 0.0421,  0.1796, -0.1255, -0.5025])\n",
      "tensor([[ 0.0418,  0.2249, -0.1258, -0.6347]]) tensor([ 0.0457, -0.0136, -0.1356, -0.2519])\n",
      "tensor([[ 0.0491,  0.0431, -0.1268, -0.3473]]) tensor([ 0.0454, -0.2065, -0.1406, -0.0048])\n",
      "tensor([[ 0.0386, -0.2115, -0.1145, -0.0965]]) tensor([ 0.0413, -0.3994, -0.1407,  0.2404])\n",
      "tensor([[ 0.0381, -0.3138, -0.1241,  0.0423]]) tensor([ 0.0333, -0.5923, -0.1359,  0.4856])\n",
      "tensor([[ 0.0007, -0.5523, -0.0834,  0.5048]]) tensor([ 0.0214, -0.7852, -0.1262,  0.7325])\n",
      "tensor([[-0.0259, -0.9198, -0.1072,  0.7832]]) tensor([ 0.0057, -0.5886, -0.1116,  0.4029])\n",
      "tensor([[-0.0273, -0.6560, -0.1213,  0.3462]]) tensor([-0.0060, -0.3921, -0.1035,  0.0773])\n",
      "tensor([[-0.0241, -0.3371, -0.1285, -0.0065]]) tensor([-0.0139, -0.1957, -0.1019, -0.2462])\n",
      "tensor([[ 0.0072, -0.1183, -0.1500, -0.2619]]) tensor([-0.0178, -0.3892, -0.1069,  0.0127])\n",
      "tensor([[ 0.0015, -0.2663, -0.1396, -0.1804]]) tensor([-0.0256, -0.5826, -0.1066,  0.2698])\n",
      "tensor([[-0.0526, -0.5572, -0.1291,  0.0705]]) tensor([-0.0372, -0.3862, -0.1012, -0.0545])\n",
      "tensor([[-0.0404, -0.3825, -0.1045, -0.0551]]) tensor([-0.0450, -0.5797, -0.1023,  0.2046])\n",
      "tensor([[-0.0637, -0.5047, -0.0858,  0.1494]]) tensor([-0.0566, -0.3833, -0.0982, -0.1185])\n",
      "tensor([[-0.0713, -0.4473, -0.0976, -0.0127]]) tensor([-0.0642, -0.5769, -0.1006,  0.1417])\n",
      "tensor([[-0.0755, -0.6302, -0.0959,  0.2217]]) tensor([-0.0758, -0.3804, -0.0978, -0.1810])\n",
      "tensor([[-0.0717, -0.4282, -0.1579, -0.1970]]) tensor([-0.0834, -0.5740, -0.1014,  0.0793])\n",
      "tensor([[-0.1348, -0.6253, -0.1637, -0.0162]]) tensor([-0.0948, -0.3776, -0.0998, -0.2435])\n",
      "tensor([[-0.1281, -0.2863, -0.1416, -0.5106]]) tensor([-0.1024, -0.1812, -0.1047, -0.5660])\n",
      "tensor([[-8.4148e-02,  3.3337e-04, -1.5528e-01, -8.9475e-01]]) tensor([-0.1060,  0.0152, -0.1160, -0.8897])\n",
      "tensor([[-0.1421,  0.1009, -0.1715, -1.0855]]) tensor([-0.1057, -0.1782, -0.1338, -0.6356])\n",
      "tensor([[-0.1317, -0.1537, -0.1905, -0.7838]]) tensor([-0.1093,  0.0185, -0.1465, -0.9672])\n",
      "tensor([[-0.1164,  0.0785, -0.1879, -1.0298]]) tensor([-0.1089, -0.1744, -0.1658, -0.7239])\n",
      "tensor([[-0.0202,  0.1360, -0.1825, -0.9523]]) tensor([-0.1124, -0.3668, -0.1803, -0.4877])\n",
      "tensor([[-0.0310, -0.4092, -0.1824, -0.2734]]) tensor([-0.1197, -0.1697, -0.1901, -0.8313])\n",
      "tensor([[-0.1044, -0.1293, -0.0880, -0.4829]]) tensor([-0.1231,  0.0274, -0.2067, -1.1773])\n",
      "18.362006841460243\n"
     ]
    }
   ],
   "source": [
    "# 2 Afficher ce que prédit le modèle vs les vraies observations\n",
    "\n",
    "model.eval()\n",
    "total = 0 # Loss totale\n",
    "for images, states in zip(data_images_bis, data_states_bis):\n",
    "    with torch.no_grad():\n",
    "        print(model(images.unsqueeze(0)),states)\n",
    "        total += np.sum(np.array((model(images.unsqueeze(0))-states)**2))\n",
    "\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'gotta_test_that_one.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 2 : GREY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "sequence_length = 2  # Number of images in each sequence\n",
    "num_episodes = 20   # Number of episodes for data collection\n",
    "\n",
    "\n",
    "# Environment Setup\n",
    "env = gymnasium.make('MonCartPole-v1', render_mode = 'rgb_array')\n",
    "data_images = []\n",
    "data_states = []\n",
    "\n",
    "# Transformer les images et les convertir en tenseurs\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((60, 135)),\n",
    "    transforms.Grayscale()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cart location for centering image crop\n",
    "def get_cart_location(screen_width):\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "# Cropping, downsampling (and Grayscaling) image\n",
    "def get_screen():\n",
    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "    screen = env.render().transpose((2, 0, 1))\n",
    "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width * 0.6)\n",
    "    cart_location = get_cart_location(screen_width)\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return transform(screen.transpose(1,2,0)).squeeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Collecter les données\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    observation = env.reset()[0]\n",
    "    images = [torch.zeros(60, 135) for _ in range(sequence_length)]\n",
    "    for t in range(1000):\n",
    "        img = env.render()\n",
    "        tensor_image = get_screen()\n",
    "        # if t == 5:\n",
    "        #     fig, axes = plt.subplots(1, 2, figsize=(15, 5))  # Crée une figure et des axes avec 1 ligne et 'n_images' colonnes\n",
    "        #     axes[0].imshow(tensor_image)\n",
    "        \n",
    "        images.append(tensor_image)\n",
    "        sequence_tensor = torch.stack(images[-sequence_length:], dim=0)\n",
    "        data_images.append(sequence_tensor)\n",
    "        data_states.append(observation)\n",
    "\n",
    "        action = env.action_space.sample()  \n",
    "        observation, reward, done, info, _ = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "# env.close()\n",
    "\n",
    "# Convert data_states to a tensor \n",
    "data_states = torch.tensor(data_states, dtype=torch.float32)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = TensorDataset(torch.stack(data_images), data_states)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CartPoleCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(2, 32, kernel_size=5, stride=1, padding=2),  # Input: 4 gray images, output: 16 channels, 60x135\n",
    "            # nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                 # Output size: ? 30x67\n",
    "            # nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            # nn.ReLU(),\n",
    "            # nn.MaxPoold(kernel_size=2, stride=2),                 # Output size: ? 15x33\n",
    "            # nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            # nn.ReLU(),\n",
    "            # nn.MaxPool2d(kernel_size=2, stride=2)                  # Output size: ? 7x16\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(64320, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,4)    # x, x_dot, theta, theta_dot\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output for the fully connected layers\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "# Instanciation du modèle\n",
    "model = CartPoleCNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 4]) torch.Size([10, 2, 60, 135])\n",
      "tensor([-0.0579, -0.3469, -0.0022,  0.4842])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x3000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for images, labels in dataloader:\n",
    "    print(labels.shape,images.shape)\n",
    "    fig,axes = plt.subplots(1,sequence_length,figsize = (20,30))\n",
    "    for i in range(sequence_length):\n",
    "        axes[i].imshow(images[0][i])\n",
    "    print(labels[0])\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.12795905768871307\n",
      "Epoch 2, Loss: 0.16815049946308136\n",
      "Epoch 3, Loss: 0.1672208309173584\n",
      "Epoch 4, Loss: 0.06543080508708954\n",
      "Epoch 5, Loss: 0.30774927139282227\n",
      "Epoch 6, Loss: 0.07811008393764496\n",
      "Epoch 7, Loss: 0.36342111229896545\n",
      "Epoch 8, Loss: 0.12097403407096863\n",
      "Epoch 9, Loss: 0.17208515107631683\n",
      "Epoch 10, Loss: 0.3119887411594391\n",
      "Epoch 11, Loss: 0.5892882943153381\n",
      "Epoch 12, Loss: 0.14269417524337769\n",
      "Epoch 13, Loss: 0.13215647637844086\n",
      "Epoch 14, Loss: 0.26543083786964417\n",
      "Epoch 15, Loss: 0.11028491705656052\n",
      "Epoch 16, Loss: 0.20775824785232544\n",
      "Epoch 17, Loss: 0.522468626499176\n",
      "Epoch 18, Loss: 0.11789672076702118\n",
      "Epoch 19, Loss: 0.34402769804000854\n",
      "Epoch 20, Loss: 0.2593497633934021\n",
      "Epoch 21, Loss: 0.064568430185318\n",
      "Epoch 22, Loss: 0.14318951964378357\n",
      "Epoch 23, Loss: 0.22737151384353638\n",
      "Epoch 24, Loss: 0.40478453040122986\n",
      "Epoch 25, Loss: 0.28532418608665466\n",
      "Epoch 26, Loss: 0.4295632243156433\n",
      "Epoch 27, Loss: 0.15064340829849243\n",
      "Epoch 28, Loss: 0.3069468140602112\n",
      "Epoch 29, Loss: 0.09490613639354706\n",
      "Epoch 30, Loss: 0.15051251649856567\n",
      "Epoch 31, Loss: 0.1543133407831192\n",
      "Epoch 32, Loss: 0.16465798020362854\n",
      "Epoch 33, Loss: 0.2567530870437622\n",
      "Epoch 34, Loss: 0.13846315443515778\n",
      "Epoch 35, Loss: 0.1739988625049591\n",
      "Epoch 36, Loss: 0.4338148832321167\n",
      "Epoch 37, Loss: 0.08458254486322403\n",
      "Epoch 38, Loss: 0.14950531721115112\n",
      "Epoch 39, Loss: 0.17352347075939178\n",
      "Epoch 40, Loss: 0.1228320524096489\n",
      "Epoch 41, Loss: 0.1791849434375763\n",
      "Epoch 42, Loss: 0.10643396526575089\n",
      "Epoch 43, Loss: 0.19770196080207825\n",
      "Epoch 44, Loss: 0.1579342633485794\n",
      "Epoch 45, Loss: 0.14996212720870972\n",
      "Epoch 46, Loss: 0.3444276452064514\n",
      "Epoch 47, Loss: 0.17646026611328125\n",
      "Epoch 48, Loss: 0.1172427237033844\n",
      "Epoch 49, Loss: 0.41419923305511475\n",
      "Epoch 50, Loss: 0.34818723797798157\n",
      "Epoch 51, Loss: 0.15568497776985168\n",
      "Epoch 52, Loss: 0.14107148349285126\n",
      "Epoch 53, Loss: 0.10160651803016663\n",
      "Epoch 54, Loss: 0.034122284501791\n",
      "Epoch 55, Loss: 0.25490444898605347\n",
      "Epoch 56, Loss: 0.5277999639511108\n",
      "Epoch 57, Loss: 0.1431201696395874\n",
      "Epoch 58, Loss: 0.08797946572303772\n",
      "Epoch 59, Loss: 0.08820928633213043\n",
      "Epoch 60, Loss: 0.10133758187294006\n",
      "Epoch 61, Loss: 0.11550122499465942\n",
      "Epoch 62, Loss: 0.045806851238012314\n",
      "Epoch 63, Loss: 0.09132952243089676\n",
      "Epoch 64, Loss: 0.07042660564184189\n",
      "Epoch 65, Loss: 0.17660991847515106\n",
      "Epoch 66, Loss: 0.17431558668613434\n",
      "Epoch 67, Loss: 0.22370803356170654\n",
      "Epoch 68, Loss: 0.23671138286590576\n",
      "Epoch 69, Loss: 0.07779441028833389\n",
      "Epoch 70, Loss: 0.13270902633666992\n",
      "Epoch 71, Loss: 0.1593846082687378\n",
      "Epoch 72, Loss: 0.17023441195487976\n",
      "Epoch 73, Loss: 0.1153985932469368\n",
      "Epoch 74, Loss: 0.2511827051639557\n",
      "Epoch 75, Loss: 0.1257251650094986\n",
      "Epoch 76, Loss: 0.20826685428619385\n",
      "Epoch 77, Loss: 0.39772093296051025\n",
      "Epoch 78, Loss: 0.26302388310432434\n",
      "Epoch 79, Loss: 0.034468553960323334\n",
      "Epoch 80, Loss: 0.3553372919559479\n",
      "Epoch 81, Loss: 0.19100528955459595\n",
      "Epoch 82, Loss: 0.14285479485988617\n",
      "Epoch 83, Loss: 0.31306564807891846\n",
      "Epoch 84, Loss: 0.1658933460712433\n",
      "Epoch 85, Loss: 0.09062151610851288\n",
      "Epoch 86, Loss: 0.1372305154800415\n",
      "Epoch 87, Loss: 0.0868682861328125\n",
      "Epoch 88, Loss: 0.4275440573692322\n",
      "Epoch 89, Loss: 0.21017518639564514\n",
      "Epoch 90, Loss: 0.1120184138417244\n",
      "Epoch 91, Loss: 0.07080287486314774\n",
      "Epoch 92, Loss: 0.8742998838424683\n",
      "Epoch 93, Loss: 0.30206263065338135\n",
      "Epoch 94, Loss: 0.09649918973445892\n",
      "Epoch 95, Loss: 0.27174803614616394\n",
      "Epoch 96, Loss: 0.14280346035957336\n",
      "Epoch 97, Loss: 0.2853206992149353\n",
      "Epoch 98, Loss: 0.04629865288734436\n",
      "Epoch 99, Loss: 0.35648679733276367\n",
      "Epoch 100, Loss: 0.49892234802246094\n",
      "Epoch 101, Loss: 0.11331404745578766\n",
      "Epoch 102, Loss: 0.12034033983945847\n",
      "Epoch 103, Loss: 0.12786133587360382\n",
      "Epoch 104, Loss: 0.12596194446086884\n",
      "Epoch 105, Loss: 0.13724187016487122\n",
      "Epoch 106, Loss: 0.28498536348342896\n",
      "Epoch 107, Loss: 0.29111093282699585\n",
      "Epoch 108, Loss: 0.20114704966545105\n",
      "Epoch 109, Loss: 0.21878491342067719\n",
      "Epoch 110, Loss: 0.10937671363353729\n",
      "Epoch 111, Loss: 0.06009489297866821\n",
      "Epoch 112, Loss: 0.6348516941070557\n",
      "Epoch 113, Loss: 0.1492498815059662\n",
      "Epoch 114, Loss: 0.42353832721710205\n",
      "Epoch 115, Loss: 0.3172977566719055\n",
      "Epoch 116, Loss: 0.08836037665605545\n",
      "Epoch 117, Loss: 0.18063586950302124\n",
      "Epoch 118, Loss: 0.4369086027145386\n",
      "Epoch 119, Loss: 0.1353740096092224\n",
      "Epoch 120, Loss: 0.21872669458389282\n",
      "Epoch 121, Loss: 0.08791038393974304\n",
      "Epoch 122, Loss: 0.200762540102005\n",
      "Epoch 123, Loss: 0.05439019203186035\n",
      "Epoch 124, Loss: 0.21900074183940887\n",
      "Epoch 125, Loss: 0.31606408953666687\n",
      "Epoch 126, Loss: 0.1652180403470993\n",
      "Epoch 127, Loss: 0.1570287048816681\n",
      "Epoch 128, Loss: 0.5868200063705444\n",
      "Epoch 129, Loss: 0.11552694439888\n",
      "Epoch 130, Loss: 0.07675136625766754\n",
      "Epoch 131, Loss: 0.34183359146118164\n",
      "Epoch 132, Loss: 0.2074672281742096\n",
      "Epoch 133, Loss: 0.2376672625541687\n",
      "Epoch 134, Loss: 0.07159005105495453\n",
      "Epoch 135, Loss: 0.3173164129257202\n",
      "Epoch 136, Loss: 0.22533690929412842\n",
      "Epoch 137, Loss: 0.04603787511587143\n",
      "Epoch 138, Loss: 0.4264526963233948\n",
      "Epoch 139, Loss: 0.25723356008529663\n",
      "Epoch 140, Loss: 0.13693219423294067\n",
      "Epoch 141, Loss: 0.28888651728630066\n",
      "Epoch 142, Loss: 0.2158116102218628\n",
      "Epoch 143, Loss: 0.09672752022743225\n",
      "Epoch 144, Loss: 0.14160723984241486\n",
      "Epoch 145, Loss: 0.19612053036689758\n",
      "Epoch 146, Loss: 0.16911137104034424\n",
      "Epoch 147, Loss: 0.5784082412719727\n",
      "Epoch 148, Loss: 0.12073855102062225\n",
      "Epoch 149, Loss: 0.12842154502868652\n",
      "Epoch 150, Loss: 0.2102596014738083\n",
      "Epoch 151, Loss: 0.10150673985481262\n",
      "Epoch 152, Loss: 0.28996604681015015\n",
      "Epoch 153, Loss: 0.25261348485946655\n",
      "Epoch 154, Loss: 0.40022024512290955\n",
      "Epoch 155, Loss: 0.2813801169395447\n",
      "Epoch 156, Loss: 0.36372050642967224\n",
      "Epoch 157, Loss: 0.12082476913928986\n",
      "Epoch 158, Loss: 0.2608303427696228\n",
      "Epoch 159, Loss: 0.17215324938297272\n",
      "Epoch 160, Loss: 0.24763989448547363\n",
      "Epoch 161, Loss: 0.16670642793178558\n",
      "Epoch 162, Loss: 0.1196756362915039\n",
      "Epoch 163, Loss: 0.23905779421329498\n",
      "Epoch 164, Loss: 0.08617599308490753\n",
      "Epoch 165, Loss: 0.2620934545993805\n",
      "Epoch 166, Loss: 0.23932293057441711\n",
      "Epoch 167, Loss: 0.5251576900482178\n",
      "Epoch 168, Loss: 0.0778031200170517\n",
      "Epoch 169, Loss: 0.295009970664978\n",
      "Epoch 170, Loss: 0.15556900203227997\n",
      "Epoch 171, Loss: 0.32641011476516724\n",
      "Epoch 172, Loss: 0.1562964767217636\n",
      "Epoch 173, Loss: 0.33649328351020813\n",
      "Epoch 174, Loss: 0.2293361872434616\n",
      "Epoch 175, Loss: 0.13977566361427307\n",
      "Epoch 176, Loss: 0.037016019225120544\n",
      "Epoch 177, Loss: 0.10277999192476273\n",
      "Epoch 178, Loss: 0.35617950558662415\n",
      "Epoch 179, Loss: 0.2724018096923828\n",
      "Epoch 180, Loss: 0.21636486053466797\n",
      "Epoch 181, Loss: 0.11204484105110168\n",
      "Epoch 182, Loss: 0.1773219108581543\n",
      "Epoch 183, Loss: 0.18409283459186554\n",
      "Epoch 184, Loss: 0.11326801031827927\n",
      "Epoch 185, Loss: 0.14221274852752686\n",
      "Epoch 186, Loss: 0.18027478456497192\n",
      "Epoch 187, Loss: 0.28616613149642944\n",
      "Epoch 188, Loss: 0.016256630420684814\n",
      "Epoch 189, Loss: 0.24521923065185547\n",
      "Epoch 190, Loss: 0.06692755967378616\n",
      "Epoch 191, Loss: 0.04456130042672157\n",
      "Epoch 192, Loss: 0.07899544388055801\n",
      "Epoch 193, Loss: 0.11342857778072357\n",
      "Epoch 194, Loss: 0.14293654263019562\n",
      "Epoch 195, Loss: 0.08630450814962387\n",
      "Epoch 196, Loss: 0.10042280703783035\n",
      "Epoch 197, Loss: 0.18589091300964355\n",
      "Epoch 198, Loss: 0.10589922964572906\n",
      "Epoch 199, Loss: 0.2540709674358368\n",
      "Epoch 200, Loss: 0.12772652506828308\n",
      "Epoch 201, Loss: 0.2938406467437744\n",
      "Epoch 202, Loss: 0.1935282200574875\n",
      "Epoch 203, Loss: 0.4253574013710022\n",
      "Epoch 204, Loss: 0.2639981806278229\n",
      "Epoch 205, Loss: 0.24351096153259277\n",
      "Epoch 206, Loss: 0.09293116629123688\n",
      "Epoch 207, Loss: 0.1438499391078949\n",
      "Epoch 208, Loss: 0.17726832628250122\n",
      "Epoch 209, Loss: 0.2322305142879486\n",
      "Epoch 210, Loss: 0.17272385954856873\n",
      "Epoch 211, Loss: 0.31334444880485535\n",
      "Epoch 212, Loss: 0.2446897327899933\n",
      "Epoch 213, Loss: 0.2076137214899063\n",
      "Epoch 214, Loss: 0.12495295703411102\n",
      "Epoch 215, Loss: 0.09569787234067917\n",
      "Epoch 216, Loss: 0.15715187788009644\n",
      "Epoch 217, Loss: 0.3191256523132324\n",
      "Epoch 218, Loss: 0.17532879114151\n",
      "Epoch 219, Loss: 0.12413356453180313\n",
      "Epoch 220, Loss: 0.05155086889863014\n",
      "Epoch 221, Loss: 0.3574693202972412\n",
      "Epoch 222, Loss: 0.409253865480423\n",
      "Epoch 223, Loss: 0.04787539690732956\n",
      "Epoch 224, Loss: 0.0792439877986908\n",
      "Epoch 225, Loss: 0.31747937202453613\n",
      "Epoch 226, Loss: 0.1560339778661728\n",
      "Epoch 227, Loss: 0.33846914768218994\n",
      "Epoch 228, Loss: 0.4491790533065796\n",
      "Epoch 229, Loss: 0.25384801626205444\n",
      "Epoch 230, Loss: 0.3224754333496094\n",
      "Epoch 231, Loss: 0.08273333311080933\n",
      "Epoch 232, Loss: 0.289800763130188\n",
      "Epoch 233, Loss: 0.13138841092586517\n",
      "Epoch 234, Loss: 0.2808108925819397\n",
      "Epoch 235, Loss: 0.46780675649642944\n",
      "Epoch 236, Loss: 0.09178505837917328\n",
      "Epoch 237, Loss: 0.2565386891365051\n",
      "Epoch 238, Loss: 0.21996693313121796\n",
      "Epoch 239, Loss: 0.19290639460086823\n",
      "Epoch 240, Loss: 0.15226489305496216\n",
      "Epoch 241, Loss: 0.13260941207408905\n",
      "Epoch 242, Loss: 0.27691516280174255\n",
      "Epoch 243, Loss: 0.2017766237258911\n",
      "Epoch 244, Loss: 0.14864712953567505\n",
      "Epoch 245, Loss: 0.16028407216072083\n",
      "Epoch 246, Loss: 0.40310385823249817\n",
      "Epoch 247, Loss: 0.2929877042770386\n",
      "Epoch 248, Loss: 0.220647394657135\n",
      "Epoch 249, Loss: 0.11957653611898422\n",
      "Epoch 250, Loss: 0.05994147062301636\n",
      "Epoch 251, Loss: 0.5145617127418518\n",
      "Epoch 252, Loss: 0.07616190612316132\n",
      "Epoch 253, Loss: 0.22743158042430878\n",
      "Epoch 254, Loss: 0.0587446391582489\n",
      "Epoch 255, Loss: 0.2255931794643402\n",
      "Epoch 256, Loss: 0.33863919973373413\n",
      "Epoch 257, Loss: 0.4986945390701294\n",
      "Epoch 258, Loss: 0.3892764151096344\n",
      "Epoch 259, Loss: 0.29929372668266296\n",
      "Epoch 260, Loss: 0.32512444257736206\n",
      "Epoch 261, Loss: 0.09485416114330292\n",
      "Epoch 262, Loss: 0.11498145759105682\n",
      "Epoch 263, Loss: 0.27692049741744995\n",
      "Epoch 264, Loss: 0.14393854141235352\n",
      "Epoch 265, Loss: 0.1253916621208191\n",
      "Epoch 266, Loss: 0.2566443979740143\n",
      "Epoch 267, Loss: 0.41641074419021606\n",
      "Epoch 268, Loss: 0.25828585028648376\n",
      "Epoch 269, Loss: 0.19621095061302185\n",
      "Epoch 270, Loss: 0.11066567897796631\n",
      "Epoch 271, Loss: 0.3427559733390808\n",
      "Epoch 272, Loss: 0.17758767306804657\n",
      "Epoch 273, Loss: 0.34601128101348877\n",
      "Epoch 274, Loss: 0.30563172698020935\n",
      "Epoch 275, Loss: 0.13630175590515137\n",
      "Epoch 276, Loss: 0.2868073582649231\n",
      "Epoch 277, Loss: 0.11947295069694519\n",
      "Epoch 278, Loss: 0.2694118618965149\n",
      "Epoch 279, Loss: 0.16379329562187195\n",
      "Epoch 280, Loss: 0.03804447501897812\n",
      "Epoch 281, Loss: 0.3808356523513794\n",
      "Epoch 282, Loss: 0.1975187212228775\n",
      "Epoch 283, Loss: 0.2647131085395813\n",
      "Epoch 284, Loss: 0.10330486297607422\n",
      "Epoch 285, Loss: 0.4295140504837036\n",
      "Epoch 286, Loss: 0.09375537931919098\n",
      "Epoch 287, Loss: 0.12817466259002686\n",
      "Epoch 288, Loss: 0.26681941747665405\n",
      "Epoch 289, Loss: 0.7485343217849731\n",
      "Epoch 290, Loss: 0.1678769737482071\n",
      "Epoch 291, Loss: 0.7278772592544556\n",
      "Epoch 292, Loss: 0.13080620765686035\n",
      "Epoch 293, Loss: 0.506493091583252\n",
      "Epoch 294, Loss: 0.07076752185821533\n",
      "Epoch 295, Loss: 0.11100711673498154\n",
      "Epoch 296, Loss: 0.27507317066192627\n",
      "Epoch 297, Loss: 0.23134517669677734\n",
      "Epoch 298, Loss: 0.37079107761383057\n",
      "Epoch 299, Loss: 0.09823815524578094\n",
      "Epoch 300, Loss: 0.2864729166030884\n",
      "Epoch 301, Loss: 0.10339072346687317\n",
      "Epoch 302, Loss: 0.08300721645355225\n",
      "Epoch 303, Loss: 0.28915083408355713\n",
      "Epoch 304, Loss: 0.3318459093570709\n",
      "Epoch 305, Loss: 0.4018222391605377\n",
      "Epoch 306, Loss: 0.331948459148407\n",
      "Epoch 307, Loss: 0.21068212389945984\n",
      "Epoch 308, Loss: 0.17253054678440094\n",
      "Epoch 309, Loss: 0.20928874611854553\n",
      "Epoch 310, Loss: 0.21643806993961334\n",
      "Epoch 311, Loss: 0.033105310052633286\n",
      "Epoch 312, Loss: 0.1014837771654129\n",
      "Epoch 313, Loss: 0.21924811601638794\n",
      "Epoch 314, Loss: 0.2938966155052185\n",
      "Epoch 315, Loss: 0.10374386608600616\n",
      "Epoch 316, Loss: 0.12282721698284149\n",
      "Epoch 317, Loss: 0.26700353622436523\n",
      "Epoch 318, Loss: 0.2316516637802124\n",
      "Epoch 319, Loss: 0.28986096382141113\n",
      "Epoch 320, Loss: 0.2333649843931198\n",
      "Epoch 321, Loss: 0.11218191683292389\n",
      "Epoch 322, Loss: 0.20878718793392181\n",
      "Epoch 323, Loss: 0.3208436667919159\n",
      "Epoch 324, Loss: 0.19141885638237\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, states)\n\u001b[0;32m      9\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 10\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m             )\n\u001b[1;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    158\u001b[0m         group,\n\u001b[0;32m    159\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    164\u001b[0m         state_steps)\n\u001b[1;32m--> 166\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 316\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\adam.py:391\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    388\u001b[0m     param \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(param)\n\u001b[0;32m    390\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m--> 391\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    392\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "num_epochs = 500 # Peut-être avec plus d'epoch on obtiendrait un meilleur résultat ? jsp\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for images, states in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, states)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voir encore mieux ce que ça donne\n",
    "\n",
    "# 1 : On collecte des images du cartpole (heuristique : random)\n",
    "\n",
    "env = gymnasium.make('CartPole-v1', render_mode = 'rgb_array')\n",
    "data_images_bis = []\n",
    "data_states_bis = []\n",
    "\n",
    "for episode in range(3):\n",
    "    observation_bis = env.reset()[0]\n",
    "    images_bis = []\n",
    "    for t in range(1000):\n",
    "        img = env.render()\n",
    "        tensor_image = transform(img).squeeze(0)  # Transform image immediately\n",
    "        images_bis.append(tensor_image)\n",
    "        \n",
    "        if len(images_bis) >= sequence_length:\n",
    "            # Stack the last sequence_length images to form a single sequence tensor\n",
    "            sequence_tensor = torch.stack(images_bis[-sequence_length:], dim=0)\n",
    "            data_images_bis.append(sequence_tensor)\n",
    "            data_states_bis.append(observation)\n",
    "        \n",
    "        action = env.action_space.sample()   # Use the heuristic policy\n",
    "        observation, reward, done, info, _ = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "data_states_bis = torch.tensor(data_states_bis, dtype=torch.float32)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "data_images_bis = torch.stack(data_images_bis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0171, 0.0640, 0.0296, 0.1519]]) tensor([ 0.0223,  0.1958,  0.0023, -0.2450])\n",
      "tensor([[ 0.0019, -0.0086,  0.0220,  0.1821]]) tensor([ 0.0262,  0.0006, -0.0026,  0.0484])\n",
      "tensor([[0.0145, 0.0682, 0.0334, 0.1532]]) tensor([ 0.0262,  0.1958, -0.0016, -0.2451])\n",
      "tensor([[ 0.0069, -0.0092,  0.0262,  0.2019]]) tensor([ 0.0301,  0.3909, -0.0065, -0.5383])\n",
      "tensor([[0.0034, 0.0329, 0.0279, 0.1633]]) tensor([ 0.0380,  0.1959, -0.0173, -0.2477])\n",
      "tensor([[0.0030, 0.0586, 0.0364, 0.1539]]) tensor([ 0.0419,  0.0010, -0.0222,  0.0395])\n",
      "tensor([[0.0073, 0.0549, 0.0386, 0.1589]]) tensor([ 0.0419, -0.1938, -0.0214,  0.3251])\n",
      "tensor([[0.0163, 0.0125, 0.0331, 0.2098]]) tensor([ 0.0380, -0.3886, -0.0149,  0.6109])\n",
      "tensor([[0.0212, 0.0400, 0.0332, 0.1914]]) tensor([ 0.0302, -0.1933, -0.0027,  0.3136])\n",
      "tensor([[0.0103, 0.0421, 0.0354, 0.1772]]) tensor([ 0.0264, -0.3884,  0.0036,  0.6054])\n",
      "tensor([[0.0264, 0.0304, 0.0305, 0.1945]]) tensor([ 0.0186, -0.5835,  0.0157,  0.8992])\n",
      "tensor([[ 0.0078, -0.0550,  0.0363,  0.2973]]) tensor([ 0.0069, -0.7789,  0.0336,  1.1968])\n",
      "tensor([[ 0.0098, -0.0319,  0.0383,  0.2852]]) tensor([-0.0086, -0.5842,  0.0576,  0.9148])\n",
      "tensor([[0.0153, 0.0195, 0.0363, 0.2250]]) tensor([-0.0203, -0.3899,  0.0759,  0.6408])\n",
      "tensor([[0.0113, 0.0224, 0.0435, 0.2388]]) tensor([-0.0281, -0.5860,  0.0887,  0.9564])\n",
      "tensor([[-3.2818e-05, -3.4491e-02,  4.6923e-02,  3.0343e-01]]) tensor([-0.0398, -0.7822,  0.1078,  1.2755])\n",
      "tensor([[-0.0019, -0.0402,  0.0495,  0.3184]]) tensor([-0.0555, -0.9785,  0.1333,  1.5999])\n",
      "tensor([[-0.0191, -0.0998,  0.0561,  0.3942]]) tensor([-0.0751, -1.1749,  0.1653,  1.9310])\n",
      "tensor([[-0.0136, -0.1293,  0.0483,  0.4353]]) tensor([-0.0986, -1.3714,  0.2039,  2.2701])\n",
      "tensor([[0.0094, 0.0610, 0.0475, 0.1965]]) tensor([-0.0488,  0.2358,  0.0397, -0.2542])\n",
      "tensor([[0.0011, 0.1060, 0.0561, 0.1533]]) tensor([-0.0441,  0.4303,  0.0346, -0.5341])\n",
      "tensor([[0.0030, 0.0932, 0.0514, 0.1606]]) tensor([-0.0355,  0.6250,  0.0239, -0.8157])\n",
      "tensor([[-0.0882,  0.0808,  0.0311,  0.0638]]) tensor([-0.0230,  0.4295,  0.0076, -0.5155])\n",
      "tensor([[0.0026, 0.0919, 0.0447, 0.1296]]) tensor([-0.0144,  0.2343, -0.0027, -0.2205])\n",
      "tensor([[0.0146, 0.1081, 0.0450, 0.1242]]) tensor([-0.0097,  0.4295, -0.0071, -0.5140])\n",
      "tensor([[0.0156, 0.0718, 0.0243, 0.1277]]) tensor([-0.0011,  0.6247, -0.0174, -0.8089])\n",
      "tensor([[-0.0138,  0.0667,  0.0105,  0.0941]]) tensor([ 0.0114,  0.4298, -0.0335, -0.5217])\n",
      "tensor([[0.0160, 0.0503, 0.0157, 0.1256]]) tensor([ 0.0200,  0.2352, -0.0440, -0.2398])\n",
      "tensor([[0.0143, 0.0541, 0.0148, 0.1224]]) tensor([ 0.0247,  0.0407, -0.0488,  0.0387])\n",
      "tensor([[0.0183, 0.0455, 0.0184, 0.1463]]) tensor([ 0.0255,  0.2365, -0.0480, -0.2690])\n",
      "tensor([[0.0165, 0.0688, 0.0222, 0.1256]]) tensor([ 0.0302,  0.4323, -0.0534, -0.5764])\n",
      "tensor([[0.0135, 0.0369, 0.0098, 0.1332]]) tensor([ 0.0389,  0.6281, -0.0649, -0.8854])\n",
      "tensor([[0.0072, 0.0369, 0.0150, 0.1254]]) tensor([ 0.0514,  0.8240, -0.0826, -1.1978])\n",
      "tensor([[-0.0197,  0.0660,  0.0098,  0.0736]]) tensor([ 0.0679,  0.6301, -0.1066, -0.9321])\n",
      "tensor([[0.0035, 0.1027, 0.0038, 0.0354]]) tensor([ 0.0805,  0.8264, -0.1252, -1.2563])\n",
      "tensor([[-0.0253,  0.0734,  0.0030,  0.0335]]) tensor([ 0.0970,  1.0229, -0.1503, -1.5854])\n",
      "tensor([[-0.0930,  0.0463,  0.0062,  0.0116]]) tensor([ 0.1175,  0.8299, -0.1820, -1.3431])\n",
      "tensor([[-0.0781,  0.0661,  0.0153,  0.0154]]) tensor([ 0.1341,  0.6375, -0.2089, -1.1125])\n",
      "tensor([[0.0022, 0.0204, 0.0454, 0.2296]]) tensor([-0.0466, -0.2348,  0.0030,  0.3124])\n",
      "tensor([[-0.0013, -0.0543,  0.0387,  0.3072]]) tensor([-0.0513, -0.4300,  0.0093,  0.6060])\n",
      "tensor([[ 0.0030, -0.0241,  0.0388,  0.2788]]) tensor([-0.0599, -0.6253,  0.0214,  0.9016])\n",
      "tensor([[-0.0162, -0.1114,  0.0360,  0.3697]]) tensor([-0.0724, -0.8207,  0.0394,  1.2009])\n",
      "tensor([[-0.0109, -0.0933,  0.0264,  0.3503]]) tensor([-0.0889, -0.6261,  0.0634,  0.9209])\n",
      "tensor([[ 0.0005, -0.0460,  0.0235,  0.2813]]) tensor([-0.1014, -0.4319,  0.0819,  0.6488])\n",
      "tensor([[-0.0070, -0.0287,  0.0417,  0.2868]]) tensor([-0.1100, -0.2380,  0.0948,  0.3829])\n",
      "tensor([[-0.0045, -0.0101,  0.0457,  0.2670]]) tensor([-0.1148, -0.4343,  0.1025,  0.7040])\n",
      "tensor([[-0.0017, -0.0109,  0.0460,  0.2725]]) tensor([-0.1235, -0.6307,  0.1166,  1.0271])\n",
      "tensor([[-5.8520e-03, -1.6692e-04,  5.7644e-02,  3.0170e-01]]) tensor([-0.1361, -0.8272,  0.1371,  1.3540])\n",
      "tensor([[-0.0128, -0.0688,  0.0448,  0.3637]]) tensor([-0.1526, -1.0237,  0.1642,  1.6862])\n",
      "tensor([[-0.0254, -0.0753,  0.0628,  0.3871]]) tensor([-0.1731, -1.2203,  0.1979,  2.0252])\n",
      "52.98755673598498\n"
     ]
    }
   ],
   "source": [
    "# 2 Afficher ce que prédit le modèle vs les vraies observations\n",
    "\n",
    "model.eval()\n",
    "total = 0 # Loss totale\n",
    "for images, states in zip(data_images_bis, data_states_bis):\n",
    "    with torch.no_grad():\n",
    "        print(model(images.unsqueeze(0)),states)\n",
    "        total += np.sum(np.array((model(images.unsqueeze(0))-states)**2))\n",
    "\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'cnn_grey_enhanced_2500.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Uniform Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hatem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.x_threshold to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.x_threshold` for environment variables or `env.get_wrapper_attr('x_threshold')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\hatem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.theta_threshold_radians to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.theta_threshold_radians` for environment variables or `env.get_wrapper_attr('theta_threshold_radians')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "sequence_length = 2  # Number of images in each sequence\n",
    "num_episodes = 20   # Number of episodes for data collection\n",
    "\n",
    "# Environment Setup\n",
    "env = gymnasium.make('MonCartPole-v1', render_mode = 'rgb_array')\n",
    "data_images = []\n",
    "data_states = []\n",
    "\n",
    "x_threshold = env.x_threshold\n",
    "theta_threshold = env.theta_threshold_radians\n",
    "# On prend quoi pour x_dot et theta_dot ?? Compliqué, prendre de manière random\n",
    "# semble peu pertinent car dépendent directement de x et theta..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
