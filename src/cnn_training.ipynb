{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F  # Ensure this import is added\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import gym\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hatem\\AppData\\Local\\Temp\\ipykernel_14984\\2022644902.py:44: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  data_states = torch.tensor(data_states, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.012585052289068699\n",
      "Epoch 2, Loss: 0.013496935367584229\n",
      "Epoch 3, Loss: 0.03656453639268875\n",
      "Epoch 4, Loss: 0.02040678635239601\n",
      "Epoch 5, Loss: 0.03142130747437477\n",
      "Epoch 6, Loss: 0.040939267724752426\n",
      "Epoch 7, Loss: 0.012346663512289524\n",
      "Epoch 8, Loss: 0.010714675299823284\n",
      "Epoch 9, Loss: 0.008351529017090797\n",
      "Epoch 10, Loss: 0.018499860540032387\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Parameters\n",
    "sequence_length = 4  # Number of images in each sequence\n",
    "num_episodes = 100   # Number of episodes for data collection\n",
    "\n",
    "# Environment Setup\n",
    "env = gym.make('CartPole-v1')\n",
    "data_images = []\n",
    "data_states = []\n",
    "\n",
    "# Transformation for images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # Resize image to manageable size\n",
    "    transforms.ToTensor()         # Convert image to PyTorch tensor\n",
    "])\n",
    "\n",
    "def heuristic_policy(observation):\n",
    "    _, _, angle, _ = observation\n",
    "    return 0 if angle < 0 else 1  # Move cart based on the angle of the pole\n",
    "\n",
    "# Data Collection using Heuristic Policy\n",
    "for episode in range(num_episodes):\n",
    "    observation = env.reset()\n",
    "    images = []\n",
    "    for t in range(1000):\n",
    "        img = env.render(mode='rgb_array')\n",
    "        img_pil = Image.fromarray(img)\n",
    "        tensor_image = transform(img_pil)  # Transform image immediately\n",
    "        images.append(tensor_image)\n",
    "        \n",
    "        if len(images) >= sequence_length:\n",
    "            # Stack the last sequence_length images to form a single sequence tensor\n",
    "            sequence_tensor = torch.stack(images[-sequence_length:], dim=0).permute(1, 0, 2, 3)\n",
    "            data_images.append(sequence_tensor)\n",
    "            data_states.append(observation)\n",
    "        \n",
    "        action = heuristic_policy(observation)  # Use the heuristic policy\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Convert data_states to a tensor\n",
    "data_states = torch.tensor(data_states, dtype=torch.float32)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = TensorDataset(torch.stack(data_images), data_states)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Model Definition\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv3d(3, 16, kernel_size=(3, 3, 3), stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2)),\n",
    "            nn.Conv3d(16, 32, kernel_size=(3, 3, 3), stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2)),\n",
    "        )\n",
    "        # Correctly calculate the input size for the linear layer based on the output from conv_layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(32 * 4 * 16 * 16, 128),  # Adjusted based on actual output size\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 4)  # Predicting 4 state variables\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor for the fully connected layer\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "# Model instantiation and training setup\n",
    "model = CNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for images, states in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, states)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'cartpole_cnn_test.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv_layers): Sequential(\n",
       "    (0): Conv3d(3, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc_layers): Sequential(\n",
       "    (0): Linear(in_features=32768, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = CNN()  # Make sure CNN is defined or imported\n",
    "cnn.load_state_dict(torch.load(\"cartpole_cnn_test.pth\"))  # Load the pretrained model\n",
    "cnn.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0139,  1.6255, -0.0506, -2.1316], grad_fn=<SelectBackward0>) tensor([-0.0140,  1.7032, -0.0226, -2.2323])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv_layers): Sequential(\n",
       "    (0): Conv3d(3, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc_layers): Sequential(\n",
       "    (0): Linear(in_features=32768, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch.manual_seed(0)  # Set a seed for reproducibility\n",
    "\n",
    "cnn.eval()\n",
    "for images, states in dataloader:\n",
    "    print(cnn(images)[4],states[4])\n",
    "    break\n",
    "cnn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04935691  0.0214029  -0.01350331  0.00454222]\n",
      "-0.013503309\n",
      "-0.048928853\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 64\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(image_data), np\u001b[38;5;241m.\u001b[39marray(state_data)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Data collection\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m image_data, state_data \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m image_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(image_data, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     66\u001b[0m state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state_data, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "Cell \u001b[1;32mIn[20], line 58\u001b[0m, in \u001b[0;36mcollect_data\u001b[1;34m(episodes)\u001b[0m\n\u001b[0;32m     55\u001b[0m             image_data\u001b[38;5;241m.\u001b[39mappend(input_images)\n\u001b[0;32m     56\u001b[0m             state_data\u001b[38;5;241m.\u001b[39mappend(observation)  \u001b[38;5;66;03m# Collect state data\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m         action \u001b[38;5;241m=\u001b[39m \u001b[43mheuristic_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Your heuristic policy\u001b[39;00m\n\u001b[0;32m     59\u001b[0m         observation, _, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(image_data), np\u001b[38;5;241m.\u001b[39marray(state_data)\n",
      "Cell \u001b[1;32mIn[20], line 4\u001b[0m, in \u001b[0;36mheuristic_policy\u001b[1;34m(observation)\u001b[0m\n\u001b[0;32m      2\u001b[0m obs \u001b[38;5;241m=\u001b[39m observation[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(obs)\n\u001b[1;32m----> 4\u001b[0m ang \u001b[38;5;241m=\u001b[39m \u001b[43mobs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(ang)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ang \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "def heuristic_policy(observation):\n",
    "    obs = observation[0]\n",
    "    print(obs)\n",
    "    ang = obs[2]\n",
    "    print(ang)\n",
    "    return 0 if ang < 0 else 1  # Move cart based on the angle of the pole\n",
    "\n",
    "# Define the CNN architecture adapted to your image preprocessing and dimensions\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 16, kernel_size=5, padding=2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, padding=2)        \n",
    "        self.fc1 = nn.Linear(32 * 21 * 21, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 4)  # Output size to predict state variables\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))        \n",
    "        x = x.view(-1, 32 * 21 * 21)  \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def preprocess_image(image):\n",
    "    # Convert image to grayscale and resize to 84x84\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    resized_image = cv2.resize(gray_image, (84, 84))\n",
    "    return resized_image\n",
    "\n",
    "def collect_data(episodes=500):\n",
    "    env = gym.make('CartPole-v2')\n",
    "    action_data = []\n",
    "    state_data = []\n",
    "    image_data = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        images = []\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            image = env.render('rgb_array')\n",
    "            processed_image = preprocess_image(image)\n",
    "            images.append(processed_image)\n",
    "\n",
    "            # Collect data only if we have 4 consecutive frames\n",
    "            if len(images) >= 4:\n",
    "                # Stack the last 4 images to create input for CNN\n",
    "                input_images = np.stack(images[-4:], axis=0)\n",
    "                input_images = input_images / 255.0  # Normalize images\n",
    "\n",
    "                image_data.append(input_images)\n",
    "                state_data.append(observation)  # Collect state data\n",
    "\n",
    "            action = heuristic_policy(observation)  # Your heuristic policy\n",
    "            observation, _, done, _ = env.step(action)\n",
    "\n",
    "    return np.array(image_data), np.array(state_data)\n",
    "\n",
    "# Data collection\n",
    "image_data, state_data = collect_data()\n",
    "image_tensor = torch.tensor(image_data, dtype=torch.float32)\n",
    "state_tensor = torch.tensor(state_data, dtype=torch.float32)\n",
    "\n",
    "# Define dataset and dataloader for training\n",
    "dataset = TensorDataset(image_tensor, state_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Model setup\n",
    "model = SimpleCNN()\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(10):  # You can adjust the number of epochs\n",
    "    total_loss = 0\n",
    "    for images, states in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(images)\n",
    "        loss = criterion(predictions, states)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}')\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'cartpole_cnn2.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0179, -0.9981,  0.0437,  1.3493], grad_fn=<SelectBackward0>) tensor([-0.0169, -1.0171,  0.0446,  1.3195])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for dimension 0 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, states \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(model(images)[\u001b[38;5;241m8\u001b[39m],states[\u001b[38;5;241m8\u001b[39m])\n\u001b[1;32m----> 4\u001b[0m     plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[1;31mIndexError\u001b[0m: index 3 is out of bounds for dimension 0 with size 3"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for images, states in dataloader:\n",
    "    print(model(images)[8],states[8])\n",
    "    plt.imshow(images[8][3])\n",
    "    break\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step successful: [-0.04015512 -0.15969469 -0.03355908  0.23401141] 1.0 False {'additional_info': False}\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v2')\n",
    "env.reset()\n",
    "try:\n",
    "    observation, reward, done, info = env.step(0)\n",
    "    print(\"Step successful:\", observation, reward, done, info)\n",
    "except Exception as e:\n",
    "    print(\"Error in step execution:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
