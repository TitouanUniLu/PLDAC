{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F  # Ensure this import is added\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import gymnasium \n",
    "import mon_env\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 1 : RGB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "sequence_length = 2  # Number of images in each sequence, IL FAUT TESTER AVEC 2 P-E CA CHANGE TOUT \n",
    "num_episodes = 300   ### JOUER AVEC CE PARAMETRE POUR AMELIORER LE MODELE : TESTER AVEC 1000 SERAIT COOL\n",
    "\n",
    "# Environment Setup\n",
    "env = gymnasium.make('MonCartPole-v1',render_mode=\"rgb_array\")\n",
    "data_images = []\n",
    "data_states = []\n",
    "\n",
    "# Transformation for images\n",
    "transform = transforms.Compose([transforms.ToPILImage(), \n",
    "                    transforms.Resize(60, interpolation=Image.LANCZOS),\n",
    "                    transforms.ToTensor()])\n",
    "\n",
    "\n",
    "# Cart location for centering image crop\n",
    "def get_cart_location(screen_width):\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "# Cropping, downsampling (and Grayscaling) image\n",
    "def get_screen():\n",
    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "    screen = env.render().transpose((2, 0, 1))\n",
    "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width * 0.6)\n",
    "    cart_location = get_cart_location(screen_width)\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    screen = torch.from_numpy(screen)\n",
    "    return transform(screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hatem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.x_threshold to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.x_threshold` for environment variables or `env.get_wrapper_attr('x_threshold')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\hatem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.state to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.state` for environment variables or `env.get_wrapper_attr('state')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Data Collection using Heuristic Policy\n",
    "for episode in range(num_episodes):\n",
    "    observation = env.reset()[0]\n",
    "    images = [torch.zeros(3, 60, 135) for _ in range(sequence_length)]\n",
    "           \n",
    "    for t in range(1000):\n",
    "        tensor_image = get_screen()  # Transform image immediately\n",
    "        # if t==4:\n",
    "        #     print(tensor_image.shape)\n",
    "        #     plt.imshow(np.array(tensor_image.permute(1,2,0)))\n",
    "        images.append(tensor_image)\n",
    "        \n",
    "        if len(images) >= sequence_length:\n",
    "            # Stack the last sequence_length images to form a single sequence tensor\n",
    "            sequence_tensor = torch.stack(images[-sequence_length:], dim=0).permute(1, 0, 2, 3)\n",
    "            data_images.append(sequence_tensor)\n",
    "            data_states.append(observation)\n",
    "        \n",
    "        action = env.action_space.sample()  \n",
    "        observation, reward, done, info, _ = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Convert data_states to a tensor\n",
    "data_states = torch.tensor(data_states, dtype=torch.float32)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = TensorDataset(torch.stack(data_images), data_states)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv3d(3, 16, kernel_size=(3, 3, 3), stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2)),\n",
    "            nn.Conv3d(16, 32, kernel_size=(3, 3, 3), stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2)),\n",
    "        )\n",
    "        # Correctly calculate the input size for the linear layer based on the output from conv_layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(int(63360/2), 128),  # Adjusted based on actual output size #63360/2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 4)  # Predicting 4 state variables\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor for the fully connected layer\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.02940896339714527\n",
      "Epoch 2, Loss: 0.012496653944253922\n",
      "Epoch 3, Loss: 0.006476971320807934\n",
      "Epoch 4, Loss: 0.02483018860220909\n",
      "Epoch 5, Loss: 0.001147132832556963\n",
      "Epoch 6, Loss: 0.05469672009348869\n",
      "Epoch 7, Loss: 0.11609227955341339\n",
      "Epoch 8, Loss: 0.033605802804231644\n",
      "Epoch 9, Loss: 0.037265874445438385\n",
      "Epoch 10, Loss: 0.04124061390757561\n"
     ]
    }
   ],
   "source": [
    "# Model instantiation and training setup\n",
    "model = CNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 10 # Peut-être avec plus d'epoch on obtiendrait un meilleur résultat ? jsp\n",
    "for epoch in range(num_epochs):\n",
    "    for images, states in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, states)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'cartpole_cnn_rgb_enhanced_2_img.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 2, 60, 135]) torch.Size([10, 4])\n",
      "tensor([[ 0.0210,  0.2931, -0.0334, -0.4318]], grad_fn=<AddmmBackward0>) tensor([ 0.1271,  0.3757, -0.0280, -0.2534])\n"
     ]
    }
   ],
   "source": [
    "# Voir un peu ce que ça donne\n",
    "\n",
    "model.eval()\n",
    "for images, states in dataloader:\n",
    "    print(images.shape,states.shape)\n",
    "    print(model(images[0].unsqueeze(0)),states[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 4]) torch.Size([10, 2, 60, 135, 3])\n",
      "torch.Size([60, 135, 3])\n",
      "torch.Size([60, 135, 3])\n",
      "tensor([ 0.0012,  0.2362, -0.0406, -0.3280])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x3000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for images, labels in dataloader:\n",
    "    images = images.permute((0,2,3,4,1))\n",
    "    print(labels.shape,images.shape)\n",
    "    fig,axes = plt.subplots(1,sequence_length,figsize = (20,30))\n",
    "    for i in range(sequence_length):\n",
    "        axes[i].imshow(images[0][i])\n",
    "        print(images[0][i].shape)\n",
    "    print(labels[0])\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\serialization.py:541\u001b[0m, in \u001b[0;36m_check_seekable\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 541\u001b[0m     \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseek\u001b[49m(f\u001b[38;5;241m.\u001b[39mtell())\n\u001b[0;32m    542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'seek'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m CNN()\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcartpole_cnn_rgb.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\serialization.py:450\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _open_buffer_writer(name_or_buffer)\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m--> 450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_buffer_reader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    452\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in mode but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\serialization.py:435\u001b[0m, in \u001b[0;36m_open_buffer_reader.__init__\u001b[1;34m(self, buffer)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, buffer):\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(buffer)\n\u001b[1;32m--> 435\u001b[0m     \u001b[43m_check_seekable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\serialization.py:544\u001b[0m, in \u001b[0;36m_check_seekable\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m    542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (io\u001b[38;5;241m.\u001b[39mUnsupportedOperation, \u001b[38;5;167;01mAttributeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 544\u001b[0m     \u001b[43mraise_err_msg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseek\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\serialization.py:537\u001b[0m, in \u001b[0;36m_check_seekable.<locals>.raise_err_msg\u001b[1;34m(patterns, e)\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n\u001b[0;32m    534\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. You can only torch.load from a file that is seekable.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    535\u001b[0m                         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please pre-load the data into a buffer like io.BytesIO and\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    536\u001b[0m                         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m try to load from it instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 537\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(msg)\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead."
     ]
    }
   ],
   "source": [
    "model = CNN()\n",
    "model = torch.load(model.state_dict(), 'cartpole_cnn_rgb.pth')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hatem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.x_threshold to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.x_threshold` for environment variables or `env.get_wrapper_attr('x_threshold')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\hatem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.state to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.state` for environment variables or `env.get_wrapper_attr('state')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# Voir encore mieux ce que ça donne\n",
    "\n",
    "# 1 : On collecte des images du cartpole (heuristique : random)\n",
    "\n",
    "env = gymnasium.make('MonCartPole-v1',render_mode=\"rgb_array\")\n",
    "data_images_bis = []\n",
    "data_states_bis = []\n",
    "\n",
    "for episode in range(3):\n",
    "    observation_bis = env.reset()[0]\n",
    "    images_bis = []\n",
    "    for t in range(1000):\n",
    "        img = get_screen()\n",
    "        # img_pil = Image.fromarray(img)\n",
    "        tensor_image = transform(img)  # Transform image immediately\n",
    "        images_bis.append(tensor_image)\n",
    "        \n",
    "        if len(images_bis) >= sequence_length:\n",
    "            # Stack the last sequence_length images to form a single sequence tensor\n",
    "            sequence_tensor = torch.stack(images_bis[-sequence_length:], dim=0).permute(1, 0, 2, 3)\n",
    "            data_images_bis.append(sequence_tensor)\n",
    "            data_states_bis.append(observation)\n",
    "        \n",
    "        action = env.action_space.sample()   # Use the heuristic policy\n",
    "        observation, reward, done, info, _ = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "data_states_bis = torch.tensor(data_states_bis, dtype=torch.float32)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "data_images_bis = torch.stack(data_images_bis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0104, -0.0314,  0.0197,  0.0868]]) tensor([ 0.0497,  0.1925,  0.0017, -0.3322])\n",
      "tensor([[ 0.0162,  0.0915,  0.0030, -0.1167]]) tensor([ 0.0535,  0.3876, -0.0049, -0.6243])\n",
      "tensor([[ 0.0200,  0.3284, -0.0379, -0.5083]]) tensor([ 0.0613,  0.1925, -0.0174, -0.3332])\n",
      "tensor([[ 0.0197,  0.2172, -0.0359, -0.3627]]) tensor([ 0.0652, -0.0023, -0.0241, -0.0460])\n",
      "tensor([[ 0.0216, -0.0398, -0.0228,  0.0510]]) tensor([ 0.0651, -0.1971, -0.0250,  0.2389])\n",
      "tensor([[ 0.0122, -0.1435, -0.0085,  0.2072]]) tensor([ 0.0612, -0.3919, -0.0202,  0.5236])\n",
      "tensor([[-0.0041, -0.4232,  0.0125,  0.5532]]) tensor([ 0.0533, -0.5867, -0.0098,  0.8099])\n",
      "tensor([[-0.0184, -0.5439,  0.0339,  0.7066]]) tensor([ 0.0416, -0.7817,  0.0064,  1.0995])\n",
      "tensor([[-0.0591, -1.0805,  0.0660,  1.3549]]) tensor([ 0.0260, -0.5866,  0.0284,  0.8088])\n",
      "tensor([[-0.0334, -0.6236,  0.0734,  0.9132]]) tensor([ 0.0142, -0.3919,  0.0446,  0.5252])\n",
      "tensor([[-0.0178, -0.3322,  0.0644,  0.5496]]) tensor([ 0.0064, -0.1975,  0.0551,  0.2469])\n",
      "tensor([[-0.0105, -0.1881,  0.0556,  0.3639]]) tensor([ 0.0024, -0.3933,  0.0600,  0.5565])\n",
      "tensor([[-0.0291, -0.4493,  0.0847,  0.7073]]) tensor([-0.0054, -0.5892,  0.0712,  0.8674])\n",
      "tensor([[-0.0491, -0.6276,  0.1116,  0.9665]]) tensor([-0.0172, -0.7852,  0.0885,  1.1816])\n",
      "tensor([[-0.0830, -1.0453,  0.1403,  1.5100]]) tensor([-0.0329, -0.9814,  0.1122,  1.5007])\n",
      "tensor([[-0.0935, -1.2034,  0.1535,  1.7077]]) tensor([-0.0525, -0.7878,  0.1422,  1.2450])\n",
      "tensor([[-0.0991, -1.1832,  0.1665,  1.7234]]) tensor([-0.0683, -0.9844,  0.1671,  1.5786])\n",
      "tensor([[-0.1140, -1.2110,  0.1803,  1.7752]]) tensor([-0.0880, -1.1811,  0.1986,  1.9184])\n",
      "tensor([[ 0.0224, -0.0151, -0.0260,  0.0159]]) tensor([ 0.0056, -0.2054, -0.0381,  0.2762])\n",
      "tensor([[ 0.0086, -0.2283, -0.0086,  0.2874]]) tensor([ 0.0015, -0.0097, -0.0325, -0.0282])\n",
      "tensor([[ 0.0240, -0.0333, -0.0307,  0.0140]]) tensor([ 0.0013, -0.2043, -0.0331,  0.2540])\n",
      "tensor([[ 0.0151, -0.1266, -0.0183,  0.1616]]) tensor([-0.0028, -0.3990, -0.0280,  0.5361])\n",
      "tensor([[-0.0016, -0.4369, -0.0050,  0.5257]]) tensor([-0.0108, -0.5937, -0.0173,  0.8198])\n",
      "tensor([[-0.0067, -0.4425,  0.0206,  0.5827]]) tensor([-2.2650e-02, -7.8858e-01, -9.1528e-04,  1.1070e+00])\n",
      "tensor([[-0.0725, -1.2519,  0.0617,  1.4933]]) tensor([-0.0384, -0.9837,  0.0212,  1.3994])\n",
      "tensor([[-0.0766, -1.2287,  0.0771,  1.5893]]) tensor([-0.0581, -0.7888,  0.0492,  1.1134])\n",
      "tensor([[-0.0593, -0.8979,  0.1120,  1.2661]]) tensor([-0.0739, -0.5944,  0.0715,  0.8365])\n",
      "tensor([[-0.0505, -0.6791,  0.1038,  1.0416]]) tensor([-0.0858, -0.7904,  0.0882,  1.1508])\n",
      "tensor([[-0.0714, -0.8360,  0.1351,  1.2613]]) tensor([-0.1016, -0.9866,  0.1112,  1.4698])\n",
      "tensor([[-0.1063, -1.4027,  0.1681,  1.9269]]) tensor([-0.1213, -1.1829,  0.1406,  1.7951])\n",
      "tensor([[-0.1389, -1.6341,  0.1960,  2.2443]]) tensor([-0.1450, -1.3793,  0.1765,  2.1279])\n",
      "tensor([[ 0.0072, -0.0167,  0.0296,  0.0802]]) tensor([ 0.0127, -0.2065,  0.0221,  0.2984])\n",
      "tensor([[-0.0022, -0.2344,  0.0323,  0.3481]]) tensor([ 0.0086, -0.4019,  0.0281,  0.5980])\n",
      "tensor([[-0.0246, -0.4944,  0.0641,  0.7235]]) tensor([ 0.0005, -0.2072,  0.0401,  0.3143])\n",
      "tensor([[-0.0057, -0.2085,  0.0542,  0.3846]]) tensor([-0.0036, -0.0126,  0.0463,  0.0345])\n",
      "tensor([[ 0.0040, -0.0100,  0.0407,  0.0986]]) tensor([-0.0039,  0.1818,  0.0470, -0.2432])\n",
      "tensor([[ 0.0040, -0.0100,  0.0407,  0.0986]]) tensor([-2.4614e-04,  3.7620e-01,  4.2167e-02, -5.2070e-01])\n",
      "tensor([[ 0.0234,  0.3621,  0.0143, -0.3877]]) tensor([ 0.0073,  0.1805,  0.0318, -0.2150])\n",
      "tensor([[ 0.0069,  0.2145,  0.0139, -0.2526]]) tensor([ 0.0109,  0.3752,  0.0275, -0.4975])\n",
      "tensor([[ 0.0070,  0.1959,  0.0129, -0.2422]]) tensor([ 0.0184,  0.1797,  0.0175, -0.1963])\n",
      "tensor([[ 0.0089, -0.0246,  0.0273,  0.0800]]) tensor([ 0.0220,  0.3745,  0.0136, -0.4834])\n",
      "tensor([[ 0.0192,  0.3363, -0.0040, -0.4511]]) tensor([ 0.0295,  0.5695,  0.0039, -0.7718])\n",
      "tensor([[ 0.0546,  0.5924, -0.0271, -0.7561]]) tensor([ 0.0409,  0.3743, -0.0115, -0.4779])\n",
      "tensor([[ 0.0262,  0.3549, -0.0464, -0.5540]]) tensor([ 0.0484,  0.1793, -0.0211, -0.1889])\n",
      "tensor([[ 0.0216, -0.0398, -0.0228,  0.0510]]) tensor([ 0.0519, -0.0155, -0.0249,  0.0971])\n",
      "tensor([[ 0.0216, -0.0398, -0.0228,  0.0510]]) tensor([ 0.0516, -0.2102, -0.0229,  0.3818])\n",
      "tensor([[ 0.0031, -0.3825, -0.0111,  0.4715]]) tensor([ 0.0474, -0.0148, -0.0153,  0.0820])\n",
      "tensor([[0.0128, 0.0124, 0.0094, 0.0486]]) tensor([ 0.0471, -0.2097, -0.0136,  0.3698])\n",
      "tensor([[ 0.0023, -0.1965,  0.0163,  0.3104]]) tensor([ 0.0429, -0.4046, -0.0063,  0.6582])\n",
      "tensor([[-0.0245, -0.6924,  0.0299,  0.8596]]) tensor([ 0.0348, -0.5997,  0.0069,  0.9489])\n",
      "tensor([[-0.0435, -0.7972,  0.0566,  1.0427]]) tensor([ 0.0228, -0.7949,  0.0259,  1.2437])\n",
      "tensor([[-0.0686, -1.1354,  0.0974,  1.5374]]) tensor([ 0.0069, -0.6001,  0.0508,  0.9593])\n",
      "tensor([[-0.0474, -0.7372,  0.0943,  1.0801]]) tensor([-0.0051, -0.7959,  0.0699,  1.2674])\n",
      "tensor([[-0.0774, -1.0860,  0.1295,  1.5040]]) tensor([-0.0210, -0.9918,  0.0953,  1.5812])\n",
      "tensor([[-0.1051, -1.4360,  0.1638,  1.9580]]) tensor([-0.0408, -0.7979,  0.1269,  1.3197])\n",
      "tensor([[-0.0933, -1.1733,  0.1568,  1.6806]]) tensor([-0.0568, -0.9944,  0.1533,  1.6492])\n",
      "tensor([[-0.1153, -1.2783,  0.1773,  1.8346]]) tensor([-0.0767, -0.8014,  0.1863,  1.4080])\n",
      "10.06732542719692\n"
     ]
    }
   ],
   "source": [
    "# 2 Afficher ce que prédit le modèle vs les vraies observations\n",
    "\n",
    "model.eval()\n",
    "total = 0 # Loss totale\n",
    "for images, states in zip(data_images_bis, data_states_bis):\n",
    "    with torch.no_grad():\n",
    "        print(model(images.unsqueeze(0)),states)\n",
    "        total += np.sum(np.array((model(images.unsqueeze(0))-states)**2))\n",
    "\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'gotta_test_that_one.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 2 : GREY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "sequence_length = 4  # Number of images in each sequence\n",
    "num_episodes = 1000   # Number of episodes for data collection\n",
    "                     # ENCORE UNE FOIS best_grey_model A ETE FAIT A 300, VOIR AVEC 1000 ? \n",
    "\n",
    "# Environment Setup\n",
    "env = gym.make('CartPole-v1')\n",
    "data_images = []\n",
    "data_states = []\n",
    "\n",
    "# Transformer les images et les convertir en tenseurs\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((60, 135)),\n",
    "    transforms.Grayscale()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cart location for centering image crop\n",
    "def get_cart_location(screen_width):\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "# Cropping, downsampling (and Grayscaling) image\n",
    "def get_screen():\n",
    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width * 0.6)\n",
    "    cart_location = get_cart_location(screen_width)\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return transform(screen.transpose(1,2,0)).squeeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hatem\\AppData\\Local\\Temp\\ipykernel_16112\\1955873250.py:28: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  data_states = torch.tensor(data_states, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Collecter les données\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    observation = env.reset()\n",
    "    images = [torch.zeros(60, 135) for _ in range(4)]\n",
    "    for t in range(1000):\n",
    "        img = env.render(mode='rgb_array')\n",
    "        tensor_image = get_screen()\n",
    "        # if t == 5:\n",
    "        #     fig, axes = plt.subplots(1, 2, figsize=(15, 5))  # Crée une figure et des axes avec 1 ligne et 'n_images' colonnes\n",
    "        #     axes[0].imshow(tensor_image)\n",
    "        #     axes[1].imshow(tensor_image_bis)\n",
    "\n",
    "        images.append(tensor_image)\n",
    "        \n",
    "        sequence_tensor = torch.stack(images[-sequence_length:], dim=0)\n",
    "        data_images.append(sequence_tensor)\n",
    "        data_states.append(observation)\n",
    "\n",
    "        action = env.action_space.sample()  \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "# env.close()\n",
    "\n",
    "# Convert data_states to a tensor \n",
    "data_states = torch.tensor(data_states, dtype=torch.float32)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = TensorDataset(torch.stack(data_images), data_states)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CartPoleCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(4, 16, kernel_size=5, stride=1, padding=2),  # Input: 4 gray images, output: 16 channels, 60x135\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                 # Output size: ? 30x67\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                 # Output size: ? 15x33\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)                  # Output size: ? 7x16\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 16, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,4)    # x, x_dot, theta, theta_dot\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output for the fully connected layers\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "# Instanciation du modèle\n",
    "model = CartPoleCNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 4]) torch.Size([10, 4, 60, 135])\n",
      "tensor([-0.0964,  0.2193,  0.1114, -0.0642])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x3000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for images, labels in dataloader:\n",
    "    print(labels.shape,images.shape)\n",
    "    fig,axes = plt.subplots(1,4,figsize = (20,30))\n",
    "    for i in range(4):\n",
    "        axes[i].imshow(images[0][i])\n",
    "    print(labels[0])\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.040232785046100616\n",
      "Epoch 2, Loss: 0.04797070100903511\n",
      "Epoch 3, Loss: 0.025605758652091026\n",
      "Epoch 4, Loss: 0.07242388278245926\n",
      "Epoch 5, Loss: 0.06184414401650429\n",
      "Epoch 6, Loss: 0.060747385025024414\n",
      "Epoch 7, Loss: 0.05459335073828697\n",
      "Epoch 8, Loss: 0.05036822333931923\n",
      "Epoch 9, Loss: 0.04405743256211281\n",
      "Epoch 10, Loss: 0.037607256323099136\n",
      "Epoch 11, Loss: 0.03315708413720131\n",
      "Epoch 12, Loss: 0.06277186423540115\n",
      "Epoch 13, Loss: 0.018332751467823982\n",
      "Epoch 14, Loss: 0.04693877696990967\n",
      "Epoch 15, Loss: 0.034181419759988785\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "num_epochs = 15 # Peut-être avec plus d'epoch on obtiendrait un meilleur résultat ? jsp\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for images, states in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, states)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voir encore mieux ce que ça donne\n",
    "\n",
    "# 1 : On collecte des images du cartpole (heuristique : random)\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "data_images_bis = []\n",
    "data_states_bis = []\n",
    "\n",
    "for episode in range(3):\n",
    "    observation_bis = env.reset()\n",
    "    images_bis = []\n",
    "    for t in range(1000):\n",
    "        img = env.render(mode='rgb_array')\n",
    "        tensor_image = transform(img).squeeze(0)  # Transform image immediately\n",
    "        images_bis.append(tensor_image)\n",
    "        \n",
    "        if len(images_bis) >= sequence_length:\n",
    "            # Stack the last sequence_length images to form a single sequence tensor\n",
    "            sequence_tensor = torch.stack(images_bis[-sequence_length:], dim=0)\n",
    "            data_images_bis.append(sequence_tensor)\n",
    "            data_states_bis.append(observation)\n",
    "        \n",
    "        action = env.action_space.sample()   # Use the heuristic policy\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "data_states_bis = torch.tensor(data_states_bis, dtype=torch.float32)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "data_images_bis = torch.stack(data_images_bis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0094,  0.1349,  0.0035, -0.1393]]) tensor([ 0.0172,  0.1797,  0.0396, -0.2858])\n",
      "tensor([[ 0.0061,  0.1454,  0.0386, -0.0396]]) tensor([ 0.0208,  0.3742,  0.0339, -0.5657])\n",
      "tensor([[0.0024, 0.1497, 0.0562, 0.0115]]) tensor([ 0.0283,  0.1787,  0.0226, -0.2625])\n",
      "tensor([[ 0.0072,  0.1407,  0.0330, -0.0509]]) tensor([ 0.0318,  0.3735,  0.0173, -0.5480])\n",
      "tensor([[ 0.0082,  0.1489,  0.0164, -0.1109]]) tensor([ 0.0393,  0.5683,  0.0064, -0.8352])\n",
      "tensor([[ 0.0052,  0.1780,  0.0446, -0.0458]]) tensor([ 0.0507,  0.7634, -0.0103, -1.1258])\n",
      "tensor([[-0.0044,  0.1806,  0.0924,  0.0875]]) tensor([ 0.0659,  0.5684, -0.0328, -0.8364])\n",
      "tensor([[-0.0038,  0.1718,  0.0887,  0.0902]]) tensor([ 0.0773,  0.7639, -0.0496, -1.1392])\n",
      "tensor([[0.0029, 0.1395, 0.0548, 0.0220]]) tensor([ 0.0926,  0.9597, -0.0724, -1.4470])\n",
      "tensor([[0.0017, 0.1368, 0.0543, 0.0210]]) tensor([ 0.1118,  1.1556, -0.1013, -1.7614])\n",
      "tensor([[ 0.0098,  0.0888,  0.0067, -0.0695]]) tensor([ 0.1349,  0.9618, -0.1365, -1.5019])\n",
      "tensor([[ 0.0093,  0.0806,  0.0113, -0.0436]]) tensor([ 0.1541,  1.1583, -0.1666, -1.8339])\n",
      "tensor([[ 0.0111,  0.1176, -0.0089, -0.1567]]) tensor([ 0.1773,  1.3548, -0.2032, -2.1734])\n",
      "tensor([[-0.0064,  0.1930,  0.0925,  0.0518]]) tensor([ 0.0238, -0.5625,  0.0627,  0.9394])\n",
      "tensor([[-0.0036,  0.2090,  0.0847,  0.0137]]) tensor([ 0.0126, -0.3683,  0.0815,  0.6671])\n",
      "tensor([[ 0.0072,  0.2119,  0.0405, -0.1129]]) tensor([ 0.0052, -0.5645,  0.0948,  0.9843])\n",
      "tensor([[ 0.0136,  0.2822,  0.0290, -0.2389]]) tensor([-0.0061, -0.3707,  0.1145,  0.7228])\n",
      "tensor([[ 0.0096,  0.3679,  0.0628, -0.2349]]) tensor([-0.0135, -0.1774,  0.1290,  0.4683])\n",
      "tensor([[ 0.0090,  0.3613,  0.0709, -0.2013]]) tensor([-0.0170, -0.3741,  0.1384,  0.7987])\n",
      "tensor([[ 0.0104,  0.3340,  0.0541, -0.2088]]) tensor([-0.0245, -0.5708,  0.1543,  1.1315])\n",
      "tensor([[ 0.0142,  0.2162,  0.0142, -0.2015]]) tensor([-0.0359, -0.3780,  0.1770,  0.8909])\n",
      "tensor([[ 0.0130,  0.3069,  0.0360, -0.2535]]) tensor([-0.0435, -0.1856,  0.1948,  0.6587])\n",
      "tensor([[ 0.0120,  0.3778,  0.0601, -0.2512]]) tensor([-0.0472,  0.0063,  0.2080,  0.4331])\n",
      "tensor([[ 0.0099,  0.3350,  0.0581, -0.1987]]) tensor([-0.0026,  0.1983, -0.0179, -0.3136])\n",
      "tensor([[ 0.0077,  0.3048,  0.0599, -0.1533]]) tensor([ 0.0014,  0.3937, -0.0242, -0.6118])\n",
      "tensor([[ 0.0060,  0.2793,  0.0602, -0.1188]]) tensor([ 0.0092,  0.5891, -0.0364, -0.9120])\n",
      "tensor([[ 0.0010,  0.2440,  0.0746, -0.0370]]) tensor([ 0.0210,  0.3945, -0.0546, -0.6310])\n",
      "tensor([[-0.0015,  0.2103,  0.0810,  0.0175]]) tensor([ 0.0289,  0.5904, -0.0673, -0.9404])\n",
      "tensor([[-0.0061,  0.1665,  0.0997,  0.1285]]) tensor([ 0.0407,  0.7863, -0.0861, -1.2534])\n",
      "tensor([[ 0.0080,  0.1355,  0.0249, -0.0692]]) tensor([ 0.0564,  0.9824, -0.1111, -1.5718])\n",
      "tensor([[-0.0097,  0.1722,  0.1341,  0.2068]]) tensor([ 0.0761,  0.7888, -0.1426, -1.3157])\n",
      "tensor([[-0.0011,  0.1352,  0.0778,  0.1058]]) tensor([ 0.0919,  0.5957, -0.1689, -1.0709])\n",
      "tensor([[0.0042, 0.1134, 0.0420, 0.0025]]) tensor([ 0.1038,  0.7927, -0.1903, -1.4114])\n",
      "54.2442552279681\n"
     ]
    }
   ],
   "source": [
    "# 2 Afficher ce que prédit le modèle vs les vraies observations\n",
    "\n",
    "model.eval()\n",
    "total = 0 # Loss totale\n",
    "for images, states in zip(data_images_bis, data_states_bis):\n",
    "    with torch.no_grad():\n",
    "        print(model(images.unsqueeze(0)),states)\n",
    "        total += np.sum(np.array((model(images.unsqueeze(0))-states)**2))\n",
    "\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'cnn_grey_1000.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST 3 : LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleCNN_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CartPoleCNN_LSTM, self).__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1), # Traite chaque image individuellement\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Flatten() # Flatten les sorties pour préparation à l'LSTM\n",
    "        )\n",
    "        self.lstm = nn.LSTM(input_size=16*30*67, hidden_size=128, batch_first=True) \n",
    "        self.fc_layer = nn.Linear(128, 4) # Output x, x_dot, theta, theta_dot\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, H, W = x.shape\n",
    "        c_in = x.view(batch_size * seq_len, 1, H, W) # Remodeler pour CNN\n",
    "        c_out = self.conv_layer(c_in)\n",
    "        r_in = c_out.view(batch_size, seq_len, -1) # Remodeler pour LSTM\n",
    "        lstm_out, (hn, cn) = self.lstm(r_in)\n",
    "        out = self.fc_layer(hn[-1]) # Utiliser le dernier hidden state\n",
    "        return out\n",
    "\n",
    "    \n",
    "# Instanciation du modèle\n",
    "model = CartPoleCNN_LSTM()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cart location for centering image crop\n",
    "def get_cart_location(screen_width):\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "# Cropping, downsampling (and Grayscaling) image\n",
    "def get_screen():\n",
    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width * 0.6)\n",
    "    cart_location = get_cart_location(screen_width)\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return transform(screen.transpose(1,2,0)).squeeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "sequence_length = 4  # Number of images in each sequence\n",
    "num_episodes = 300   # Number of episodes for data collection\n",
    "                     # ENCORE UNE FOIS best_grey_model A ETE FAIT A 300, VOIR AVEC 1000 ? \n",
    "\n",
    "# Environment Setup\n",
    "env = gym.make('CartPole-v1')\n",
    "data_images = []\n",
    "data_states = []\n",
    "\n",
    "# Transformer les images et les convertir en tenseurs\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((60, 135)),\n",
    "    transforms.Grayscale()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hatem\\AppData\\Local\\Temp\\ipykernel_14524\\1740750942.py:26: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  data_states = torch.tensor(data_states, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "for episode in range(num_episodes):\n",
    "    observation = env.reset()\n",
    "    images = [torch.zeros(60, 135) for _ in range(4)]\n",
    "    for t in range(1000):\n",
    "        img = env.render(mode='rgb_array')\n",
    "        tensor_image = get_screen()\n",
    "        # if t == 5:\n",
    "        #     fig, axes = plt.subplots(1, 2, figsize=(15, 5))  # Crée une figure et des axes avec 1 ligne et 'n_images' colonnes\n",
    "        #     axes[0].imshow(tensor_image)\n",
    "        #     axes[1].imshow(tensor_image_bis)\n",
    "\n",
    "        images.append(tensor_image)\n",
    "        \n",
    "        sequence_tensor = torch.stack(images[-sequence_length:], dim=0)\n",
    "        data_images.append(sequence_tensor)\n",
    "        data_states.append(observation)\n",
    "\n",
    "        action = env.action_space.sample()  \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Convert data_states to a tensor \n",
    "data_states = torch.tensor(data_states, dtype=torch.float32)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = TensorDataset(torch.stack(data_images), data_states)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1642354428768158\n",
      "Epoch 2, Loss: 0.20948323607444763\n",
      "Epoch 3, Loss: 0.18387961387634277\n",
      "Epoch 4, Loss: 0.17517533898353577\n",
      "Epoch 5, Loss: 0.34073683619499207\n",
      "Epoch 6, Loss: 0.40426793694496155\n",
      "Epoch 7, Loss: 0.13599316775798798\n",
      "Epoch 8, Loss: 0.329766184091568\n",
      "Epoch 9, Loss: 0.277312695980072\n",
      "Epoch 10, Loss: 0.3386182188987732\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "num_epochs = 10 \n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for images, states in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, states)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "torch.save(model.state_dict(), 'cnn_lstm_grey.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
